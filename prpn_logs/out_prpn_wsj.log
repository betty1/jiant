Starting main job...
04/19 11:03:52 AM: fastText library not found!
04/19 11:03:54 AM: Loading config from config/prpn.conf
04/19 11:03:55 AM: Waiting on git info....
04/19 11:03:55 AM: Git branch: prpn
04/19 11:03:55 AM: Git SHA: 39abaff9a9d1348ba93e4fc947b6e185ef98e6db
04/19 11:03:55 AM: Parsed args: 
{
  "FASTTEXT_MODEL_FILE": "",
  "JIANT_DATA_DIR": "/scratch/pmh330/jiant-data/",
  "NFS_PROJECT_PREFIX": "/beegfs/pmh330/jiant-prpn-outs",
  "allow_missing_task_map": 0,
  "allow_reuse_of_pretraining_parameters": 0,
  "allow_untrained_encoder_parameters": 0,
  "batch_size": 32,
  "bert_embeddings_mode": "none",
  "bert_fine_tune": 0,
  "bert_model_name": "",
  "bidirectional": 0,
  "bpp_base": 1,
  "char_embs": 0,
  "char_filter_sizes": "2,3,4,5",
  "classifier": "mlp",
  "classifier_dropout": 0.2,
  "classifier_hid_dim": 512,
  "classifier_loss_fn": "",
  "classifier_span_pooling": "x,y",
  "cola": {},
  "cola_classifier_dropout": 0.2,
  "cola_classifier_hid_dim": 256,
  "cola_d_proj": 256,
  "cola_lr": 0.0003,
  "cola_val_interval": 100,
  "cove": 0,
  "cove_fine_tune": 0,
  "cuda": 0,
  "d_char": 100,
  "d_ff": 2048,
  "d_hid": 400,
  "d_hid_attn": 512,
  "d_proj": 512,
  "d_tproj": 64,
  "d_word": 200,
  "data_dir": "/scratch/pmh330/jiant-data/",
  "dec_val_scale": 250,
  "do_full_eval": 1,
  "do_pretrain": 1,
  "do_target_task_training": 0,
  "dropout": 0.2,
  "dropout_embs": 0.2,
  "edgeprobe_cnn_context": 0,
  "edges-ccg-parse": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-ccg-tag": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-constituent-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-constituent-ptb": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-coref-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-coref-ontonotes-conll": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-dep-labeling": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-dep-labeling-ewt": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-dpr": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-ner-conll2003": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 250
  },
  "edges-ner-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-ner-tacred": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-nonterminal-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-pos-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-rel-semeval": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-rel-tacred": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-spr1": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-spr2": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-srl-conll2005": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-srl-conll2012": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-tmpl": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "elmo": 0,
  "elmo_chars_only": 1,
  "elmo_weight_file_path": "none",
  "embeddings_train": 0,
  "eval_data_fraction": 1,
  "eval_max_vals": 1000,
  "eval_val_interval": 500,
  "exp_dir": "/beegfs/pmh330/jiant-prpn-outs/prpn-wsj/",
  "exp_name": "prpn-wsj",
  "fastText": 0,
  "fastText_model_file": "",
  "force_include_wsj_vocabulary": 0,
  "global_ro_exp_dir": "/nfs/jsalt/share/exp/default",
  "grounded": {},
  "grounded_d_proj": 2048,
  "groundedsw": {},
  "groundedsw_d_proj": 2048,
  "is_probing_task": 0,
  "keep_all_checkpoints": 0,
  "load_eval_checkpoint": "none",
  "load_model": 1,
  "local_log_path": "/beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0/log.log",
  "lr": 0.001,
  "lr_decay_factor": 0.5,
  "lr_patience": 5,
  "max_char_v_size": 250,
  "max_epochs": -1,
  "max_grad_norm": 0.5,
  "max_seq_len": 70,
  "max_targ_word_v_size": 20000,
  "max_vals": 1000,
  "max_word_v_size": 30000,
  "min_lr": 1e-06,
  "mnli": {},
  "mnli-alt": {},
  "mnli-alt_classifier_dropout": 0.2,
  "mnli-alt_classifier_hid_dim": 512,
  "mnli-alt_lr": 0.0003,
  "mnli-alt_pair_attn": 1,
  "mnli-alt_val_interval": 1000,
  "mnli-diagnostic": {
    "use_classifier": "mnli"
  },
  "mnli_classifier_dropout": 0.2,
  "mnli_classifier_hid_dim": 512,
  "mnli_lr": 0.0003,
  "mnli_pair_attn": 1,
  "mnli_val_interval": 1000,
  "mrpc": {},
  "mrpc_classifier_dropout": 0.2,
  "mrpc_classifier_hid_dim": 256,
  "mrpc_d_proj": 256,
  "mrpc_lr": 0.0003,
  "mrpc_pair_attn": 0,
  "mrpc_val_interval": 100,
  "n_char_filters": 100,
  "n_heads": 8,
  "n_layers_enc": 2,
  "n_layers_highway": 0,
  "n_slots": 15,
  "nli-prob": {
    "probe_path": ""
  },
  "onlstm_chunk_size": 10,
  "onlstm_dropconnect": 0.5,
  "onlstm_dropouth": 0.3,
  "onlstm_dropouti": 0.3,
  "onlstm_tying": 0,
  "openai_embeddings_mode": "none",
  "openai_transformer": 0,
  "openai_transformer_ckpt": "",
  "openai_transformer_fine_tune": 0,
  "optimizer": "adam",
  "pair_attn": 1,
  "patience": 5,
  "pool_type": "max",
  "pretrain_tasks": "wsj",
  "project_dir": "/beegfs/pmh330/jiant-prpn-outs",
  "qnli": {},
  "qnli-alt": {},
  "qnli-alt_classifier_dropout": 0.2,
  "qnli-alt_classifier_hid_dim": 512,
  "qnli-alt_lr": 0.0003,
  "qnli-alt_pair_attn": 1,
  "qnli-alt_val_interval": 1000,
  "qnli_classifier_dropout": 0.2,
  "qnli_classifier_hid_dim": 512,
  "qnli_lr": 0.0003,
  "qnli_pair_attn": 1,
  "qnli_val_interval": 1000,
  "qqp": {},
  "qqp-alt": {},
  "qqp-alt_classifier_dropout": 0.2,
  "qqp-alt_classifier_hid_dim": 512,
  "qqp-alt_lr": 0.0003,
  "qqp-alt_pair_attn": 1,
  "qqp-alt_val_interval": 1000,
  "qqp_classifier_dropout": 0.2,
  "qqp_classifier_hid_dim": 512,
  "qqp_lr": 0.0003,
  "qqp_pair_attn": 1,
  "qqp_val_interval": 1000,
  "random_seed": 1234,
  "reindex_tasks": "",
  "reload_indexing": 0,
  "reload_tasks": 0,
  "reload_vocab": 0,
  "remote_log_name": "prpn-wsj__prpn-0",
  "rte": {},
  "rte_classifier_dropout": 0.4,
  "rte_classifier_hid_dim": 128,
  "rte_d_proj": 128,
  "rte_lr": 0.0003,
  "rte_pair_attn": 0,
  "rte_val_interval": 100,
  "run_dir": "/beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0",
  "run_name": "prpn-0",
  "s2s": {
    "attention": "bilinear",
    "d_hid_dec": 1024,
    "n_layers_dec": 1,
    "output_proj_input_dim": 1024,
    "target_embedding_dim": 300
  },
  "scaling_method": "uniform",
  "scheduler_threshold": 0.0001,
  "sent_enc": "prpn",
  "sep_embs_for_skip": 0,
  "shared_optimizer": 1,
  "shared_pair_attn": 0,
  "skip_embs": 0,
  "sst": {},
  "sst_classifier_dropout": 0.2,
  "sst_classifier_hid_dim": 256,
  "sst_d_proj": 256,
  "sst_lr": 0.0003,
  "sst_val_interval": 100,
  "sts-b": {},
  "sts-b-alt": {},
  "sts-b-alt_classifier_dropout": 0.2,
  "sts-b-alt_classifier_hid_dim": 512,
  "sts-b-alt_lr": 0.0003,
  "sts-b-alt_pair_attn": 1,
  "sts-b-alt_val_interval": 1000,
  "sts-b_classifier_dropout": 0.2,
  "sts-b_classifier_hid_dim": 512,
  "sts-b_lr": 0.0003,
  "sts-b_pair_attn": 1,
  "sts-b_val_interval": 1000,
  "target_tasks": "wsj",
  "tokenizer": "MosesTokenizer",
  "track_batch_utilization": 0,
  "trainer_type": "sampling",
  "training_data_fraction": 1,
  "transfer_paradigm": "frozen",
  "use_classifier": "",
  "val_data_limit": 5000,
  "val_interval": 1000,
  "warmup": 4000,
  "weighting_method": "proportional",
  "wnli": {},
  "wnli_classifier_dropout": 0.4,
  "wnli_classifier_hid_dim": 128,
  "wnli_d_proj": 128,
  "wnli_lr": 0.0003,
  "wnli_pair_attn": 0,
  "wnli_val_interval": 100,
  "word_embs": "scratch",
  "word_embs_file": "",
  "write_preds": 0,
  "write_strict_glue_format": 0
}
04/19 11:03:55 AM: Saved config to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0/params.conf
04/19 11:03:55 AM: Using random seed 1234
04/19 11:03:55 AM: Using GPU 0
04/19 11:03:55 AM: Loading tasks...
04/19 11:03:55 AM: Writing pre-preprocessed tasks to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/
04/19 11:03:55 AM: 	Loaded existing task wsj
04/19 11:03:55 AM: 	Task 'wsj': train=26560 val=2108 test=2356
04/19 11:03:55 AM: 	Finished loading tasks: wsj.
04/19 11:03:55 AM: Loading token dictionary from /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/vocab.
04/19 11:03:55 AM: 	Loaded vocab from /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/vocab
04/19 11:03:55 AM: 	Vocab namespace tokens: size 10002
04/19 11:03:55 AM: 	Vocab namespace chars: size 57
04/19 11:03:55 AM: 	Finished building vocab.
04/19 11:03:55 AM: 	Task 'wsj', split 'train': Found preprocessed copy in /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/preproc/wsj__train_data
04/19 11:03:55 AM: 	Task 'wsj', split 'val': Found preprocessed copy in /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/preproc/wsj__val_data
04/19 11:03:55 AM: 	Task 'wsj', split 'test': Found preprocessed copy in /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/preproc/wsj__test_data
04/19 11:03:55 AM: 	Task 'wsj': cleared in-memory data.
04/19 11:03:55 AM: 	Finished indexing tasks
04/19 11:03:55 AM: 	Lazy-loading indexed data for task='wsj' from /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/preproc
04/19 11:03:55 AM: All tasks initialized with data iterators.
04/19 11:03:55 AM: 	  Training on wsj
04/19 11:03:55 AM: 	  Evaluating on wsj
04/19 11:03:55 AM: 	Finished loading tasks in 0.611s
04/19 11:03:55 AM: 	 Tasks: ['wsj']
04/19 11:03:55 AM: Building model...
04/19 11:03:55 AM: 	Training word embeddings from scratch.
04/19 11:03:56 AM: 	Not using character embeddings!
04/19 11:03:56 AM: Initializing parameters
04/19 11:03:56 AM: Done initializing parameters; the following parameters are using their default initialization from their code
04/19 11:03:56 AM:    prpnlayer.emb.weight
04/19 11:03:56 AM:    prpnlayer.embedder.token_embedder_words.weight
04/19 11:03:56 AM:    prpnlayer.parser.gate.1.bias
04/19 11:03:56 AM:    prpnlayer.parser.gate.1.weight
04/19 11:03:56 AM:    prpnlayer.parser.gate.2.bias
04/19 11:03:56 AM:    prpnlayer.parser.gate.2.weight
04/19 11:03:56 AM:    prpnlayer.parser.gate.5.bias
04/19 11:03:56 AM:    prpnlayer.parser.gate.5.weight
04/19 11:03:56 AM:    prpnlayer.predictor.ffd.1.bias
04/19 11:03:56 AM:    prpnlayer.predictor.ffd.1.weight
04/19 11:03:56 AM:    prpnlayer.predictor.ffd.2.bias
04/19 11:03:56 AM:    prpnlayer.predictor.ffd.2.weight
04/19 11:03:56 AM:    prpnlayer.predictor.projector_pred.1.bias
04/19 11:03:56 AM:    prpnlayer.predictor.projector_pred.1.weight
04/19 11:03:56 AM:    prpnlayer.reader.0.memory_rnn.bias_hh
04/19 11:03:56 AM:    prpnlayer.reader.0.memory_rnn.bias_ih
04/19 11:03:56 AM:    prpnlayer.reader.0.memory_rnn.c_norm.beta
04/19 11:03:56 AM:    prpnlayer.reader.0.memory_rnn.c_norm.gamma
04/19 11:03:56 AM:    prpnlayer.reader.0.memory_rnn.hh.0.bias
04/19 11:03:56 AM:    prpnlayer.reader.0.memory_rnn.hh.0.weight
04/19 11:03:56 AM:    prpnlayer.reader.0.memory_rnn.hh.1.beta
04/19 11:03:56 AM:    prpnlayer.reader.0.memory_rnn.hh.1.gamma
04/19 11:03:56 AM:    prpnlayer.reader.0.memory_rnn.ih.0.bias
04/19 11:03:56 AM:    prpnlayer.reader.0.memory_rnn.ih.0.weight
04/19 11:03:56 AM:    prpnlayer.reader.0.memory_rnn.ih.1.beta
04/19 11:03:56 AM:    prpnlayer.reader.0.memory_rnn.ih.1.gamma
04/19 11:03:56 AM:    prpnlayer.reader.0.memory_rnn.weight_hh
04/19 11:03:56 AM:    prpnlayer.reader.0.memory_rnn.weight_ih
04/19 11:03:56 AM:    prpnlayer.reader.0.projector_summ.1.bias
04/19 11:03:56 AM:    prpnlayer.reader.0.projector_summ.1.weight
04/19 11:03:56 AM:    prpnlayer.reader.1.memory_rnn.bias_hh
04/19 11:03:56 AM:    prpnlayer.reader.1.memory_rnn.bias_ih
04/19 11:03:56 AM:    prpnlayer.reader.1.memory_rnn.c_norm.beta
04/19 11:03:56 AM:    prpnlayer.reader.1.memory_rnn.c_norm.gamma
04/19 11:03:56 AM:    prpnlayer.reader.1.memory_rnn.hh.0.bias
04/19 11:03:56 AM:    prpnlayer.reader.1.memory_rnn.hh.0.weight
04/19 11:03:56 AM:    prpnlayer.reader.1.memory_rnn.hh.1.beta
04/19 11:03:56 AM:    prpnlayer.reader.1.memory_rnn.hh.1.gamma
04/19 11:03:56 AM:    prpnlayer.reader.1.memory_rnn.ih.0.bias
04/19 11:03:56 AM:    prpnlayer.reader.1.memory_rnn.ih.0.weight
04/19 11:03:56 AM:    prpnlayer.reader.1.memory_rnn.ih.1.beta
04/19 11:03:56 AM:    prpnlayer.reader.1.memory_rnn.ih.1.gamma
04/19 11:03:56 AM:    prpnlayer.reader.1.memory_rnn.weight_hh
04/19 11:03:56 AM:    prpnlayer.reader.1.memory_rnn.weight_ih
04/19 11:03:56 AM:    prpnlayer.reader.1.projector_summ.1.bias
04/19 11:03:56 AM:    prpnlayer.reader.1.projector_summ.1.weight
04/19 11:03:56 AM: Initializing parameters
04/19 11:03:56 AM: Done initializing parameters; the following parameters are using their default initialization from their code
04/19 11:03:56 AM:    _phrase_layer.emb.weight
04/19 11:03:56 AM:    _phrase_layer.parser.gate.1.bias
04/19 11:03:56 AM:    _phrase_layer.parser.gate.1.weight
04/19 11:03:56 AM:    _phrase_layer.parser.gate.2.bias
04/19 11:03:56 AM:    _phrase_layer.parser.gate.2.weight
04/19 11:03:56 AM:    _phrase_layer.parser.gate.5.bias
04/19 11:03:56 AM:    _phrase_layer.parser.gate.5.weight
04/19 11:03:56 AM:    _phrase_layer.predictor.ffd.1.bias
04/19 11:03:56 AM:    _phrase_layer.predictor.ffd.1.weight
04/19 11:03:56 AM:    _phrase_layer.predictor.ffd.2.bias
04/19 11:03:56 AM:    _phrase_layer.predictor.ffd.2.weight
04/19 11:03:56 AM:    _phrase_layer.predictor.projector_pred.1.bias
04/19 11:03:56 AM:    _phrase_layer.predictor.projector_pred.1.weight
04/19 11:03:56 AM:    _phrase_layer.reader.0.memory_rnn.bias_hh
04/19 11:03:56 AM:    _phrase_layer.reader.0.memory_rnn.bias_ih
04/19 11:03:56 AM:    _phrase_layer.reader.0.memory_rnn.c_norm.beta
04/19 11:03:56 AM:    _phrase_layer.reader.0.memory_rnn.c_norm.gamma
04/19 11:03:56 AM:    _phrase_layer.reader.0.memory_rnn.hh.0.bias
04/19 11:03:56 AM:    _phrase_layer.reader.0.memory_rnn.hh.0.weight
04/19 11:03:56 AM:    _phrase_layer.reader.0.memory_rnn.hh.1.beta
04/19 11:03:56 AM:    _phrase_layer.reader.0.memory_rnn.hh.1.gamma
04/19 11:03:56 AM:    _phrase_layer.reader.0.memory_rnn.ih.0.bias
04/19 11:03:56 AM:    _phrase_layer.reader.0.memory_rnn.ih.0.weight
04/19 11:03:56 AM:    _phrase_layer.reader.0.memory_rnn.ih.1.beta
04/19 11:03:56 AM:    _phrase_layer.reader.0.memory_rnn.ih.1.gamma
04/19 11:03:56 AM:    _phrase_layer.reader.0.memory_rnn.weight_hh
04/19 11:03:56 AM:    _phrase_layer.reader.0.memory_rnn.weight_ih
04/19 11:03:56 AM:    _phrase_layer.reader.0.projector_summ.1.bias
04/19 11:03:56 AM:    _phrase_layer.reader.0.projector_summ.1.weight
04/19 11:03:56 AM:    _phrase_layer.reader.1.memory_rnn.bias_hh
04/19 11:03:56 AM:    _phrase_layer.reader.1.memory_rnn.bias_ih
04/19 11:03:56 AM:    _phrase_layer.reader.1.memory_rnn.c_norm.beta
04/19 11:03:56 AM:    _phrase_layer.reader.1.memory_rnn.c_norm.gamma
04/19 11:03:56 AM:    _phrase_layer.reader.1.memory_rnn.hh.0.bias
04/19 11:03:56 AM:    _phrase_layer.reader.1.memory_rnn.hh.0.weight
04/19 11:03:56 AM:    _phrase_layer.reader.1.memory_rnn.hh.1.beta
04/19 11:03:56 AM:    _phrase_layer.reader.1.memory_rnn.hh.1.gamma
04/19 11:03:56 AM:    _phrase_layer.reader.1.memory_rnn.ih.0.bias
04/19 11:03:56 AM:    _phrase_layer.reader.1.memory_rnn.ih.0.weight
04/19 11:03:56 AM:    _phrase_layer.reader.1.memory_rnn.ih.1.beta
04/19 11:03:56 AM:    _phrase_layer.reader.1.memory_rnn.ih.1.gamma
04/19 11:03:56 AM:    _phrase_layer.reader.1.memory_rnn.weight_hh
04/19 11:03:56 AM:    _phrase_layer.reader.1.memory_rnn.weight_ih
04/19 11:03:56 AM:    _phrase_layer.reader.1.projector_summ.1.bias
04/19 11:03:56 AM:    _phrase_layer.reader.1.projector_summ.1.weight
04/19 11:03:56 AM:    _text_field_embedder.token_embedder_words.weight
04/19 11:03:56 AM: Using PRPN sentence encoder!
04/19 11:03:56 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
04/19 11:03:56 AM: CURRENTLY DEFINED PARAMETERS: 
04/19 11:03:56 AM: cls_type = mlp
04/19 11:03:56 AM: d_hid = 512
04/19 11:03:56 AM: d_proj = 512
04/19 11:03:56 AM: shared_pair_attn = 0
04/19 11:03:56 AM: attn = 1
04/19 11:03:56 AM: d_hid_attn = 512
04/19 11:03:56 AM: dropout = 0.2
04/19 11:03:56 AM: cls_loss_fn = 
04/19 11:03:56 AM: cls_span_pooling = x,y
04/19 11:03:56 AM: edgeprobe_cnn_context = 0
04/19 11:03:56 AM: use_classifier = wsj
04/19 11:03:56 AM: 	Task 'wsj' params: {
  "cls_type": "mlp",
  "d_hid": 512,
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "",
  "cls_span_pooling": "x,y",
  "edgeprobe_cnn_context": 0,
  "use_classifier": "wsj"
}
04/19 11:04:06 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): ElmoTextFieldEmbedder(
      (token_embedder_words): Embedding()
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): PRPN(
      (drop): Dropout(p=0.2)
      (idrop): Dropout(p=0.2)
      (rdrop): Dropout(p=0.0)
      (embedder): ElmoTextFieldEmbedder(
        (token_embedder_words): Embedding()
      )
      (emb): Embedding(10002, 200)
      (parser): ParsingNetwork(
        (drop): Dropout(p=0.2)
        (gate): Sequential(
          (0): Dropout(p=0.2)
          (1): Conv1d(200, 400, kernel_size=(6,), stride=(1,))
          (2): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Dropout(p=0.2)
          (5): Conv1d(400, 2, kernel_size=(1,), stride=(1,), groups=2)
          (6): Sigmoid()
        )
      )
      (reader): ModuleList(
        (0): ReadingNetwork(
          (drop): Dropout(p=0.2)
          (memory_rnn): LSTMCell(
            200, 400
            (ih): Sequential(
              (0): Linear(in_features=200, out_features=1600, bias=True)
              (1): LayerNorm()
            )
            (hh): Sequential(
              (0): Linear(in_features=400, out_features=1600, bias=True)
              (1): LayerNorm()
            )
            (c_norm): LayerNorm()
            (drop): Dropout(p=0)
          )
          (projector_summ): Sequential(
            (0): Dropout(p=0.2)
            (1): Linear(in_features=600, out_features=400, bias=True)
            (2): Dropout(p=0.2)
          )
        )
        (1): ReadingNetwork(
          (drop): Dropout(p=0.2)
          (memory_rnn): LSTMCell(
            400, 400
            (ih): Sequential(
              (0): Linear(in_features=400, out_features=1600, bias=True)
              (1): LayerNorm()
            )
            (hh): Sequential(
              (0): Linear(in_features=400, out_features=1600, bias=True)
              (1): LayerNorm()
            )
            (c_norm): LayerNorm()
            (drop): Dropout(p=0)
          )
          (projector_summ): Sequential(
            (0): Dropout(p=0.2)
            (1): Linear(in_features=800, out_features=400, bias=True)
            (2): Dropout(p=0.2)
          )
        )
      )
      (predictor): PredictNetwork(
        (drop): Dropout(p=0.2)
        (projector_pred): Sequential(
          (0): Dropout(p=0.2)
          (1): Linear(in_features=400, out_features=400, bias=True)
          (2): Dropout(p=0.2)
        )
        (ffd): Sequential(
          (0): Dropout(p=0.2)
          (1): Linear(in_features=800, out_features=200, bias=True)
          (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): Tanh()
        )
      )
    )
    (_dropout): Dropout(p=0.2)
  )
  (wsj_hid2voc): Linear(in_features=200, out_features=10002, bias=True)
  (wsj_mdl): Linear(in_features=200, out_features=10002, bias=True)
)
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.emb.weight: torch.Size([10002, 200]) = 2000400
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.parser.gate.1.weight: torch.Size([400, 200, 6]) = 480000
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.parser.gate.1.bias: torch.Size([400]) = 400
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.parser.gate.2.weight: torch.Size([400]) = 400
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.parser.gate.2.bias: torch.Size([400]) = 400
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.parser.gate.5.weight: torch.Size([2, 200, 1]) = 400
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.parser.gate.5.bias: torch.Size([2]) = 2
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.weight_ih: torch.Size([1600, 200]) = 320000
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.weight_hh: torch.Size([1600, 400]) = 640000
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.bias_ih: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.bias_hh: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.ih.0.weight: torch.Size([1600, 200]) = 320000
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.ih.0.bias: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.ih.1.gamma: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.ih.1.beta: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.hh.0.weight: torch.Size([1600, 400]) = 640000
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.hh.0.bias: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.hh.1.gamma: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.hh.1.beta: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.c_norm.gamma: torch.Size([400]) = 400
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.c_norm.beta: torch.Size([400]) = 400
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.projector_summ.1.weight: torch.Size([400, 600]) = 240000
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.projector_summ.1.bias: torch.Size([400]) = 400
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.weight_ih: torch.Size([1600, 400]) = 640000
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.weight_hh: torch.Size([1600, 400]) = 640000
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.bias_ih: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.bias_hh: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.ih.0.weight: torch.Size([1600, 400]) = 640000
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.ih.0.bias: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.ih.1.gamma: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.ih.1.beta: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.hh.0.weight: torch.Size([1600, 400]) = 640000
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.hh.0.bias: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.hh.1.gamma: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.hh.1.beta: torch.Size([1600]) = 1600
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.c_norm.gamma: torch.Size([400]) = 400
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.c_norm.beta: torch.Size([400]) = 400
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.projector_summ.1.weight: torch.Size([400, 800]) = 320000
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.projector_summ.1.bias: torch.Size([400]) = 400
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.predictor.projector_pred.1.weight: torch.Size([400, 400]) = 160000
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.predictor.projector_pred.1.bias: torch.Size([400]) = 400
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.predictor.ffd.1.weight: torch.Size([200, 800]) = 160000
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.predictor.ffd.1.bias: torch.Size([200]) = 200
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.predictor.ffd.2.weight: torch.Size([200]) = 200
04/19 11:04:06 AM: >> Trainable param sent_encoder._phrase_layer.predictor.ffd.2.bias: torch.Size([200]) = 200
04/19 11:04:06 AM: >> Trainable param wsj_hid2voc.weight: torch.Size([10002, 200]) = 2000400
04/19 11:04:06 AM: >> Trainable param wsj_hid2voc.bias: torch.Size([10002]) = 10002
04/19 11:04:06 AM: Total number of parameters: 11881804 (1.18818e+07)
04/19 11:04:06 AM: Number of trainable parameters: 9881404 (9.8814e+06)
04/19 11:04:06 AM: 	Finished building model in 10.234s
04/19 11:04:06 AM: Will run the following steps:
Training model on tasks: wsj
Evaluating model on tasks: wsj
04/19 11:04:06 AM: Training...
04/19 11:04:06 AM: 	Using ReduceLROnPlateau scheduler!
04/19 11:04:06 AM: patience = 5
04/19 11:04:06 AM: val_interval = 1000
04/19 11:04:06 AM: max_vals = 1000
04/19 11:04:06 AM: cuda_device = 0
04/19 11:04:06 AM: grad_norm = 0.5
04/19 11:04:06 AM: grad_clipping = None
04/19 11:04:06 AM: lr_decay = 0.99
04/19 11:04:06 AM: min_lr = 1e-06
04/19 11:04:06 AM: keep_all_checkpoints = 0
04/19 11:04:06 AM: val_data_limit = 5000
04/19 11:04:06 AM: max_epochs = -1
04/19 11:04:06 AM: dec_val_scale = 250
04/19 11:04:06 AM: training_data_fraction = 1
04/19 11:04:06 AM: type = adam
04/19 11:04:06 AM: parameter_groups = None
04/19 11:04:06 AM: Number of trainable parameters: 9881404
04/19 11:04:06 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
04/19 11:04:06 AM: CURRENTLY DEFINED PARAMETERS: 
04/19 11:04:06 AM: lr = 0.001
04/19 11:04:06 AM: amsgrad = True
04/19 11:04:06 AM: type = reduce_on_plateau
04/19 11:04:06 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
04/19 11:04:06 AM: CURRENTLY DEFINED PARAMETERS: 
04/19 11:04:06 AM: mode = min
04/19 11:04:06 AM: factor = 0.5
04/19 11:04:06 AM: patience = 5
04/19 11:04:06 AM: threshold = 0.0001
04/19 11:04:06 AM: threshold_mode = abs
04/19 11:04:06 AM: verbose = True
04/19 11:04:06 AM: type = adam
04/19 11:04:06 AM: parameter_groups = None
04/19 11:04:06 AM: Number of trainable parameters: 9881404
04/19 11:04:06 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
04/19 11:04:06 AM: CURRENTLY DEFINED PARAMETERS: 
04/19 11:04:06 AM: lr = 0.001
04/19 11:04:06 AM: amsgrad = True
04/19 11:04:06 AM: type = reduce_on_plateau
04/19 11:04:06 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
04/19 11:04:06 AM: CURRENTLY DEFINED PARAMETERS: 
04/19 11:04:06 AM: mode = min
04/19 11:04:06 AM: factor = 0.5
04/19 11:04:06 AM: patience = 5
04/19 11:04:06 AM: threshold = 0.0001
04/19 11:04:06 AM: threshold_mode = abs
04/19 11:04:06 AM: verbose = True
04/19 11:04:06 AM: Found checkpoint main_epoch_11.th. Loading.
04/19 11:04:11 AM: Loaded model from checkpoint. Starting at pass 11000.
04/19 11:04:11 AM: Training examples per task: {'wsj': 26560}
04/19 11:04:11 AM: Sampling tasks proportional to number of training examples.
04/19 11:04:11 AM: Using weighting method: proportional, with normalized sample weights [1.] 
04/19 11:04:11 AM: Using loss scaling method: uniform, with weights {'wsj': 1.0}
04/19 11:04:11 AM: Beginning training. Stopping metric: wsj_perplexity
04/19 11:04:11 AM: Stopped training after 11 validation checks
04/19 11:04:11 AM: Trained wsj for 11000 batches or 13.253 epochs
04/19 11:04:11 AM: ***** VALIDATION RESULTS *****
04/19 11:04:11 AM: wsj_perplexity, 5, wsj_loss: 4.72897, macro_avg: 113.17857, micro_avg: 113.17857, wsj_perplexity: 113.17857
04/19 11:04:11 AM: micro_avg, 5, wsj_loss: 4.72897, macro_avg: 113.17857, micro_avg: 113.17857, wsj_perplexity: 113.17857
04/19 11:04:11 AM: macro_avg, 5, wsj_loss: 4.72897, macro_avg: 113.17857, micro_avg: 113.17857, wsj_perplexity: 113.17857
04/19 11:04:11 AM: In strict mode because do_target_task_training is off. Will crash if any tasks are missing from the checkpoint.
04/19 11:04:11 AM: Loaded model state from /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0/model_state_eval_best.th
04/19 11:04:11 AM: Evaluating...
04/19 11:04:11 AM: Evaluating on: wsj, split: val
/home/pmh330/miniconda3/envs/jiant/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/pmh330/miniconda3/envs/jiant/lib/python3.6/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
04/19 11:04:18 AM: Task wsj: has no predictions!
04/19 11:04:18 AM: Writing results for split 'val' to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/results.tsv
04/19 11:04:18 AM: micro_avg: 114.418, macro_avg: 114.418, wsj_perplexity: 114.418
04/19 11:04:18 AM: Done!
Starting main job...
04/19 11:06:12 AM: fastText library not found!
04/19 11:06:16 AM: Loading config from config/prpn.conf
04/19 11:06:17 AM: Waiting on git info....
04/19 11:06:17 AM: Git branch: prpn
04/19 11:06:17 AM: Git SHA: 39abaff9a9d1348ba93e4fc947b6e185ef98e6db
04/19 11:06:17 AM: Parsed args: 
{
  "FASTTEXT_MODEL_FILE": "",
  "JIANT_DATA_DIR": "/scratch/pmh330/jiant-data/",
  "NFS_PROJECT_PREFIX": "/beegfs/pmh330/jiant-prpn-outs",
  "allow_missing_task_map": 0,
  "allow_reuse_of_pretraining_parameters": 0,
  "allow_untrained_encoder_parameters": 0,
  "batch_size": 32,
  "bert_embeddings_mode": "none",
  "bert_fine_tune": 0,
  "bert_model_name": "",
  "bidirectional": 0,
  "bpp_base": 1,
  "char_embs": 0,
  "char_filter_sizes": "2,3,4,5",
  "classifier": "mlp",
  "classifier_dropout": 0.2,
  "classifier_hid_dim": 512,
  "classifier_loss_fn": "",
  "classifier_span_pooling": "x,y",
  "cola": {},
  "cola_classifier_dropout": 0.2,
  "cola_classifier_hid_dim": 256,
  "cola_d_proj": 256,
  "cola_lr": 0.0003,
  "cola_val_interval": 100,
  "cove": 0,
  "cove_fine_tune": 0,
  "cuda": 0,
  "d_char": 100,
  "d_ff": 2048,
  "d_hid": 400,
  "d_hid_attn": 512,
  "d_proj": 512,
  "d_tproj": 64,
  "d_word": 200,
  "data_dir": "/scratch/pmh330/jiant-data/",
  "dec_val_scale": 250,
  "do_full_eval": 1,
  "do_pretrain": 1,
  "do_target_task_training": 0,
  "dropout": 0.2,
  "dropout_embs": 0.2,
  "edgeprobe_cnn_context": 0,
  "edges-ccg-parse": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-ccg-tag": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-constituent-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-constituent-ptb": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-coref-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-coref-ontonotes-conll": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-dep-labeling": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-dep-labeling-ewt": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-dpr": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-ner-conll2003": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 250
  },
  "edges-ner-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-ner-tacred": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-nonterminal-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-pos-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-rel-semeval": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-rel-tacred": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-spr1": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-spr2": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-srl-conll2005": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-srl-conll2012": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-tmpl": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "elmo": 0,
  "elmo_chars_only": 1,
  "elmo_weight_file_path": "none",
  "embeddings_train": 0,
  "eval_data_fraction": 1,
  "eval_max_vals": 1000,
  "eval_val_interval": 500,
  "exp_dir": "/beegfs/pmh330/jiant-prpn-outs/prpn-wsj/",
  "exp_name": "prpn-wsj",
  "fastText": 0,
  "fastText_model_file": "",
  "force_include_wsj_vocabulary": 0,
  "global_ro_exp_dir": "/nfs/jsalt/share/exp/default",
  "grounded": {},
  "grounded_d_proj": 2048,
  "groundedsw": {},
  "groundedsw_d_proj": 2048,
  "is_probing_task": 0,
  "keep_all_checkpoints": 0,
  "load_eval_checkpoint": "none",
  "load_model": 1,
  "local_log_path": "/beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0/log.log",
  "lr": 0.001,
  "lr_decay_factor": 0.5,
  "lr_patience": 5,
  "max_char_v_size": 250,
  "max_epochs": -1,
  "max_grad_norm": 0.5,
  "max_seq_len": 70,
  "max_targ_word_v_size": 20000,
  "max_vals": 1000,
  "max_word_v_size": 30000,
  "min_lr": 1e-06,
  "mnli": {},
  "mnli-alt": {},
  "mnli-alt_classifier_dropout": 0.2,
  "mnli-alt_classifier_hid_dim": 512,
  "mnli-alt_lr": 0.0003,
  "mnli-alt_pair_attn": 1,
  "mnli-alt_val_interval": 1000,
  "mnli-diagnostic": {
    "use_classifier": "mnli"
  },
  "mnli_classifier_dropout": 0.2,
  "mnli_classifier_hid_dim": 512,
  "mnli_lr": 0.0003,
  "mnli_pair_attn": 1,
  "mnli_val_interval": 1000,
  "mrpc": {},
  "mrpc_classifier_dropout": 0.2,
  "mrpc_classifier_hid_dim": 256,
  "mrpc_d_proj": 256,
  "mrpc_lr": 0.0003,
  "mrpc_pair_attn": 0,
  "mrpc_val_interval": 100,
  "n_char_filters": 100,
  "n_heads": 8,
  "n_layers_enc": 2,
  "n_layers_highway": 0,
  "n_slots": 15,
  "nli-prob": {
    "probe_path": ""
  },
  "onlstm_chunk_size": 10,
  "onlstm_dropconnect": 0.5,
  "onlstm_dropouth": 0.3,
  "onlstm_dropouti": 0.3,
  "onlstm_tying": 0,
  "openai_embeddings_mode": "none",
  "openai_transformer": 0,
  "openai_transformer_ckpt": "",
  "openai_transformer_fine_tune": 0,
  "optimizer": "adam",
  "pair_attn": 1,
  "patience": 5,
  "pool_type": "max",
  "pretrain_tasks": "wsj",
  "project_dir": "/beegfs/pmh330/jiant-prpn-outs",
  "qnli": {},
  "qnli-alt": {},
  "qnli-alt_classifier_dropout": 0.2,
  "qnli-alt_classifier_hid_dim": 512,
  "qnli-alt_lr": 0.0003,
  "qnli-alt_pair_attn": 1,
  "qnli-alt_val_interval": 1000,
  "qnli_classifier_dropout": 0.2,
  "qnli_classifier_hid_dim": 512,
  "qnli_lr": 0.0003,
  "qnli_pair_attn": 1,
  "qnli_val_interval": 1000,
  "qqp": {},
  "qqp-alt": {},
  "qqp-alt_classifier_dropout": 0.2,
  "qqp-alt_classifier_hid_dim": 512,
  "qqp-alt_lr": 0.0003,
  "qqp-alt_pair_attn": 1,
  "qqp-alt_val_interval": 1000,
  "qqp_classifier_dropout": 0.2,
  "qqp_classifier_hid_dim": 512,
  "qqp_lr": 0.0003,
  "qqp_pair_attn": 1,
  "qqp_val_interval": 1000,
  "random_seed": 1234,
  "reindex_tasks": "",
  "reload_indexing": 0,
  "reload_tasks": 0,
  "reload_vocab": 0,
  "remote_log_name": "prpn-wsj__prpn-0",
  "rte": {},
  "rte_classifier_dropout": 0.4,
  "rte_classifier_hid_dim": 128,
  "rte_d_proj": 128,
  "rte_lr": 0.0003,
  "rte_pair_attn": 0,
  "rte_val_interval": 100,
  "run_dir": "/beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0",
  "run_name": "prpn-0",
  "s2s": {
    "attention": "bilinear",
    "d_hid_dec": 1024,
    "n_layers_dec": 1,
    "output_proj_input_dim": 1024,
    "target_embedding_dim": 300
  },
  "scaling_method": "uniform",
  "scheduler_threshold": 0.0001,
  "sent_enc": "prpn",
  "sep_embs_for_skip": 0,
  "shared_optimizer": 1,
  "shared_pair_attn": 0,
  "skip_embs": 0,
  "sst": {},
  "sst_classifier_dropout": 0.2,
  "sst_classifier_hid_dim": 256,
  "sst_d_proj": 256,
  "sst_lr": 0.0003,
  "sst_val_interval": 100,
  "sts-b": {},
  "sts-b-alt": {},
  "sts-b-alt_classifier_dropout": 0.2,
  "sts-b-alt_classifier_hid_dim": 512,
  "sts-b-alt_lr": 0.0003,
  "sts-b-alt_pair_attn": 1,
  "sts-b-alt_val_interval": 1000,
  "sts-b_classifier_dropout": 0.2,
  "sts-b_classifier_hid_dim": 512,
  "sts-b_lr": 0.0003,
  "sts-b_pair_attn": 1,
  "sts-b_val_interval": 1000,
  "target_tasks": "wsj",
  "tokenizer": "MosesTokenizer",
  "track_batch_utilization": 0,
  "trainer_type": "sampling",
  "training_data_fraction": 1,
  "transfer_paradigm": "frozen",
  "use_classifier": "",
  "val_data_limit": 5000,
  "val_interval": 1000,
  "warmup": 4000,
  "weighting_method": "proportional",
  "wnli": {},
  "wnli_classifier_dropout": 0.4,
  "wnli_classifier_hid_dim": 128,
  "wnli_d_proj": 128,
  "wnli_lr": 0.0003,
  "wnli_pair_attn": 0,
  "wnli_val_interval": 100,
  "word_embs": "scratch",
  "word_embs_file": "",
  "write_preds": 0,
  "write_strict_glue_format": 0
}
04/19 11:06:17 AM: Saved config to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0/params.conf
04/19 11:06:17 AM: Using random seed 1234
04/19 11:06:17 AM: Using GPU 0
04/19 11:06:17 AM: Loading tasks...
04/19 11:06:17 AM: Writing pre-preprocessed tasks to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/
04/19 11:06:17 AM: 	Creating task wsj from scratch
04/19 11:06:17 AM: BLEU scoring is turned off (current code in progress).Please use outputed prediction files to score offline
04/19 11:06:17 AM: 	Task 'wsj': train=13280 val=1054 test=1178
04/19 11:06:17 AM: 	Finished loading tasks: wsj.
04/19 11:06:17 AM: 	Building vocab from scratch
04/19 11:06:17 AM: 	Counting words for task: 'wsj'
04/19 11:06:19 AM: 	Finished counting words
04/19 11:06:19 AM: 	Saved vocab to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/vocab
04/19 11:06:19 AM: Loading token dictionary from /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/vocab.
04/19 11:06:19 AM: 	Loaded vocab from /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/vocab
04/19 11:06:19 AM: 	Vocab namespace tokens: size 10002
04/19 11:06:19 AM: 	Vocab namespace chars: size 57
04/19 11:06:19 AM: 	Finished building vocab.
04/19 11:06:19 AM: 	Task 'wsj', split 'train': indexing from scratch
04/19 11:06:28 AM: 	Task 'wsj', split 'train': saved 13280 instances to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/preproc/wsj__train_data
04/19 11:06:28 AM: 	Task 'wsj', split 'val': indexing from scratch
04/19 11:06:29 AM: 	Task 'wsj', split 'val': saved 1054 instances to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/preproc/wsj__val_data
04/19 11:06:29 AM: 	Task 'wsj', split 'test': indexing from scratch
04/19 11:06:30 AM: 	Task 'wsj', split 'test': saved 1178 instances to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/preproc/wsj__test_data
04/19 11:06:30 AM: 	Task 'wsj': cleared in-memory data.
04/19 11:06:30 AM: 	Finished indexing tasks
04/19 11:06:30 AM: 	Lazy-loading indexed data for task='wsj' from /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/preproc
04/19 11:06:30 AM: All tasks initialized with data iterators.
04/19 11:06:30 AM: 	  Training on wsj
04/19 11:06:30 AM: 	  Evaluating on wsj
04/19 11:06:30 AM: 	Finished loading tasks in 13.231s
04/19 11:06:30 AM: 	 Tasks: ['wsj']
04/19 11:06:30 AM: Building model...
04/19 11:06:30 AM: 	Training word embeddings from scratch.
04/19 11:06:30 AM: 	Not using character embeddings!
04/19 11:06:30 AM: Initializing parameters
04/19 11:06:30 AM: Done initializing parameters; the following parameters are using their default initialization from their code
04/19 11:06:30 AM:    prpnlayer.emb.weight
04/19 11:06:30 AM:    prpnlayer.embedder.token_embedder_words.weight
04/19 11:06:30 AM:    prpnlayer.parser.gate.1.bias
04/19 11:06:30 AM:    prpnlayer.parser.gate.1.weight
04/19 11:06:30 AM:    prpnlayer.parser.gate.2.bias
04/19 11:06:30 AM:    prpnlayer.parser.gate.2.weight
04/19 11:06:30 AM:    prpnlayer.parser.gate.5.bias
04/19 11:06:30 AM:    prpnlayer.parser.gate.5.weight
04/19 11:06:30 AM:    prpnlayer.predictor.ffd.1.bias
04/19 11:06:30 AM:    prpnlayer.predictor.ffd.1.weight
04/19 11:06:30 AM:    prpnlayer.predictor.ffd.2.bias
04/19 11:06:30 AM:    prpnlayer.predictor.ffd.2.weight
04/19 11:06:30 AM:    prpnlayer.predictor.projector_pred.1.bias
04/19 11:06:30 AM:    prpnlayer.predictor.projector_pred.1.weight
04/19 11:06:30 AM:    prpnlayer.reader.0.memory_rnn.bias_hh
04/19 11:06:30 AM:    prpnlayer.reader.0.memory_rnn.bias_ih
04/19 11:06:30 AM:    prpnlayer.reader.0.memory_rnn.c_norm.beta
04/19 11:06:30 AM:    prpnlayer.reader.0.memory_rnn.c_norm.gamma
04/19 11:06:30 AM:    prpnlayer.reader.0.memory_rnn.hh.0.bias
04/19 11:06:30 AM:    prpnlayer.reader.0.memory_rnn.hh.0.weight
04/19 11:06:30 AM:    prpnlayer.reader.0.memory_rnn.hh.1.beta
04/19 11:06:30 AM:    prpnlayer.reader.0.memory_rnn.hh.1.gamma
04/19 11:06:30 AM:    prpnlayer.reader.0.memory_rnn.ih.0.bias
04/19 11:06:30 AM:    prpnlayer.reader.0.memory_rnn.ih.0.weight
04/19 11:06:30 AM:    prpnlayer.reader.0.memory_rnn.ih.1.beta
04/19 11:06:30 AM:    prpnlayer.reader.0.memory_rnn.ih.1.gamma
04/19 11:06:30 AM:    prpnlayer.reader.0.memory_rnn.weight_hh
04/19 11:06:30 AM:    prpnlayer.reader.0.memory_rnn.weight_ih
04/19 11:06:30 AM:    prpnlayer.reader.0.projector_summ.1.bias
04/19 11:06:30 AM:    prpnlayer.reader.0.projector_summ.1.weight
04/19 11:06:30 AM:    prpnlayer.reader.1.memory_rnn.bias_hh
04/19 11:06:30 AM:    prpnlayer.reader.1.memory_rnn.bias_ih
04/19 11:06:30 AM:    prpnlayer.reader.1.memory_rnn.c_norm.beta
04/19 11:06:30 AM:    prpnlayer.reader.1.memory_rnn.c_norm.gamma
04/19 11:06:30 AM:    prpnlayer.reader.1.memory_rnn.hh.0.bias
04/19 11:06:30 AM:    prpnlayer.reader.1.memory_rnn.hh.0.weight
04/19 11:06:30 AM:    prpnlayer.reader.1.memory_rnn.hh.1.beta
04/19 11:06:30 AM:    prpnlayer.reader.1.memory_rnn.hh.1.gamma
04/19 11:06:30 AM:    prpnlayer.reader.1.memory_rnn.ih.0.bias
04/19 11:06:30 AM:    prpnlayer.reader.1.memory_rnn.ih.0.weight
04/19 11:06:30 AM:    prpnlayer.reader.1.memory_rnn.ih.1.beta
04/19 11:06:30 AM:    prpnlayer.reader.1.memory_rnn.ih.1.gamma
04/19 11:06:30 AM:    prpnlayer.reader.1.memory_rnn.weight_hh
04/19 11:06:30 AM:    prpnlayer.reader.1.memory_rnn.weight_ih
04/19 11:06:30 AM:    prpnlayer.reader.1.projector_summ.1.bias
04/19 11:06:30 AM:    prpnlayer.reader.1.projector_summ.1.weight
04/19 11:06:30 AM: Initializing parameters
04/19 11:06:30 AM: Done initializing parameters; the following parameters are using their default initialization from their code
04/19 11:06:30 AM:    _phrase_layer.emb.weight
04/19 11:06:30 AM:    _phrase_layer.parser.gate.1.bias
04/19 11:06:30 AM:    _phrase_layer.parser.gate.1.weight
04/19 11:06:30 AM:    _phrase_layer.parser.gate.2.bias
04/19 11:06:30 AM:    _phrase_layer.parser.gate.2.weight
04/19 11:06:30 AM:    _phrase_layer.parser.gate.5.bias
04/19 11:06:30 AM:    _phrase_layer.parser.gate.5.weight
04/19 11:06:30 AM:    _phrase_layer.predictor.ffd.1.bias
04/19 11:06:30 AM:    _phrase_layer.predictor.ffd.1.weight
04/19 11:06:30 AM:    _phrase_layer.predictor.ffd.2.bias
04/19 11:06:30 AM:    _phrase_layer.predictor.ffd.2.weight
04/19 11:06:30 AM:    _phrase_layer.predictor.projector_pred.1.bias
04/19 11:06:30 AM:    _phrase_layer.predictor.projector_pred.1.weight
04/19 11:06:30 AM:    _phrase_layer.reader.0.memory_rnn.bias_hh
04/19 11:06:30 AM:    _phrase_layer.reader.0.memory_rnn.bias_ih
04/19 11:06:30 AM:    _phrase_layer.reader.0.memory_rnn.c_norm.beta
04/19 11:06:30 AM:    _phrase_layer.reader.0.memory_rnn.c_norm.gamma
04/19 11:06:30 AM:    _phrase_layer.reader.0.memory_rnn.hh.0.bias
04/19 11:06:30 AM:    _phrase_layer.reader.0.memory_rnn.hh.0.weight
04/19 11:06:30 AM:    _phrase_layer.reader.0.memory_rnn.hh.1.beta
04/19 11:06:30 AM:    _phrase_layer.reader.0.memory_rnn.hh.1.gamma
04/19 11:06:30 AM:    _phrase_layer.reader.0.memory_rnn.ih.0.bias
04/19 11:06:30 AM:    _phrase_layer.reader.0.memory_rnn.ih.0.weight
04/19 11:06:30 AM:    _phrase_layer.reader.0.memory_rnn.ih.1.beta
04/19 11:06:30 AM:    _phrase_layer.reader.0.memory_rnn.ih.1.gamma
04/19 11:06:30 AM:    _phrase_layer.reader.0.memory_rnn.weight_hh
04/19 11:06:30 AM:    _phrase_layer.reader.0.memory_rnn.weight_ih
04/19 11:06:30 AM:    _phrase_layer.reader.0.projector_summ.1.bias
04/19 11:06:30 AM:    _phrase_layer.reader.0.projector_summ.1.weight
04/19 11:06:30 AM:    _phrase_layer.reader.1.memory_rnn.bias_hh
04/19 11:06:30 AM:    _phrase_layer.reader.1.memory_rnn.bias_ih
04/19 11:06:30 AM:    _phrase_layer.reader.1.memory_rnn.c_norm.beta
04/19 11:06:30 AM:    _phrase_layer.reader.1.memory_rnn.c_norm.gamma
04/19 11:06:30 AM:    _phrase_layer.reader.1.memory_rnn.hh.0.bias
04/19 11:06:30 AM:    _phrase_layer.reader.1.memory_rnn.hh.0.weight
04/19 11:06:30 AM:    _phrase_layer.reader.1.memory_rnn.hh.1.beta
04/19 11:06:30 AM:    _phrase_layer.reader.1.memory_rnn.hh.1.gamma
04/19 11:06:30 AM:    _phrase_layer.reader.1.memory_rnn.ih.0.bias
04/19 11:06:30 AM:    _phrase_layer.reader.1.memory_rnn.ih.0.weight
04/19 11:06:30 AM:    _phrase_layer.reader.1.memory_rnn.ih.1.beta
04/19 11:06:30 AM:    _phrase_layer.reader.1.memory_rnn.ih.1.gamma
04/19 11:06:30 AM:    _phrase_layer.reader.1.memory_rnn.weight_hh
04/19 11:06:30 AM:    _phrase_layer.reader.1.memory_rnn.weight_ih
04/19 11:06:30 AM:    _phrase_layer.reader.1.projector_summ.1.bias
04/19 11:06:30 AM:    _phrase_layer.reader.1.projector_summ.1.weight
04/19 11:06:30 AM:    _text_field_embedder.token_embedder_words.weight
04/19 11:06:30 AM: Using PRPN sentence encoder!
04/19 11:06:30 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
04/19 11:06:30 AM: CURRENTLY DEFINED PARAMETERS: 
04/19 11:06:30 AM: cls_type = mlp
04/19 11:06:30 AM: d_hid = 512
04/19 11:06:30 AM: d_proj = 512
04/19 11:06:30 AM: shared_pair_attn = 0
04/19 11:06:30 AM: attn = 1
04/19 11:06:30 AM: d_hid_attn = 512
04/19 11:06:30 AM: dropout = 0.2
04/19 11:06:30 AM: cls_loss_fn = 
04/19 11:06:30 AM: cls_span_pooling = x,y
04/19 11:06:30 AM: edgeprobe_cnn_context = 0
04/19 11:06:30 AM: use_classifier = wsj
04/19 11:06:30 AM: 	Task 'wsj' params: {
  "cls_type": "mlp",
  "d_hid": 512,
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "",
  "cls_span_pooling": "x,y",
  "edgeprobe_cnn_context": 0,
  "use_classifier": "wsj"
}
04/19 11:06:37 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): ElmoTextFieldEmbedder(
      (token_embedder_words): Embedding()
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): PRPN(
      (drop): Dropout(p=0.2)
      (idrop): Dropout(p=0.2)
      (rdrop): Dropout(p=0.0)
      (embedder): ElmoTextFieldEmbedder(
        (token_embedder_words): Embedding()
      )
      (emb): Embedding(10002, 200)
      (parser): ParsingNetwork(
        (drop): Dropout(p=0.2)
        (gate): Sequential(
          (0): Dropout(p=0.2)
          (1): Conv1d(200, 400, kernel_size=(6,), stride=(1,))
          (2): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU()
          (4): Dropout(p=0.2)
          (5): Conv1d(400, 2, kernel_size=(1,), stride=(1,), groups=2)
          (6): Sigmoid()
        )
      )
      (reader): ModuleList(
        (0): ReadingNetwork(
          (drop): Dropout(p=0.2)
          (memory_rnn): LSTMCell(
            200, 400
            (ih): Sequential(
              (0): Linear(in_features=200, out_features=1600, bias=True)
              (1): LayerNorm()
            )
            (hh): Sequential(
              (0): Linear(in_features=400, out_features=1600, bias=True)
              (1): LayerNorm()
            )
            (c_norm): LayerNorm()
            (drop): Dropout(p=0)
          )
          (projector_summ): Sequential(
            (0): Dropout(p=0.2)
            (1): Linear(in_features=600, out_features=400, bias=True)
            (2): Dropout(p=0.2)
          )
        )
        (1): ReadingNetwork(
          (drop): Dropout(p=0.2)
          (memory_rnn): LSTMCell(
            400, 400
            (ih): Sequential(
              (0): Linear(in_features=400, out_features=1600, bias=True)
              (1): LayerNorm()
            )
            (hh): Sequential(
              (0): Linear(in_features=400, out_features=1600, bias=True)
              (1): LayerNorm()
            )
            (c_norm): LayerNorm()
            (drop): Dropout(p=0)
          )
          (projector_summ): Sequential(
            (0): Dropout(p=0.2)
            (1): Linear(in_features=800, out_features=400, bias=True)
            (2): Dropout(p=0.2)
          )
        )
      )
      (predictor): PredictNetwork(
        (drop): Dropout(p=0.2)
        (projector_pred): Sequential(
          (0): Dropout(p=0.2)
          (1): Linear(in_features=400, out_features=400, bias=True)
          (2): Dropout(p=0.2)
        )
        (ffd): Sequential(
          (0): Dropout(p=0.2)
          (1): Linear(in_features=800, out_features=200, bias=True)
          (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): Tanh()
        )
      )
    )
    (_dropout): Dropout(p=0.2)
  )
  (wsj_hid2voc): Linear(in_features=200, out_features=10002, bias=True)
  (wsj_mdl): Linear(in_features=200, out_features=10002, bias=True)
)
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.emb.weight: torch.Size([10002, 200]) = 2000400
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.parser.gate.1.weight: torch.Size([400, 200, 6]) = 480000
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.parser.gate.1.bias: torch.Size([400]) = 400
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.parser.gate.2.weight: torch.Size([400]) = 400
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.parser.gate.2.bias: torch.Size([400]) = 400
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.parser.gate.5.weight: torch.Size([2, 200, 1]) = 400
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.parser.gate.5.bias: torch.Size([2]) = 2
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.weight_ih: torch.Size([1600, 200]) = 320000
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.weight_hh: torch.Size([1600, 400]) = 640000
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.bias_ih: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.bias_hh: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.ih.0.weight: torch.Size([1600, 200]) = 320000
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.ih.0.bias: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.ih.1.gamma: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.ih.1.beta: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.hh.0.weight: torch.Size([1600, 400]) = 640000
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.hh.0.bias: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.hh.1.gamma: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.hh.1.beta: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.c_norm.gamma: torch.Size([400]) = 400
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.memory_rnn.c_norm.beta: torch.Size([400]) = 400
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.projector_summ.1.weight: torch.Size([400, 600]) = 240000
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.0.projector_summ.1.bias: torch.Size([400]) = 400
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.weight_ih: torch.Size([1600, 400]) = 640000
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.weight_hh: torch.Size([1600, 400]) = 640000
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.bias_ih: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.bias_hh: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.ih.0.weight: torch.Size([1600, 400]) = 640000
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.ih.0.bias: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.ih.1.gamma: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.ih.1.beta: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.hh.0.weight: torch.Size([1600, 400]) = 640000
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.hh.0.bias: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.hh.1.gamma: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.hh.1.beta: torch.Size([1600]) = 1600
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.c_norm.gamma: torch.Size([400]) = 400
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.memory_rnn.c_norm.beta: torch.Size([400]) = 400
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.projector_summ.1.weight: torch.Size([400, 800]) = 320000
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.reader.1.projector_summ.1.bias: torch.Size([400]) = 400
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.predictor.projector_pred.1.weight: torch.Size([400, 400]) = 160000
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.predictor.projector_pred.1.bias: torch.Size([400]) = 400
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.predictor.ffd.1.weight: torch.Size([200, 800]) = 160000
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.predictor.ffd.1.bias: torch.Size([200]) = 200
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.predictor.ffd.2.weight: torch.Size([200]) = 200
04/19 11:06:37 AM: >> Trainable param sent_encoder._phrase_layer.predictor.ffd.2.bias: torch.Size([200]) = 200
04/19 11:06:37 AM: >> Trainable param wsj_hid2voc.weight: torch.Size([10002, 200]) = 2000400
04/19 11:06:37 AM: >> Trainable param wsj_hid2voc.bias: torch.Size([10002]) = 10002
04/19 11:06:37 AM: Total number of parameters: 11881804 (1.18818e+07)
04/19 11:06:37 AM: Number of trainable parameters: 9881404 (9.8814e+06)
04/19 11:06:37 AM: 	Finished building model in 6.996s
04/19 11:06:37 AM: Will run the following steps:
Training model on tasks: wsj
Evaluating model on tasks: wsj
04/19 11:06:37 AM: Training...
04/19 11:06:37 AM: 	Using ReduceLROnPlateau scheduler!
04/19 11:06:37 AM: patience = 5
04/19 11:06:37 AM: val_interval = 1000
04/19 11:06:37 AM: max_vals = 1000
04/19 11:06:37 AM: cuda_device = 0
04/19 11:06:37 AM: grad_norm = 0.5
04/19 11:06:37 AM: grad_clipping = None
04/19 11:06:37 AM: lr_decay = 0.99
04/19 11:06:37 AM: min_lr = 1e-06
04/19 11:06:37 AM: keep_all_checkpoints = 0
04/19 11:06:37 AM: val_data_limit = 5000
04/19 11:06:37 AM: max_epochs = -1
04/19 11:06:37 AM: dec_val_scale = 250
04/19 11:06:37 AM: training_data_fraction = 1
04/19 11:06:37 AM: type = adam
04/19 11:06:37 AM: parameter_groups = None
04/19 11:06:37 AM: Number of trainable parameters: 9881404
04/19 11:06:37 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
04/19 11:06:37 AM: CURRENTLY DEFINED PARAMETERS: 
04/19 11:06:37 AM: lr = 0.001
04/19 11:06:37 AM: amsgrad = True
04/19 11:06:37 AM: type = reduce_on_plateau
04/19 11:06:37 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
04/19 11:06:37 AM: CURRENTLY DEFINED PARAMETERS: 
04/19 11:06:37 AM: mode = min
04/19 11:06:37 AM: factor = 0.5
04/19 11:06:37 AM: patience = 5
04/19 11:06:37 AM: threshold = 0.0001
04/19 11:06:37 AM: threshold_mode = abs
04/19 11:06:37 AM: verbose = True
04/19 11:06:37 AM: type = adam
04/19 11:06:37 AM: parameter_groups = None
04/19 11:06:37 AM: Number of trainable parameters: 9881404
04/19 11:06:37 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
04/19 11:06:37 AM: CURRENTLY DEFINED PARAMETERS: 
04/19 11:06:37 AM: lr = 0.001
04/19 11:06:37 AM: amsgrad = True
04/19 11:06:37 AM: type = reduce_on_plateau
04/19 11:06:37 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
04/19 11:06:37 AM: CURRENTLY DEFINED PARAMETERS: 
04/19 11:06:37 AM: mode = min
04/19 11:06:37 AM: factor = 0.5
04/19 11:06:37 AM: patience = 5
04/19 11:06:37 AM: threshold = 0.0001
04/19 11:06:37 AM: threshold_mode = abs
04/19 11:06:37 AM: verbose = True
04/19 11:06:37 AM: Not loading.
04/19 11:06:37 AM: Training examples per task: {'wsj': 13280}
04/19 11:06:37 AM: Sampling tasks proportional to number of training examples.
04/19 11:06:37 AM: Using weighting method: proportional, with normalized sample weights [1.] 
04/19 11:06:37 AM: Using loss scaling method: uniform, with weights {'wsj': 1.0}
04/19 11:06:37 AM: Beginning training. Stopping metric: wsj_perplexity
/home/pmh330/miniconda3/envs/jiant/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/pmh330/miniconda3/envs/jiant/lib/python3.6/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
04/19 11:06:48 AM: Update 5: task wsj, batch 5 (5): perplexity: 10607.0013, wsj_loss: 9.2693 ||
04/19 11:06:58 AM: Update 17: task wsj, batch 17 (17): perplexity: 10088.1361, wsj_loss: 9.2191 ||
04/19 11:07:09 AM: Update 29: task wsj, batch 29 (29): perplexity: 9513.1258, wsj_loss: 9.1604 ||
04/19 11:07:19 AM: Update 41: task wsj, batch 41 (41): perplexity: 8831.8326, wsj_loss: 9.0861 ||
04/19 11:07:29 AM: Update 53: task wsj, batch 53 (53): perplexity: 8059.5420, wsj_loss: 8.9946 ||
04/19 11:07:40 AM: Update 65: task wsj, batch 65 (65): perplexity: 7231.2073, wsj_loss: 8.8862 ||
04/19 11:07:50 AM: Update 77: task wsj, batch 77 (77): perplexity: 6302.1429, wsj_loss: 8.7486 ||
04/19 11:08:00 AM: Update 89: task wsj, batch 89 (89): perplexity: 5438.3386, wsj_loss: 8.6012 ||
04/19 11:08:11 AM: Update 101: task wsj, batch 101 (101): perplexity: 4655.9733, wsj_loss: 8.4459 ||
04/19 11:08:21 AM: Update 113: task wsj, batch 113 (113): perplexity: 4011.7479, wsj_loss: 8.2970 ||
04/19 11:08:31 AM: Update 125: task wsj, batch 125 (125): perplexity: 3482.2927, wsj_loss: 8.1554 ||
04/19 11:08:42 AM: Update 137: task wsj, batch 137 (137): perplexity: 3044.3263, wsj_loss: 8.0210 ||
04/19 11:08:52 AM: Update 149: task wsj, batch 149 (149): perplexity: 2678.9550, wsj_loss: 7.8932 ||
04/19 11:09:03 AM: Update 161: task wsj, batch 161 (161): perplexity: 2382.9827, wsj_loss: 7.7761 ||
04/19 11:09:13 AM: Update 173: task wsj, batch 173 (173): perplexity: 2153.9534, wsj_loss: 7.6751 ||
04/19 11:09:23 AM: Update 185: task wsj, batch 185 (185): perplexity: 1959.4447, wsj_loss: 7.5804 ||
04/19 11:09:33 AM: Update 197: task wsj, batch 197 (197): perplexity: 1804.8657, wsj_loss: 7.4982 ||
04/19 11:09:44 AM: Update 209: task wsj, batch 209 (209): perplexity: 1669.3643, wsj_loss: 7.4202 ||
04/19 11:09:54 AM: Update 221: task wsj, batch 221 (221): perplexity: 1554.5245, wsj_loss: 7.3489 ||
04/19 11:10:05 AM: Update 233: task wsj, batch 233 (233): perplexity: 1461.4104, wsj_loss: 7.2872 ||
04/19 11:10:15 AM: Update 245: task wsj, batch 245 (245): perplexity: 1378.5779, wsj_loss: 7.2288 ||
04/19 11:10:25 AM: Update 257: task wsj, batch 257 (257): perplexity: 1303.3763, wsj_loss: 7.1727 ||
04/19 11:10:35 AM: Update 269: task wsj, batch 269 (269): perplexity: 1237.9116, wsj_loss: 7.1212 ||
04/19 11:10:46 AM: Update 281: task wsj, batch 281 (281): perplexity: 1180.0974, wsj_loss: 7.0734 ||
04/19 11:10:56 AM: Update 293: task wsj, batch 293 (293): perplexity: 1127.7977, wsj_loss: 7.0280 ||
04/19 11:11:06 AM: Update 305: task wsj, batch 305 (305): perplexity: 1079.9495, wsj_loss: 6.9847 ||
04/19 11:11:18 AM: Update 314: task wsj, batch 314 (314): perplexity: 1046.3439, wsj_loss: 6.9531 ||
04/19 11:11:28 AM: Update 326: task wsj, batch 326 (326): perplexity: 1004.3208, wsj_loss: 6.9121 ||
04/19 11:11:39 AM: Update 338: task wsj, batch 338 (338): perplexity: 966.7868, wsj_loss: 6.8740 ||
04/19 11:11:49 AM: Update 350: task wsj, batch 350 (350): perplexity: 932.4290, wsj_loss: 6.8378 ||
04/19 11:12:00 AM: Update 362: task wsj, batch 362 (362): perplexity: 899.2248, wsj_loss: 6.8015 ||
04/19 11:12:10 AM: Update 374: task wsj, batch 374 (374): perplexity: 868.0016, wsj_loss: 6.7662 ||
04/19 11:12:20 AM: Update 386: task wsj, batch 386 (386): perplexity: 838.6031, wsj_loss: 6.7317 ||
04/19 11:12:30 AM: Update 398: task wsj, batch 398 (398): perplexity: 810.6722, wsj_loss: 6.6979 ||
04/19 11:12:41 AM: Update 410: task wsj, batch 410 (410): perplexity: 785.9210, wsj_loss: 6.6669 ||
04/19 11:12:53 AM: Update 417: task wsj, batch 417 (417): perplexity: 771.8097, wsj_loss: 6.6487 ||
04/19 11:13:04 AM: Update 429: task wsj, batch 429 (429): perplexity: 750.3397, wsj_loss: 6.6205 ||
04/19 11:13:14 AM: Update 441: task wsj, batch 441 (441): perplexity: 730.0602, wsj_loss: 6.5931 ||
04/19 11:13:25 AM: Update 453: task wsj, batch 453 (453): perplexity: 711.3132, wsj_loss: 6.5671 ||
04/19 11:13:35 AM: Update 465: task wsj, batch 465 (465): perplexity: 693.5683, wsj_loss: 6.5418 ||
04/19 11:13:45 AM: Update 477: task wsj, batch 477 (477): perplexity: 676.7503, wsj_loss: 6.5173 ||
04/19 11:13:56 AM: Update 489: task wsj, batch 489 (489): perplexity: 661.0080, wsj_loss: 6.4938 ||
04/19 11:14:06 AM: Update 501: task wsj, batch 501 (501): perplexity: 645.9128, wsj_loss: 6.4707 ||
04/19 11:14:16 AM: Update 513: task wsj, batch 513 (513): perplexity: 631.7324, wsj_loss: 6.4485 ||
04/19 11:14:27 AM: Update 525: task wsj, batch 525 (525): perplexity: 617.8725, wsj_loss: 6.4263 ||
04/19 11:14:37 AM: Update 537: task wsj, batch 537 (537): perplexity: 604.7052, wsj_loss: 6.4047 ||
04/19 11:14:47 AM: Update 549: task wsj, batch 549 (549): perplexity: 592.3096, wsj_loss: 6.3840 ||
04/19 11:14:57 AM: Update 561: task wsj, batch 561 (561): perplexity: 580.2121, wsj_loss: 6.3634 ||
04/19 11:15:08 AM: Update 573: task wsj, batch 573 (573): perplexity: 569.1616, wsj_loss: 6.3442 ||
04/19 11:15:18 AM: Update 585: task wsj, batch 585 (585): perplexity: 558.1740, wsj_loss: 6.3247 ||
04/19 11:15:29 AM: Update 597: task wsj, batch 597 (597): perplexity: 547.9433, wsj_loss: 6.3062 ||
04/19 11:15:39 AM: Update 609: task wsj, batch 609 (609): perplexity: 538.4325, wsj_loss: 6.2887 ||
04/19 11:15:49 AM: Update 621: task wsj, batch 621 (621): perplexity: 528.7988, wsj_loss: 6.2706 ||
04/19 11:16:00 AM: Update 633: task wsj, batch 633 (633): perplexity: 519.8406, wsj_loss: 6.2535 ||
04/19 11:16:10 AM: Update 645: task wsj, batch 645 (645): perplexity: 511.2195, wsj_loss: 6.2368 ||
04/19 11:16:21 AM: Update 657: task wsj, batch 657 (657): perplexity: 503.0992, wsj_loss: 6.2208 ||
04/19 11:16:31 AM: Update 669: task wsj, batch 669 (669): perplexity: 495.0269, wsj_loss: 6.2046 ||
04/19 11:16:41 AM: Update 681: task wsj, batch 681 (681): perplexity: 487.5860, wsj_loss: 6.1895 ||
04/19 11:16:51 AM: Update 693: task wsj, batch 693 (693): perplexity: 480.5873, wsj_loss: 6.1750 ||
04/19 11:17:02 AM: Update 705: task wsj, batch 705 (705): perplexity: 473.7952, wsj_loss: 6.1608 ||
04/19 11:17:13 AM: Update 718: task wsj, batch 718 (718): perplexity: 466.2669, wsj_loss: 6.1448 ||
04/19 11:17:26 AM: Update 730: task wsj, batch 730 (730): perplexity: 459.5960, wsj_loss: 6.1303 ||
04/19 11:17:36 AM: Update 742: task wsj, batch 742 (742): perplexity: 453.2458, wsj_loss: 6.1164 ||
04/19 11:17:46 AM: Update 754: task wsj, batch 754 (754): perplexity: 447.0705, wsj_loss: 6.1027 ||
04/19 11:17:57 AM: Update 766: task wsj, batch 766 (766): perplexity: 440.3171, wsj_loss: 6.0875 ||
04/19 11:18:07 AM: Update 778: task wsj, batch 778 (778): perplexity: 433.8419, wsj_loss: 6.0727 ||
04/19 11:18:17 AM: Update 790: task wsj, batch 790 (790): perplexity: 427.8137, wsj_loss: 6.0587 ||
04/19 11:18:27 AM: Update 802: task wsj, batch 802 (802): perplexity: 422.3194, wsj_loss: 6.0458 ||
04/19 11:18:37 AM: Update 814: task wsj, batch 814 (814): perplexity: 416.6020, wsj_loss: 6.0321 ||
04/19 11:18:48 AM: Update 826: task wsj, batch 826 (826): perplexity: 410.9368, wsj_loss: 6.0184 ||
04/19 11:19:00 AM: Update 833: task wsj, batch 833 (833): perplexity: 408.4694, wsj_loss: 6.0124 ||
04/19 11:19:10 AM: Update 845: task wsj, batch 845 (845): perplexity: 403.5714, wsj_loss: 6.0004 ||
04/19 11:19:20 AM: Update 857: task wsj, batch 857 (857): perplexity: 398.7711, wsj_loss: 5.9884 ||
04/19 11:19:31 AM: Update 869: task wsj, batch 869 (869): perplexity: 394.0679, wsj_loss: 5.9765 ||
04/19 11:19:41 AM: Update 881: task wsj, batch 881 (881): perplexity: 389.6304, wsj_loss: 5.9652 ||
04/19 11:19:51 AM: Update 893: task wsj, batch 893 (893): perplexity: 385.5800, wsj_loss: 5.9547 ||
04/19 11:20:01 AM: Update 905: task wsj, batch 905 (905): perplexity: 381.4390, wsj_loss: 5.9440 ||
04/19 11:20:12 AM: Update 918: task wsj, batch 918 (918): perplexity: 377.1909, wsj_loss: 5.9328 ||
04/19 11:20:23 AM: Update 930: task wsj, batch 930 (930): perplexity: 373.3142, wsj_loss: 5.9224 ||
04/19 11:20:33 AM: Update 942: task wsj, batch 942 (942): perplexity: 369.4539, wsj_loss: 5.9120 ||
04/19 11:20:43 AM: Update 954: task wsj, batch 954 (954): perplexity: 365.6222, wsj_loss: 5.9016 ||
04/19 11:20:54 AM: Update 966: task wsj, batch 966 (966): perplexity: 361.9807, wsj_loss: 5.8916 ||
04/19 11:21:04 AM: Update 978: task wsj, batch 978 (978): perplexity: 358.3887, wsj_loss: 5.8816 ||
04/19 11:21:14 AM: Update 990: task wsj, batch 990 (990): perplexity: 355.1236, wsj_loss: 5.8725 ||
04/19 11:21:23 AM: ***** Pass 1000 / Epoch 1 *****
04/19 11:21:23 AM: wsj: trained on 1000 batches, 2.410 epochs
04/19 11:21:23 AM: Validating...
04/19 11:21:24 AM: Batch 5/33: perplexity: 203.5084, wsj_loss: 5.3157 || , for evaluation data
04/19 11:21:32 AM: Best model found for wsj.
04/19 11:21:32 AM: Best model found for micro.
04/19 11:21:32 AM: Best model found for macro.
04/19 11:21:32 AM: Advancing scheduler.
04/19 11:21:32 AM: 	Best macro_avg: 158.256
04/19 11:21:32 AM: 	# bad epochs: 0
04/19 11:21:32 AM: Statistic: wsj_loss
04/19 11:21:32 AM: 	training: 5.864510
04/19 11:21:32 AM: 	validation: 5.064216
04/19 11:21:32 AM: Statistic: macro_avg
04/19 11:21:32 AM: 	validation: 158.256328
04/19 11:21:32 AM: Statistic: micro_avg
04/19 11:21:32 AM: 	validation: 158.256328
04/19 11:21:32 AM: Statistic: wsj_perplexity
04/19 11:21:32 AM: 	training: 352.309593
04/19 11:21:32 AM: 	validation: 158.256328
04/19 11:21:32 AM: global_lr: 0.001000
04/19 11:21:32 AM: Saved files to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0
04/19 11:21:34 AM: Update 1003: task wsj, batch 3 (1003): perplexity: 147.6737, wsj_loss: 4.9950 ||
04/19 11:21:45 AM: Update 1016: task wsj, batch 16 (1016): perplexity: 152.6727, wsj_loss: 5.0283 ||
04/19 11:21:55 AM: Update 1028: task wsj, batch 28 (1028): perplexity: 156.4999, wsj_loss: 5.0531 ||
04/19 11:22:06 AM: Update 1040: task wsj, batch 40 (1040): perplexity: 160.1219, wsj_loss: 5.0759 ||
04/19 11:22:16 AM: Update 1052: task wsj, batch 52 (1052): perplexity: 159.3811, wsj_loss: 5.0713 ||
04/19 11:22:26 AM: Update 1064: task wsj, batch 64 (1064): perplexity: 160.0195, wsj_loss: 5.0753 ||
04/19 11:22:37 AM: Update 1076: task wsj, batch 76 (1076): perplexity: 159.4985, wsj_loss: 5.0720 ||
04/19 11:22:47 AM: Update 1088: task wsj, batch 88 (1088): perplexity: 160.1982, wsj_loss: 5.0764 ||
04/19 11:22:57 AM: Update 1100: task wsj, batch 100 (1100): perplexity: 160.7210, wsj_loss: 5.0797 ||
04/19 11:23:08 AM: Update 1112: task wsj, batch 112 (1112): perplexity: 159.9701, wsj_loss: 5.0750 ||
04/19 11:23:18 AM: Update 1124: task wsj, batch 124 (1124): perplexity: 158.7822, wsj_loss: 5.0675 ||
04/19 11:23:29 AM: Update 1136: task wsj, batch 136 (1136): perplexity: 158.6266, wsj_loss: 5.0666 ||
04/19 11:23:41 AM: Update 1146: task wsj, batch 146 (1146): perplexity: 158.1370, wsj_loss: 5.0635 ||
04/19 11:23:51 AM: Update 1158: task wsj, batch 158 (1158): perplexity: 156.6140, wsj_loss: 5.0538 ||
04/19 11:24:01 AM: Update 1170: task wsj, batch 170 (1170): perplexity: 155.2604, wsj_loss: 5.0451 ||
04/19 11:24:12 AM: Update 1182: task wsj, batch 182 (1182): perplexity: 154.5860, wsj_loss: 5.0408 ||
04/19 11:24:22 AM: Update 1194: task wsj, batch 194 (1194): perplexity: 153.6226, wsj_loss: 5.0345 ||
04/19 11:24:32 AM: Update 1206: task wsj, batch 206 (1206): perplexity: 152.7390, wsj_loss: 5.0287 ||
04/19 11:24:43 AM: Update 1218: task wsj, batch 218 (1218): perplexity: 151.5973, wsj_loss: 5.0212 ||
04/19 11:24:53 AM: Update 1230: task wsj, batch 230 (1230): perplexity: 150.5595, wsj_loss: 5.0144 ||
04/19 11:25:04 AM: Update 1242: task wsj, batch 242 (1242): perplexity: 149.9378, wsj_loss: 5.0102 ||
04/19 11:25:16 AM: Update 1249: task wsj, batch 249 (1249): perplexity: 149.4383, wsj_loss: 5.0069 ||
04/19 11:25:26 AM: Update 1261: task wsj, batch 261 (1261): perplexity: 148.7591, wsj_loss: 5.0023 ||
04/19 11:25:37 AM: Update 1273: task wsj, batch 273 (1273): perplexity: 148.2343, wsj_loss: 4.9988 ||
04/19 11:25:47 AM: Update 1285: task wsj, batch 285 (1285): perplexity: 147.6706, wsj_loss: 4.9950 ||
04/19 11:25:57 AM: Update 1297: task wsj, batch 297 (1297): perplexity: 147.3920, wsj_loss: 4.9931 ||
04/19 11:26:08 AM: Update 1309: task wsj, batch 309 (1309): perplexity: 146.3873, wsj_loss: 4.9863 ||
04/19 11:26:18 AM: Update 1321: task wsj, batch 321 (1321): perplexity: 145.7883, wsj_loss: 4.9822 ||
04/19 11:26:29 AM: Update 1333: task wsj, batch 333 (1333): perplexity: 145.5927, wsj_loss: 4.9808 ||
04/19 11:26:39 AM: Update 1345: task wsj, batch 345 (1345): perplexity: 145.1654, wsj_loss: 4.9779 ||
04/19 11:26:50 AM: Update 1357: task wsj, batch 357 (1357): perplexity: 144.9367, wsj_loss: 4.9763 ||
04/19 11:27:00 AM: Update 1369: task wsj, batch 369 (1369): perplexity: 144.4367, wsj_loss: 4.9728 ||
04/19 11:27:11 AM: Update 1381: task wsj, batch 381 (1381): perplexity: 144.0965, wsj_loss: 4.9705 ||
04/19 11:27:21 AM: Update 1393: task wsj, batch 393 (1393): perplexity: 144.0389, wsj_loss: 4.9701 ||
04/19 11:27:31 AM: Update 1405: task wsj, batch 405 (1405): perplexity: 143.6998, wsj_loss: 4.9677 ||
04/19 11:27:42 AM: Update 1417: task wsj, batch 417 (1417): perplexity: 143.6589, wsj_loss: 4.9674 ||
04/19 11:27:53 AM: Update 1429: task wsj, batch 429 (1429): perplexity: 143.2845, wsj_loss: 4.9648 ||
04/19 11:28:03 AM: Update 1441: task wsj, batch 441 (1441): perplexity: 142.7676, wsj_loss: 4.9612 ||
04/19 11:28:13 AM: Update 1453: task wsj, batch 453 (1453): perplexity: 142.6950, wsj_loss: 4.9607 ||
04/19 11:28:24 AM: Update 1465: task wsj, batch 465 (1465): perplexity: 142.4523, wsj_loss: 4.9590 ||
04/19 11:28:34 AM: Update 1477: task wsj, batch 477 (1477): perplexity: 142.4024, wsj_loss: 4.9587 ||
04/19 11:28:45 AM: Update 1489: task wsj, batch 489 (1489): perplexity: 142.3079, wsj_loss: 4.9580 ||
04/19 11:28:55 AM: Update 1501: task wsj, batch 501 (1501): perplexity: 142.0694, wsj_loss: 4.9563 ||
04/19 11:29:06 AM: Update 1513: task wsj, batch 513 (1513): perplexity: 141.8995, wsj_loss: 4.9551 ||
04/19 11:29:16 AM: Update 1525: task wsj, batch 525 (1525): perplexity: 141.6075, wsj_loss: 4.9531 ||
04/19 11:29:26 AM: Update 1537: task wsj, batch 537 (1537): perplexity: 141.3789, wsj_loss: 4.9514 ||
04/19 11:29:37 AM: Update 1549: task wsj, batch 549 (1549): perplexity: 140.9997, wsj_loss: 4.9488 ||
04/19 11:29:47 AM: Update 1561: task wsj, batch 561 (1561): perplexity: 140.6282, wsj_loss: 4.9461 ||
04/19 11:29:58 AM: Update 1570: task wsj, batch 570 (1570): perplexity: 140.3201, wsj_loss: 4.9439 ||
04/19 11:30:08 AM: Update 1582: task wsj, batch 582 (1582): perplexity: 139.7932, wsj_loss: 4.9402 ||
04/19 11:30:18 AM: Update 1594: task wsj, batch 594 (1594): perplexity: 139.4532, wsj_loss: 4.9377 ||
04/19 11:30:29 AM: Update 1606: task wsj, batch 606 (1606): perplexity: 138.9833, wsj_loss: 4.9344 ||
04/19 11:30:40 AM: Update 1618: task wsj, batch 618 (1618): perplexity: 138.5513, wsj_loss: 4.9312 ||
04/19 11:30:50 AM: Update 1630: task wsj, batch 630 (1630): perplexity: 138.0492, wsj_loss: 4.9276 ||
04/19 11:31:01 AM: Update 1642: task wsj, batch 642 (1642): perplexity: 137.4993, wsj_loss: 4.9236 ||
04/19 11:31:11 AM: Update 1654: task wsj, batch 654 (1654): perplexity: 136.9369, wsj_loss: 4.9195 ||
04/19 11:31:27 AM: Update 1665: task wsj, batch 665 (1665): perplexity: 136.4129, wsj_loss: 4.9157 ||
04/19 11:31:38 AM: Update 1677: task wsj, batch 677 (1677): perplexity: 135.9202, wsj_loss: 4.9121 ||
04/19 11:31:48 AM: Update 1689: task wsj, batch 689 (1689): perplexity: 135.5136, wsj_loss: 4.9091 ||
04/19 11:31:58 AM: Update 1701: task wsj, batch 701 (1701): perplexity: 135.1146, wsj_loss: 4.9061 ||
04/19 11:32:09 AM: Update 1713: task wsj, batch 713 (1713): perplexity: 134.7846, wsj_loss: 4.9037 ||
04/19 11:32:19 AM: Update 1725: task wsj, batch 725 (1725): perplexity: 134.4194, wsj_loss: 4.9010 ||
04/19 11:32:30 AM: Update 1737: task wsj, batch 737 (1737): perplexity: 134.0361, wsj_loss: 4.8981 ||
04/19 11:32:40 AM: Update 1749: task wsj, batch 749 (1749): perplexity: 133.7474, wsj_loss: 4.8960 ||
04/19 11:32:51 AM: Update 1761: task wsj, batch 761 (1761): perplexity: 133.4572, wsj_loss: 4.8938 ||
04/19 11:33:01 AM: Update 1773: task wsj, batch 773 (1773): perplexity: 133.2591, wsj_loss: 4.8923 ||
04/19 11:33:12 AM: Update 1785: task wsj, batch 785 (1785): perplexity: 132.9242, wsj_loss: 4.8898 ||
04/19 11:33:22 AM: Update 1797: task wsj, batch 797 (1797): perplexity: 132.7577, wsj_loss: 4.8885 ||
04/19 11:33:33 AM: Update 1809: task wsj, batch 809 (1809): perplexity: 132.5058, wsj_loss: 4.8866 ||
04/19 11:33:43 AM: Update 1821: task wsj, batch 821 (1821): perplexity: 132.2753, wsj_loss: 4.8849 ||
04/19 11:33:53 AM: Update 1833: task wsj, batch 833 (1833): perplexity: 131.9307, wsj_loss: 4.8823 ||
04/19 11:34:04 AM: Update 1845: task wsj, batch 845 (1845): perplexity: 131.6010, wsj_loss: 4.8798 ||
04/19 11:34:14 AM: Update 1857: task wsj, batch 857 (1857): perplexity: 131.4493, wsj_loss: 4.8786 ||
04/19 11:34:25 AM: Update 1869: task wsj, batch 869 (1869): perplexity: 131.1951, wsj_loss: 4.8767 ||
04/19 11:34:35 AM: Update 1881: task wsj, batch 881 (1881): perplexity: 130.9710, wsj_loss: 4.8750 ||
04/19 11:34:46 AM: Update 1893: task wsj, batch 893 (1893): perplexity: 130.8102, wsj_loss: 4.8737 ||
04/19 11:34:56 AM: Update 1905: task wsj, batch 905 (1905): perplexity: 130.7174, wsj_loss: 4.8730 ||
04/19 11:35:06 AM: Update 1917: task wsj, batch 917 (1917): perplexity: 130.3512, wsj_loss: 4.8702 ||
04/19 11:35:17 AM: Update 1929: task wsj, batch 929 (1929): perplexity: 130.2413, wsj_loss: 4.8694 ||
04/19 11:35:27 AM: Update 1941: task wsj, batch 941 (1941): perplexity: 130.0928, wsj_loss: 4.8682 ||
04/19 11:35:38 AM: Update 1953: task wsj, batch 953 (1953): perplexity: 129.9279, wsj_loss: 4.8670 ||
04/19 11:35:48 AM: Update 1965: task wsj, batch 965 (1965): perplexity: 129.7708, wsj_loss: 4.8658 ||
04/19 11:35:58 AM: Update 1977: task wsj, batch 977 (1977): perplexity: 129.5514, wsj_loss: 4.8641 ||
04/19 11:36:09 AM: Update 1986: task wsj, batch 986 (1986): perplexity: 129.2311, wsj_loss: 4.8616 ||
04/19 11:36:19 AM: Update 1998: task wsj, batch 998 (1998): perplexity: 128.9134, wsj_loss: 4.8591 ||
04/19 11:36:21 AM: ***** Pass 2000 / Epoch 2 *****
04/19 11:36:21 AM: wsj: trained on 1000 batches, 2.410 epochs
04/19 11:36:21 AM: Validating...
04/19 11:36:29 AM: Batch 30/33: perplexity: 120.2598, wsj_loss: 4.7897 || , for evaluation data
04/19 11:36:30 AM: Best model found for wsj.
04/19 11:36:30 AM: Best model found for micro.
04/19 11:36:30 AM: Best model found for macro.
04/19 11:36:30 AM: Advancing scheduler.
04/19 11:36:30 AM: 	Best macro_avg: 120.193
04/19 11:36:30 AM: 	# bad epochs: 0
04/19 11:36:30 AM: Statistic: wsj_loss
04/19 11:36:30 AM: 	training: 4.858767
04/19 11:36:30 AM: 	validation: 4.789102
04/19 11:36:30 AM: Statistic: macro_avg
04/19 11:36:30 AM: 	validation: 120.193375
04/19 11:36:30 AM: Statistic: micro_avg
04/19 11:36:30 AM: 	validation: 120.193375
04/19 11:36:30 AM: Statistic: wsj_perplexity
04/19 11:36:30 AM: 	training: 128.865164
04/19 11:36:30 AM: 	validation: 120.193375
04/19 11:36:30 AM: global_lr: 0.001000
04/19 11:36:30 AM: Saved files to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0
04/19 11:36:40 AM: Update 2011: task wsj, batch 11 (2011): perplexity: 102.1467, wsj_loss: 4.6264 ||
04/19 11:36:50 AM: Update 2023: task wsj, batch 23 (2023): perplexity: 102.7104, wsj_loss: 4.6319 ||
04/19 11:37:00 AM: Update 2035: task wsj, batch 35 (2035): perplexity: 102.6361, wsj_loss: 4.6312 ||
04/19 11:37:11 AM: Update 2047: task wsj, batch 47 (2047): perplexity: 101.8743, wsj_loss: 4.6237 ||
04/19 11:37:21 AM: Update 2059: task wsj, batch 59 (2059): perplexity: 102.3339, wsj_loss: 4.6282 ||
04/19 11:37:31 AM: Update 2071: task wsj, batch 71 (2071): perplexity: 101.6723, wsj_loss: 4.6218 ||
04/19 11:37:47 AM: Update 2081: task wsj, batch 81 (2081): perplexity: 101.9103, wsj_loss: 4.6241 ||
04/19 11:37:57 AM: Update 2093: task wsj, batch 93 (2093): perplexity: 101.0667, wsj_loss: 4.6158 ||
04/19 11:38:07 AM: Update 2105: task wsj, batch 105 (2105): perplexity: 100.9359, wsj_loss: 4.6145 ||
04/19 11:38:18 AM: Update 2117: task wsj, batch 117 (2117): perplexity: 100.6946, wsj_loss: 4.6121 ||
04/19 11:38:28 AM: Update 2129: task wsj, batch 129 (2129): perplexity: 101.0541, wsj_loss: 4.6157 ||
04/19 11:38:39 AM: Update 2141: task wsj, batch 141 (2141): perplexity: 101.2248, wsj_loss: 4.6173 ||
04/19 11:38:49 AM: Update 2153: task wsj, batch 153 (2153): perplexity: 100.9546, wsj_loss: 4.6147 ||
04/19 11:38:59 AM: Update 2165: task wsj, batch 165 (2165): perplexity: 100.5762, wsj_loss: 4.6109 ||
04/19 11:39:10 AM: Update 2177: task wsj, batch 177 (2177): perplexity: 100.7317, wsj_loss: 4.6125 ||
04/19 11:39:20 AM: Update 2189: task wsj, batch 189 (2189): perplexity: 101.0191, wsj_loss: 4.6153 ||
04/19 11:39:30 AM: Update 2201: task wsj, batch 201 (2201): perplexity: 101.0677, wsj_loss: 4.6158 ||
04/19 11:39:41 AM: Update 2213: task wsj, batch 213 (2213): perplexity: 101.2398, wsj_loss: 4.6175 ||
04/19 11:39:51 AM: Update 2225: task wsj, batch 225 (2225): perplexity: 101.2828, wsj_loss: 4.6179 ||
04/19 11:40:01 AM: Update 2237: task wsj, batch 237 (2237): perplexity: 101.5417, wsj_loss: 4.6205 ||
04/19 11:40:11 AM: Update 2249: task wsj, batch 249 (2249): perplexity: 101.6971, wsj_loss: 4.6220 ||
04/19 11:40:22 AM: Update 2261: task wsj, batch 261 (2261): perplexity: 101.9136, wsj_loss: 4.6241 ||
04/19 11:40:32 AM: Update 2273: task wsj, batch 273 (2273): perplexity: 102.0976, wsj_loss: 4.6259 ||
04/19 11:40:42 AM: Update 2285: task wsj, batch 285 (2285): perplexity: 102.2367, wsj_loss: 4.6273 ||
04/19 11:40:53 AM: Update 2297: task wsj, batch 297 (2297): perplexity: 102.2754, wsj_loss: 4.6277 ||
04/19 11:41:03 AM: Update 2309: task wsj, batch 309 (2309): perplexity: 102.3116, wsj_loss: 4.6280 ||
04/19 11:41:13 AM: Update 2321: task wsj, batch 321 (2321): perplexity: 102.2236, wsj_loss: 4.6272 ||
04/19 11:41:24 AM: Update 2333: task wsj, batch 333 (2333): perplexity: 102.3929, wsj_loss: 4.6288 ||
04/19 11:41:34 AM: Update 2345: task wsj, batch 345 (2345): perplexity: 102.4257, wsj_loss: 4.6291 ||
04/19 11:41:45 AM: Update 2357: task wsj, batch 357 (2357): perplexity: 102.4752, wsj_loss: 4.6296 ||
04/19 11:41:55 AM: Update 2369: task wsj, batch 369 (2369): perplexity: 102.5217, wsj_loss: 4.6301 ||
04/19 11:42:05 AM: Update 2381: task wsj, batch 381 (2381): perplexity: 102.6265, wsj_loss: 4.6311 ||
04/19 11:42:16 AM: Update 2393: task wsj, batch 393 (2393): perplexity: 102.8183, wsj_loss: 4.6330 ||
04/19 11:42:26 AM: Update 2402: task wsj, batch 402 (2402): perplexity: 102.6616, wsj_loss: 4.6314 ||
04/19 11:42:37 AM: Update 2414: task wsj, batch 414 (2414): perplexity: 102.4665, wsj_loss: 4.6295 ||
04/19 11:42:47 AM: Update 2426: task wsj, batch 426 (2426): perplexity: 102.1892, wsj_loss: 4.6268 ||
04/19 11:42:57 AM: Update 2438: task wsj, batch 438 (2438): perplexity: 101.8591, wsj_loss: 4.6236 ||
04/19 11:43:08 AM: Update 2450: task wsj, batch 450 (2450): perplexity: 101.5490, wsj_loss: 4.6205 ||
04/19 11:43:18 AM: Update 2462: task wsj, batch 462 (2462): perplexity: 101.1870, wsj_loss: 4.6170 ||
04/19 11:43:28 AM: Update 2474: task wsj, batch 474 (2474): perplexity: 100.9351, wsj_loss: 4.6145 ||
04/19 11:43:39 AM: Update 2486: task wsj, batch 486 (2486): perplexity: 100.7676, wsj_loss: 4.6128 ||
04/19 11:43:55 AM: Update 2497: task wsj, batch 497 (2497): perplexity: 100.2969, wsj_loss: 4.6081 ||
04/19 11:44:05 AM: Update 2509: task wsj, batch 509 (2509): perplexity: 100.0076, wsj_loss: 4.6052 ||
04/19 11:44:16 AM: Update 2521: task wsj, batch 521 (2521): perplexity: 99.7571, wsj_loss: 4.6027 ||
04/19 11:44:26 AM: Update 2533: task wsj, batch 533 (2533): perplexity: 99.7277, wsj_loss: 4.6024 ||
04/19 11:44:37 AM: Update 2545: task wsj, batch 545 (2545): perplexity: 99.6975, wsj_loss: 4.6021 ||
04/19 11:44:47 AM: Update 2557: task wsj, batch 557 (2557): perplexity: 99.6020, wsj_loss: 4.6012 ||
04/19 11:44:57 AM: Update 2569: task wsj, batch 569 (2569): perplexity: 99.5038, wsj_loss: 4.6002 ||
04/19 11:45:08 AM: Update 2581: task wsj, batch 581 (2581): perplexity: 99.2884, wsj_loss: 4.5980 ||
04/19 11:45:18 AM: Update 2593: task wsj, batch 593 (2593): perplexity: 99.1370, wsj_loss: 4.5965 ||
04/19 11:45:29 AM: Update 2605: task wsj, batch 605 (2605): perplexity: 98.9346, wsj_loss: 4.5945 ||
04/19 11:45:39 AM: Update 2617: task wsj, batch 617 (2617): perplexity: 98.7822, wsj_loss: 4.5929 ||
04/19 11:45:49 AM: Update 2629: task wsj, batch 629 (2629): perplexity: 98.6853, wsj_loss: 4.5919 ||
04/19 11:46:00 AM: Update 2641: task wsj, batch 641 (2641): perplexity: 98.5776, wsj_loss: 4.5908 ||
04/19 11:46:10 AM: Update 2653: task wsj, batch 653 (2653): perplexity: 98.5603, wsj_loss: 4.5907 ||
04/19 11:46:21 AM: Update 2665: task wsj, batch 665 (2665): perplexity: 98.3818, wsj_loss: 4.5889 ||
04/19 11:46:31 AM: Update 2677: task wsj, batch 677 (2677): perplexity: 98.3134, wsj_loss: 4.5882 ||
04/19 11:46:42 AM: Update 2689: task wsj, batch 689 (2689): perplexity: 98.1248, wsj_loss: 4.5862 ||
04/19 11:46:52 AM: Update 2701: task wsj, batch 701 (2701): perplexity: 98.0143, wsj_loss: 4.5851 ||
04/19 11:47:02 AM: Update 2713: task wsj, batch 713 (2713): perplexity: 97.9661, wsj_loss: 4.5846 ||
04/19 11:47:13 AM: Update 2725: task wsj, batch 725 (2725): perplexity: 97.9109, wsj_loss: 4.5841 ||
04/19 11:47:23 AM: Update 2737: task wsj, batch 737 (2737): perplexity: 97.8995, wsj_loss: 4.5839 ||
04/19 11:47:33 AM: Update 2749: task wsj, batch 749 (2749): perplexity: 97.8559, wsj_loss: 4.5835 ||
04/19 11:47:44 AM: Update 2761: task wsj, batch 761 (2761): perplexity: 97.7192, wsj_loss: 4.5821 ||
04/19 11:47:54 AM: Update 2773: task wsj, batch 773 (2773): perplexity: 97.7302, wsj_loss: 4.5822 ||
04/19 11:48:04 AM: Update 2785: task wsj, batch 785 (2785): perplexity: 97.7105, wsj_loss: 4.5820 ||
04/19 11:48:15 AM: Update 2797: task wsj, batch 797 (2797): perplexity: 97.6807, wsj_loss: 4.5817 ||
04/19 11:48:25 AM: Update 2809: task wsj, batch 809 (2809): perplexity: 97.6863, wsj_loss: 4.5818 ||
04/19 11:48:36 AM: Update 2818: task wsj, batch 818 (2818): perplexity: 97.5993, wsj_loss: 4.5809 ||
04/19 11:48:46 AM: Update 2830: task wsj, batch 830 (2830): perplexity: 97.4180, wsj_loss: 4.5790 ||
04/19 11:48:56 AM: Update 2842: task wsj, batch 842 (2842): perplexity: 97.2445, wsj_loss: 4.5772 ||
04/19 11:49:06 AM: Update 2854: task wsj, batch 854 (2854): perplexity: 97.0759, wsj_loss: 4.5755 ||
04/19 11:49:17 AM: Update 2866: task wsj, batch 866 (2866): perplexity: 96.8262, wsj_loss: 4.5729 ||
04/19 11:49:27 AM: Update 2878: task wsj, batch 878 (2878): perplexity: 96.5537, wsj_loss: 4.5701 ||
04/19 11:49:37 AM: Update 2890: task wsj, batch 890 (2890): perplexity: 96.2872, wsj_loss: 4.5673 ||
04/19 11:49:47 AM: Update 2902: task wsj, batch 902 (2902): perplexity: 96.1190, wsj_loss: 4.5656 ||
04/19 11:50:03 AM: Update 2913: task wsj, batch 913 (2913): perplexity: 95.9740, wsj_loss: 4.5641 ||
04/19 11:50:14 AM: Update 2925: task wsj, batch 925 (2925): perplexity: 95.8421, wsj_loss: 4.5627 ||
04/19 11:50:24 AM: Update 2937: task wsj, batch 937 (2937): perplexity: 95.7121, wsj_loss: 4.5613 ||
04/19 11:50:35 AM: Update 2949: task wsj, batch 949 (2949): perplexity: 95.5057, wsj_loss: 4.5592 ||
04/19 11:50:45 AM: Update 2961: task wsj, batch 961 (2961): perplexity: 95.3115, wsj_loss: 4.5572 ||
04/19 11:50:55 AM: Update 2973: task wsj, batch 973 (2973): perplexity: 95.1854, wsj_loss: 4.5558 ||
04/19 11:51:06 AM: Update 2985: task wsj, batch 985 (2985): perplexity: 95.0519, wsj_loss: 4.5544 ||
04/19 11:51:16 AM: Update 2997: task wsj, batch 997 (2997): perplexity: 94.9573, wsj_loss: 4.5534 ||
04/19 11:51:19 AM: ***** Pass 3000 / Epoch 3 *****
04/19 11:51:19 AM: wsj: trained on 1000 batches, 2.410 epochs
04/19 11:51:19 AM: Validating...
04/19 11:51:27 AM: Batch 26/33: perplexity: 120.3814, wsj_loss: 4.7907 || , for evaluation data
04/19 11:51:29 AM: Best model found for wsj.
04/19 11:51:29 AM: Best model found for micro.
04/19 11:51:29 AM: Best model found for macro.
04/19 11:51:29 AM: Advancing scheduler.
04/19 11:51:29 AM: 	Best macro_avg: 111.042
04/19 11:51:29 AM: 	# bad epochs: 0
04/19 11:51:29 AM: Statistic: wsj_loss
04/19 11:51:29 AM: 	training: 4.553044
04/19 11:51:29 AM: 	validation: 4.709907
04/19 11:51:29 AM: Statistic: macro_avg
04/19 11:51:29 AM: 	validation: 111.041868
04/19 11:51:29 AM: Statistic: micro_avg
04/19 11:51:29 AM: 	validation: 111.041868
04/19 11:51:29 AM: Statistic: wsj_perplexity
04/19 11:51:29 AM: 	training: 94.920948
04/19 11:51:29 AM: 	validation: 111.041868
04/19 11:51:29 AM: global_lr: 0.001000
04/19 11:51:29 AM: Saved files to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0
04/19 11:51:38 AM: Update 3010: task wsj, batch 10 (3010): perplexity: 86.0202, wsj_loss: 4.4546 ||
04/19 11:51:48 AM: Update 3022: task wsj, batch 22 (3022): perplexity: 87.5045, wsj_loss: 4.4717 ||
04/19 11:51:58 AM: Update 3034: task wsj, batch 34 (3034): perplexity: 86.0702, wsj_loss: 4.4552 ||
04/19 11:52:09 AM: Update 3046: task wsj, batch 46 (3046): perplexity: 85.6473, wsj_loss: 4.4502 ||
04/19 11:52:19 AM: Update 3058: task wsj, batch 58 (3058): perplexity: 84.9354, wsj_loss: 4.4419 ||
04/19 11:52:30 AM: Update 3070: task wsj, batch 70 (3070): perplexity: 85.1317, wsj_loss: 4.4442 ||
04/19 11:52:40 AM: Update 3082: task wsj, batch 82 (3082): perplexity: 85.5868, wsj_loss: 4.4495 ||
04/19 11:52:51 AM: Update 3094: task wsj, batch 94 (3094): perplexity: 85.7890, wsj_loss: 4.4519 ||
04/19 11:53:01 AM: Update 3106: task wsj, batch 106 (3106): perplexity: 86.2542, wsj_loss: 4.4573 ||
04/19 11:53:11 AM: Update 3118: task wsj, batch 118 (3118): perplexity: 86.7228, wsj_loss: 4.4627 ||
04/19 11:53:22 AM: Update 3130: task wsj, batch 130 (3130): perplexity: 86.8339, wsj_loss: 4.4640 ||
04/19 11:53:32 AM: Update 3142: task wsj, batch 142 (3142): perplexity: 87.2199, wsj_loss: 4.4684 ||
04/19 11:53:42 AM: Update 3154: task wsj, batch 154 (3154): perplexity: 87.4677, wsj_loss: 4.4713 ||
04/19 11:53:53 AM: Update 3166: task wsj, batch 166 (3166): perplexity: 87.0857, wsj_loss: 4.4669 ||
04/19 11:54:03 AM: Update 3178: task wsj, batch 178 (3178): perplexity: 87.2567, wsj_loss: 4.4689 ||
04/19 11:54:13 AM: Update 3190: task wsj, batch 190 (3190): perplexity: 87.2524, wsj_loss: 4.4688 ||
04/19 11:54:24 AM: Update 3202: task wsj, batch 202 (3202): perplexity: 87.3339, wsj_loss: 4.4697 ||
04/19 11:54:34 AM: Update 3214: task wsj, batch 214 (3214): perplexity: 87.2147, wsj_loss: 4.4684 ||
04/19 11:54:48 AM: Update 3226: task wsj, batch 226 (3226): perplexity: 86.9205, wsj_loss: 4.4650 ||
04/19 11:54:58 AM: Update 3238: task wsj, batch 238 (3238): perplexity: 86.5205, wsj_loss: 4.4604 ||
04/19 11:55:08 AM: Update 3250: task wsj, batch 250 (3250): perplexity: 86.3781, wsj_loss: 4.4587 ||
04/19 11:55:19 AM: Update 3262: task wsj, batch 262 (3262): perplexity: 86.1413, wsj_loss: 4.4560 ||
04/19 11:55:29 AM: Update 3274: task wsj, batch 274 (3274): perplexity: 85.5167, wsj_loss: 4.4487 ||
04/19 11:55:40 AM: Update 3286: task wsj, batch 286 (3286): perplexity: 85.2235, wsj_loss: 4.4453 ||
04/19 11:55:50 AM: Update 3298: task wsj, batch 298 (3298): perplexity: 84.8710, wsj_loss: 4.4411 ||
04/19 11:56:00 AM: Update 3310: task wsj, batch 310 (3310): perplexity: 84.3744, wsj_loss: 4.4353 ||
04/19 11:56:11 AM: Update 3322: task wsj, batch 322 (3322): perplexity: 84.0640, wsj_loss: 4.4316 ||
04/19 11:56:23 AM: Update 3329: task wsj, batch 329 (3329): perplexity: 83.9415, wsj_loss: 4.4301 ||
04/19 11:56:34 AM: Update 3341: task wsj, batch 341 (3341): perplexity: 83.7892, wsj_loss: 4.4283 ||
04/19 11:56:44 AM: Update 3353: task wsj, batch 353 (3353): perplexity: 83.7218, wsj_loss: 4.4275 ||
04/19 11:56:54 AM: Update 3365: task wsj, batch 365 (3365): perplexity: 83.6947, wsj_loss: 4.4272 ||
04/19 11:57:05 AM: Update 3377: task wsj, batch 377 (3377): perplexity: 83.5732, wsj_loss: 4.4257 ||
04/19 11:57:15 AM: Update 3389: task wsj, batch 389 (3389): perplexity: 83.3589, wsj_loss: 4.4232 ||
04/19 11:57:25 AM: Update 3401: task wsj, batch 401 (3401): perplexity: 83.1458, wsj_loss: 4.4206 ||
04/19 11:57:36 AM: Update 3413: task wsj, batch 413 (3413): perplexity: 83.0727, wsj_loss: 4.4197 ||
04/19 11:57:46 AM: Update 3425: task wsj, batch 425 (3425): perplexity: 82.9919, wsj_loss: 4.4187 ||
04/19 11:57:56 AM: Update 3437: task wsj, batch 437 (3437): perplexity: 82.8076, wsj_loss: 4.4165 ||
04/19 11:58:06 AM: Update 3449: task wsj, batch 449 (3449): perplexity: 82.7833, wsj_loss: 4.4162 ||
04/19 11:58:17 AM: Update 3461: task wsj, batch 461 (3461): perplexity: 82.7357, wsj_loss: 4.4157 ||
04/19 11:58:27 AM: Update 3473: task wsj, batch 473 (3473): perplexity: 82.6811, wsj_loss: 4.4150 ||
04/19 11:58:38 AM: Update 3485: task wsj, batch 485 (3485): perplexity: 82.6812, wsj_loss: 4.4150 ||
04/19 11:58:48 AM: Update 3497: task wsj, batch 497 (3497): perplexity: 82.4849, wsj_loss: 4.4126 ||
04/19 11:58:59 AM: Update 3509: task wsj, batch 509 (3509): perplexity: 82.3713, wsj_loss: 4.4112 ||
04/19 11:59:09 AM: Update 3521: task wsj, batch 521 (3521): perplexity: 82.3257, wsj_loss: 4.4107 ||
04/19 11:59:19 AM: Update 3533: task wsj, batch 533 (3533): perplexity: 82.2470, wsj_loss: 4.4097 ||
04/19 11:59:30 AM: Update 3545: task wsj, batch 545 (3545): perplexity: 82.2724, wsj_loss: 4.4100 ||
04/19 11:59:40 AM: Update 3557: task wsj, batch 557 (3557): perplexity: 82.2627, wsj_loss: 4.4099 ||
04/19 11:59:51 AM: Update 3569: task wsj, batch 569 (3569): perplexity: 82.2666, wsj_loss: 4.4100 ||
04/19 12:00:01 PM: Update 3581: task wsj, batch 581 (3581): perplexity: 82.2061, wsj_loss: 4.4092 ||
04/19 12:00:11 PM: Update 3593: task wsj, batch 593 (3593): perplexity: 82.1813, wsj_loss: 4.4089 ||
04/19 12:00:22 PM: Update 3605: task wsj, batch 605 (3605): perplexity: 82.1644, wsj_loss: 4.4087 ||
04/19 12:00:32 PM: Update 3617: task wsj, batch 617 (3617): perplexity: 82.2436, wsj_loss: 4.4097 ||
04/19 12:00:43 PM: Update 3629: task wsj, batch 629 (3629): perplexity: 82.2548, wsj_loss: 4.4098 ||
04/19 12:00:53 PM: Update 3641: task wsj, batch 641 (3641): perplexity: 82.3061, wsj_loss: 4.4104 ||
04/19 12:01:04 PM: Update 3650: task wsj, batch 650 (3650): perplexity: 82.1942, wsj_loss: 4.4091 ||
04/19 12:01:14 PM: Update 3662: task wsj, batch 662 (3662): perplexity: 82.0748, wsj_loss: 4.4076 ||
04/19 12:01:24 PM: Update 3674: task wsj, batch 674 (3674): perplexity: 81.8747, wsj_loss: 4.4052 ||
04/19 12:01:35 PM: Update 3686: task wsj, batch 686 (3686): perplexity: 81.6687, wsj_loss: 4.4027 ||
04/19 12:01:45 PM: Update 3698: task wsj, batch 698 (3698): perplexity: 81.5509, wsj_loss: 4.4012 ||
04/19 12:01:55 PM: Update 3710: task wsj, batch 710 (3710): perplexity: 81.3392, wsj_loss: 4.3986 ||
04/19 12:02:06 PM: Update 3722: task wsj, batch 722 (3722): perplexity: 81.1634, wsj_loss: 4.3965 ||
04/19 12:02:16 PM: Update 3734: task wsj, batch 734 (3734): perplexity: 81.0095, wsj_loss: 4.3946 ||
04/19 12:02:32 PM: Update 3745: task wsj, batch 745 (3745): perplexity: 80.8841, wsj_loss: 4.3930 ||
04/19 12:02:43 PM: Update 3757: task wsj, batch 757 (3757): perplexity: 80.8124, wsj_loss: 4.3921 ||
04/19 12:02:53 PM: Update 3769: task wsj, batch 769 (3769): perplexity: 80.7467, wsj_loss: 4.3913 ||
04/19 12:03:04 PM: Update 3781: task wsj, batch 781 (3781): perplexity: 80.6331, wsj_loss: 4.3899 ||
04/19 12:03:14 PM: Update 3793: task wsj, batch 793 (3793): perplexity: 80.5453, wsj_loss: 4.3888 ||
04/19 12:03:24 PM: Update 3805: task wsj, batch 805 (3805): perplexity: 80.4239, wsj_loss: 4.3873 ||
04/19 12:03:35 PM: Update 3817: task wsj, batch 817 (3817): perplexity: 80.3455, wsj_loss: 4.3863 ||
04/19 12:03:45 PM: Update 3829: task wsj, batch 829 (3829): perplexity: 80.2573, wsj_loss: 4.3852 ||
04/19 12:03:55 PM: Update 3841: task wsj, batch 841 (3841): perplexity: 80.1912, wsj_loss: 4.3844 ||
04/19 12:04:06 PM: Update 3853: task wsj, batch 853 (3853): perplexity: 80.0873, wsj_loss: 4.3831 ||
04/19 12:04:16 PM: Update 3865: task wsj, batch 865 (3865): perplexity: 80.0236, wsj_loss: 4.3823 ||
04/19 12:04:26 PM: Update 3877: task wsj, batch 877 (3877): perplexity: 79.9349, wsj_loss: 4.3812 ||
04/19 12:04:36 PM: Update 3889: task wsj, batch 889 (3889): perplexity: 79.9344, wsj_loss: 4.3812 ||
04/19 12:04:47 PM: Update 3901: task wsj, batch 901 (3901): perplexity: 79.9210, wsj_loss: 4.3810 ||
04/19 12:04:57 PM: Update 3913: task wsj, batch 913 (3913): perplexity: 79.8492, wsj_loss: 4.3801 ||
04/19 12:05:07 PM: Update 3925: task wsj, batch 925 (3925): perplexity: 79.7879, wsj_loss: 4.3794 ||
04/19 12:05:17 PM: Update 3937: task wsj, batch 937 (3937): perplexity: 79.7413, wsj_loss: 4.3788 ||
04/19 12:05:28 PM: Update 3949: task wsj, batch 949 (3949): perplexity: 79.7020, wsj_loss: 4.3783 ||
04/19 12:05:38 PM: Update 3961: task wsj, batch 961 (3961): perplexity: 79.7054, wsj_loss: 4.3783 ||
04/19 12:05:48 PM: Update 3973: task wsj, batch 973 (3973): perplexity: 79.7140, wsj_loss: 4.3784 ||
04/19 12:05:58 PM: Update 3985: task wsj, batch 985 (3985): perplexity: 79.6810, wsj_loss: 4.3780 ||
04/19 12:06:09 PM: Update 3997: task wsj, batch 997 (3997): perplexity: 79.6022, wsj_loss: 4.3770 ||
04/19 12:06:11 PM: ***** Pass 4000 / Epoch 4 *****
04/19 12:06:11 PM: wsj: trained on 1000 batches, 2.410 epochs
04/19 12:06:11 PM: Validating...
04/19 12:06:19 PM: Batch 27/33: perplexity: 114.0293, wsj_loss: 4.7365 || , for evaluation data
04/19 12:06:20 PM: Best model found for wsj.
04/19 12:06:20 PM: Best model found for micro.
04/19 12:06:20 PM: Best model found for macro.
04/19 12:06:20 PM: Advancing scheduler.
04/19 12:06:20 PM: 	Best macro_avg: 108.300
04/19 12:06:20 PM: 	# bad epochs: 0
04/19 12:06:20 PM: Statistic: wsj_loss
04/19 12:06:20 PM: 	training: 4.377292
04/19 12:06:20 PM: 	validation: 4.684909
04/19 12:06:20 PM: Statistic: macro_avg
04/19 12:06:20 PM: 	validation: 108.300465
04/19 12:06:20 PM: Statistic: micro_avg
04/19 12:06:20 PM: 	validation: 108.300465
04/19 12:06:20 PM: Statistic: wsj_perplexity
04/19 12:06:20 PM: 	training: 79.622085
04/19 12:06:20 PM: 	validation: 108.300465
04/19 12:06:20 PM: global_lr: 0.001000
04/19 12:06:21 PM: Saved files to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0
04/19 12:06:29 PM: Update 4010: task wsj, batch 10 (4010): perplexity: 77.8298, wsj_loss: 4.3545 ||
04/19 12:06:40 PM: Update 4022: task wsj, batch 22 (4022): perplexity: 74.8899, wsj_loss: 4.3160 ||
04/19 12:06:50 PM: Update 4034: task wsj, batch 34 (4034): perplexity: 74.8554, wsj_loss: 4.3156 ||
04/19 12:07:01 PM: Update 4046: task wsj, batch 46 (4046): perplexity: 74.8201, wsj_loss: 4.3151 ||
04/19 12:07:14 PM: Update 4058: task wsj, batch 58 (4058): perplexity: 75.4786, wsj_loss: 4.3238 ||
04/19 12:07:24 PM: Update 4070: task wsj, batch 70 (4070): perplexity: 74.2825, wsj_loss: 4.3079 ||
04/19 12:07:35 PM: Update 4082: task wsj, batch 82 (4082): perplexity: 73.8204, wsj_loss: 4.3016 ||
04/19 12:07:45 PM: Update 4094: task wsj, batch 94 (4094): perplexity: 73.3403, wsj_loss: 4.2951 ||
04/19 12:07:55 PM: Update 4106: task wsj, batch 106 (4106): perplexity: 72.5917, wsj_loss: 4.2849 ||
04/19 12:08:06 PM: Update 4118: task wsj, batch 118 (4118): perplexity: 71.9252, wsj_loss: 4.2756 ||
04/19 12:08:16 PM: Update 4130: task wsj, batch 130 (4130): perplexity: 72.0129, wsj_loss: 4.2768 ||
04/19 12:08:26 PM: Update 4142: task wsj, batch 142 (4142): perplexity: 71.8148, wsj_loss: 4.2741 ||
04/19 12:08:37 PM: Update 4154: task wsj, batch 154 (4154): perplexity: 71.1054, wsj_loss: 4.2642 ||
04/19 12:08:50 PM: Update 4161: task wsj, batch 161 (4161): perplexity: 71.0420, wsj_loss: 4.2633 ||
04/19 12:09:00 PM: Update 4173: task wsj, batch 173 (4173): perplexity: 71.0377, wsj_loss: 4.2632 ||
04/19 12:09:11 PM: Update 4185: task wsj, batch 185 (4185): perplexity: 70.7564, wsj_loss: 4.2592 ||
04/19 12:09:21 PM: Update 4197: task wsj, batch 197 (4197): perplexity: 70.5820, wsj_loss: 4.2568 ||
04/19 12:09:31 PM: Update 4209: task wsj, batch 209 (4209): perplexity: 70.5750, wsj_loss: 4.2567 ||
04/19 12:09:42 PM: Update 4221: task wsj, batch 221 (4221): perplexity: 70.4500, wsj_loss: 4.2549 ||
04/19 12:09:52 PM: Update 4233: task wsj, batch 233 (4233): perplexity: 70.5970, wsj_loss: 4.2570 ||
04/19 12:10:03 PM: Update 4245: task wsj, batch 245 (4245): perplexity: 70.6715, wsj_loss: 4.2580 ||
04/19 12:10:13 PM: Update 4257: task wsj, batch 257 (4257): perplexity: 70.5239, wsj_loss: 4.2560 ||
04/19 12:10:23 PM: Update 4269: task wsj, batch 269 (4269): perplexity: 70.5262, wsj_loss: 4.2560 ||
04/19 12:10:34 PM: Update 4281: task wsj, batch 281 (4281): perplexity: 70.6980, wsj_loss: 4.2584 ||
04/19 12:10:44 PM: Update 4293: task wsj, batch 293 (4293): perplexity: 70.7302, wsj_loss: 4.2589 ||
04/19 12:10:54 PM: Update 4305: task wsj, batch 305 (4305): perplexity: 70.8102, wsj_loss: 4.2600 ||
04/19 12:11:05 PM: Update 4317: task wsj, batch 317 (4317): perplexity: 70.9627, wsj_loss: 4.2622 ||
04/19 12:11:15 PM: Update 4329: task wsj, batch 329 (4329): perplexity: 70.9536, wsj_loss: 4.2620 ||
04/19 12:11:25 PM: Update 4341: task wsj, batch 341 (4341): perplexity: 70.9367, wsj_loss: 4.2618 ||
04/19 12:11:36 PM: Update 4353: task wsj, batch 353 (4353): perplexity: 70.9301, wsj_loss: 4.2617 ||
04/19 12:11:46 PM: Update 4365: task wsj, batch 365 (4365): perplexity: 71.1639, wsj_loss: 4.2650 ||
04/19 12:11:56 PM: Update 4377: task wsj, batch 377 (4377): perplexity: 71.1149, wsj_loss: 4.2643 ||
04/19 12:12:06 PM: Update 4389: task wsj, batch 389 (4389): perplexity: 71.2339, wsj_loss: 4.2660 ||
04/19 12:12:17 PM: Update 4401: task wsj, batch 401 (4401): perplexity: 71.0266, wsj_loss: 4.2631 ||
04/19 12:12:27 PM: Update 4413: task wsj, batch 413 (4413): perplexity: 71.1020, wsj_loss: 4.2641 ||
04/19 12:12:38 PM: Update 4425: task wsj, batch 425 (4425): perplexity: 71.2079, wsj_loss: 4.2656 ||
04/19 12:12:48 PM: Update 4437: task wsj, batch 437 (4437): perplexity: 71.1909, wsj_loss: 4.2654 ||
04/19 12:12:58 PM: Update 4449: task wsj, batch 449 (4449): perplexity: 71.3672, wsj_loss: 4.2678 ||
04/19 12:13:09 PM: Update 4461: task wsj, batch 461 (4461): perplexity: 71.4930, wsj_loss: 4.2696 ||
04/19 12:13:19 PM: Update 4473: task wsj, batch 473 (4473): perplexity: 71.5355, wsj_loss: 4.2702 ||
04/19 12:13:30 PM: Update 4482: task wsj, batch 482 (4482): perplexity: 71.4944, wsj_loss: 4.2696 ||
04/19 12:13:40 PM: Update 4494: task wsj, batch 494 (4494): perplexity: 71.3589, wsj_loss: 4.2677 ||
04/19 12:13:51 PM: Update 4506: task wsj, batch 506 (4506): perplexity: 71.2172, wsj_loss: 4.2657 ||
04/19 12:14:01 PM: Update 4518: task wsj, batch 518 (4518): perplexity: 70.9805, wsj_loss: 4.2624 ||
04/19 12:14:11 PM: Update 4530: task wsj, batch 530 (4530): perplexity: 70.7635, wsj_loss: 4.2593 ||
04/19 12:14:22 PM: Update 4542: task wsj, batch 542 (4542): perplexity: 70.6466, wsj_loss: 4.2577 ||
04/19 12:14:32 PM: Update 4554: task wsj, batch 554 (4554): perplexity: 70.5216, wsj_loss: 4.2559 ||
04/19 12:14:42 PM: Update 4566: task wsj, batch 566 (4566): perplexity: 70.4834, wsj_loss: 4.2554 ||
04/19 12:14:58 PM: Update 4577: task wsj, batch 577 (4577): perplexity: 70.3137, wsj_loss: 4.2530 ||
04/19 12:15:08 PM: Update 4589: task wsj, batch 589 (4589): perplexity: 70.3571, wsj_loss: 4.2536 ||
04/19 12:15:19 PM: Update 4602: task wsj, batch 602 (4602): perplexity: 70.3200, wsj_loss: 4.2531 ||
04/19 12:15:30 PM: Update 4614: task wsj, batch 614 (4614): perplexity: 70.2116, wsj_loss: 4.2515 ||
04/19 12:15:40 PM: Update 4626: task wsj, batch 626 (4626): perplexity: 70.0916, wsj_loss: 4.2498 ||
04/19 12:15:50 PM: Update 4638: task wsj, batch 638 (4638): perplexity: 69.9517, wsj_loss: 4.2478 ||
04/19 12:16:00 PM: Update 4650: task wsj, batch 650 (4650): perplexity: 69.8881, wsj_loss: 4.2469 ||
04/19 12:16:11 PM: Update 4663: task wsj, batch 663 (4663): perplexity: 69.8422, wsj_loss: 4.2462 ||
04/19 12:16:22 PM: Update 4675: task wsj, batch 675 (4675): perplexity: 69.8083, wsj_loss: 4.2458 ||
04/19 12:16:32 PM: Update 4687: task wsj, batch 687 (4687): perplexity: 69.7865, wsj_loss: 4.2454 ||
04/19 12:16:42 PM: Update 4699: task wsj, batch 699 (4699): perplexity: 69.7029, wsj_loss: 4.2442 ||
04/19 12:16:52 PM: Update 4711: task wsj, batch 711 (4711): perplexity: 69.7384, wsj_loss: 4.2448 ||
04/19 12:17:02 PM: Update 4723: task wsj, batch 723 (4723): perplexity: 69.6764, wsj_loss: 4.2439 ||
04/19 12:17:13 PM: Update 4735: task wsj, batch 735 (4735): perplexity: 69.6759, wsj_loss: 4.2439 ||
04/19 12:17:23 PM: Update 4747: task wsj, batch 747 (4747): perplexity: 69.6241, wsj_loss: 4.2431 ||
04/19 12:17:33 PM: Update 4759: task wsj, batch 759 (4759): perplexity: 69.5742, wsj_loss: 4.2424 ||
04/19 12:17:43 PM: Update 4771: task wsj, batch 771 (4771): perplexity: 69.5714, wsj_loss: 4.2424 ||
04/19 12:17:54 PM: Update 4783: task wsj, batch 783 (4783): perplexity: 69.5250, wsj_loss: 4.2417 ||
04/19 12:18:04 PM: Update 4795: task wsj, batch 795 (4795): perplexity: 69.5546, wsj_loss: 4.2421 ||
04/19 12:18:14 PM: Update 4807: task wsj, batch 807 (4807): perplexity: 69.5830, wsj_loss: 4.2425 ||
04/19 12:18:24 PM: Update 4819: task wsj, batch 819 (4819): perplexity: 69.5645, wsj_loss: 4.2423 ||
04/19 12:18:35 PM: Update 4831: task wsj, batch 831 (4831): perplexity: 69.5066, wsj_loss: 4.2414 ||
04/19 12:18:45 PM: Update 4843: task wsj, batch 843 (4843): perplexity: 69.5336, wsj_loss: 4.2418 ||
04/19 12:18:55 PM: Update 4855: task wsj, batch 855 (4855): perplexity: 69.6079, wsj_loss: 4.2429 ||
04/19 12:19:06 PM: Update 4867: task wsj, batch 867 (4867): perplexity: 69.6048, wsj_loss: 4.2428 ||
04/19 12:19:16 PM: Update 4879: task wsj, batch 879 (4879): perplexity: 69.6173, wsj_loss: 4.2430 ||
04/19 12:19:29 PM: Update 4890: task wsj, batch 890 (4890): perplexity: 69.6767, wsj_loss: 4.2439 ||
04/19 12:19:39 PM: Update 4902: task wsj, batch 902 (4902): perplexity: 69.5209, wsj_loss: 4.2416 ||
04/19 12:19:49 PM: Update 4914: task wsj, batch 914 (4914): perplexity: 69.4251, wsj_loss: 4.2402 ||
04/19 12:19:59 PM: Update 4926: task wsj, batch 926 (4926): perplexity: 69.3594, wsj_loss: 4.2393 ||
04/19 12:20:10 PM: Update 4938: task wsj, batch 938 (4938): perplexity: 69.2906, wsj_loss: 4.2383 ||
04/19 12:20:20 PM: Update 4950: task wsj, batch 950 (4950): perplexity: 69.2068, wsj_loss: 4.2371 ||
04/19 12:20:31 PM: Update 4962: task wsj, batch 962 (4962): perplexity: 69.1420, wsj_loss: 4.2362 ||
04/19 12:20:41 PM: Update 4974: task wsj, batch 974 (4974): perplexity: 69.0332, wsj_loss: 4.2346 ||
04/19 12:20:52 PM: Update 4986: task wsj, batch 986 (4986): perplexity: 68.8655, wsj_loss: 4.2322 ||
04/19 12:21:04 PM: Update 4993: task wsj, batch 993 (4993): perplexity: 68.8195, wsj_loss: 4.2315 ||
04/19 12:21:10 PM: ***** Pass 5000 / Epoch 5 *****
04/19 12:21:10 PM: wsj: trained on 1000 batches, 2.410 epochs
04/19 12:21:10 PM: Validating...
04/19 12:21:14 PM: Batch 15/33: perplexity: 127.3512, wsj_loss: 4.8469 || , for evaluation data
04/19 12:21:19 PM: Best model found for wsj.
04/19 12:21:19 PM: Best model found for micro.
04/19 12:21:19 PM: Best model found for macro.
04/19 12:21:19 PM: Advancing scheduler.
04/19 12:21:19 PM: 	Best macro_avg: 106.757
04/19 12:21:19 PM: 	# bad epochs: 0
04/19 12:21:19 PM: Statistic: wsj_loss
04/19 12:21:19 PM: 	training: 4.231386
04/19 12:21:19 PM: 	validation: 4.670560
04/19 12:21:19 PM: Statistic: macro_avg
04/19 12:21:19 PM: 	validation: 106.757477
04/19 12:21:19 PM: Statistic: micro_avg
04/19 12:21:19 PM: 	validation: 106.757477
04/19 12:21:19 PM: Statistic: wsj_perplexity
04/19 12:21:19 PM: 	training: 68.812553
04/19 12:21:19 PM: 	validation: 106.757477
04/19 12:21:19 PM: global_lr: 0.001000
04/19 12:21:19 PM: Saved files to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0
04/19 12:21:25 PM: Update 5006: task wsj, batch 6 (5006): perplexity: 61.0831, wsj_loss: 4.1122 ||
04/19 12:21:35 PM: Update 5018: task wsj, batch 18 (5018): perplexity: 62.2046, wsj_loss: 4.1304 ||
04/19 12:21:46 PM: Update 5030: task wsj, batch 30 (5030): perplexity: 62.2958, wsj_loss: 4.1319 ||
04/19 12:21:56 PM: Update 5042: task wsj, batch 42 (5042): perplexity: 62.7215, wsj_loss: 4.1387 ||
04/19 12:22:07 PM: Update 5054: task wsj, batch 54 (5054): perplexity: 62.3078, wsj_loss: 4.1321 ||
04/19 12:22:17 PM: Update 5066: task wsj, batch 66 (5066): perplexity: 63.5913, wsj_loss: 4.1525 ||
04/19 12:22:27 PM: Update 5078: task wsj, batch 78 (5078): perplexity: 64.1771, wsj_loss: 4.1616 ||
04/19 12:22:38 PM: Update 5090: task wsj, batch 90 (5090): perplexity: 64.7172, wsj_loss: 4.1700 ||
04/19 12:22:48 PM: Update 5102: task wsj, batch 102 (5102): perplexity: 64.6706, wsj_loss: 4.1693 ||
04/19 12:22:59 PM: Update 5114: task wsj, batch 114 (5114): perplexity: 64.7339, wsj_loss: 4.1703 ||
04/19 12:23:09 PM: Update 5126: task wsj, batch 126 (5126): perplexity: 64.7806, wsj_loss: 4.1710 ||
04/19 12:23:19 PM: Update 5138: task wsj, batch 138 (5138): perplexity: 64.8751, wsj_loss: 4.1725 ||
04/19 12:23:30 PM: Update 5150: task wsj, batch 150 (5150): perplexity: 64.7460, wsj_loss: 4.1705 ||
04/19 12:23:40 PM: Update 5162: task wsj, batch 162 (5162): perplexity: 64.9139, wsj_loss: 4.1731 ||
04/19 12:23:51 PM: Update 5174: task wsj, batch 174 (5174): perplexity: 65.3053, wsj_loss: 4.1791 ||
04/19 12:24:01 PM: Update 5186: task wsj, batch 186 (5186): perplexity: 65.3658, wsj_loss: 4.1800 ||
04/19 12:24:12 PM: Update 5198: task wsj, batch 198 (5198): perplexity: 65.3021, wsj_loss: 4.1790 ||
04/19 12:24:22 PM: Update 5210: task wsj, batch 210 (5210): perplexity: 65.2201, wsj_loss: 4.1778 ||
04/19 12:24:32 PM: Update 5222: task wsj, batch 222 (5222): perplexity: 65.3558, wsj_loss: 4.1798 ||
04/19 12:24:43 PM: Update 5234: task wsj, batch 234 (5234): perplexity: 65.5112, wsj_loss: 4.1822 ||
04/19 12:24:53 PM: Update 5246: task wsj, batch 246 (5246): perplexity: 65.4900, wsj_loss: 4.1819 ||
04/19 12:25:03 PM: Update 5258: task wsj, batch 258 (5258): perplexity: 65.3328, wsj_loss: 4.1795 ||
04/19 12:25:14 PM: Update 5270: task wsj, batch 270 (5270): perplexity: 65.4643, wsj_loss: 4.1815 ||
04/19 12:25:25 PM: Update 5282: task wsj, batch 282 (5282): perplexity: 65.6055, wsj_loss: 4.1837 ||
04/19 12:25:35 PM: Update 5294: task wsj, batch 294 (5294): perplexity: 65.5868, wsj_loss: 4.1834 ||
04/19 12:25:49 PM: Update 5306: task wsj, batch 306 (5306): perplexity: 65.6268, wsj_loss: 4.1840 ||
04/19 12:26:00 PM: Update 5318: task wsj, batch 318 (5318): perplexity: 65.3742, wsj_loss: 4.1801 ||
04/19 12:26:10 PM: Update 5330: task wsj, batch 330 (5330): perplexity: 65.3106, wsj_loss: 4.1792 ||
04/19 12:26:20 PM: Update 5342: task wsj, batch 342 (5342): perplexity: 65.0946, wsj_loss: 4.1758 ||
04/19 12:26:31 PM: Update 5354: task wsj, batch 354 (5354): perplexity: 64.8548, wsj_loss: 4.1722 ||
04/19 12:26:41 PM: Update 5366: task wsj, batch 366 (5366): perplexity: 64.7021, wsj_loss: 4.1698 ||
04/19 12:26:52 PM: Update 5378: task wsj, batch 378 (5378): perplexity: 64.4861, wsj_loss: 4.1664 ||
04/19 12:27:02 PM: Update 5390: task wsj, batch 390 (5390): perplexity: 64.2940, wsj_loss: 4.1635 ||
04/19 12:27:13 PM: Update 5402: task wsj, batch 402 (5402): perplexity: 64.1711, wsj_loss: 4.1616 ||
04/19 12:27:25 PM: Update 5409: task wsj, batch 409 (5409): perplexity: 64.0422, wsj_loss: 4.1595 ||
04/19 12:27:36 PM: Update 5421: task wsj, batch 421 (5421): perplexity: 64.0484, wsj_loss: 4.1596 ||
04/19 12:27:46 PM: Update 5433: task wsj, batch 433 (5433): perplexity: 63.8973, wsj_loss: 4.1573 ||
04/19 12:27:57 PM: Update 5445: task wsj, batch 445 (5445): perplexity: 63.8348, wsj_loss: 4.1563 ||
04/19 12:28:07 PM: Update 5457: task wsj, batch 457 (5457): perplexity: 63.7980, wsj_loss: 4.1557 ||
04/19 12:28:17 PM: Update 5469: task wsj, batch 469 (5469): perplexity: 63.7068, wsj_loss: 4.1543 ||
04/19 12:28:28 PM: Update 5481: task wsj, batch 481 (5481): perplexity: 63.5709, wsj_loss: 4.1522 ||
04/19 12:28:38 PM: Update 5493: task wsj, batch 493 (5493): perplexity: 63.4583, wsj_loss: 4.1504 ||
04/19 12:28:49 PM: Update 5505: task wsj, batch 505 (5505): perplexity: 63.4625, wsj_loss: 4.1504 ||
04/19 12:28:59 PM: Update 5517: task wsj, batch 517 (5517): perplexity: 63.4877, wsj_loss: 4.1508 ||
04/19 12:29:10 PM: Update 5529: task wsj, batch 529 (5529): perplexity: 63.4801, wsj_loss: 4.1507 ||
04/19 12:29:20 PM: Update 5541: task wsj, batch 541 (5541): perplexity: 63.3461, wsj_loss: 4.1486 ||
04/19 12:29:31 PM: Update 5553: task wsj, batch 553 (5553): perplexity: 63.3130, wsj_loss: 4.1481 ||
04/19 12:29:41 PM: Update 5565: task wsj, batch 565 (5565): perplexity: 63.3646, wsj_loss: 4.1489 ||
04/19 12:29:52 PM: Update 5577: task wsj, batch 577 (5577): perplexity: 63.4463, wsj_loss: 4.1502 ||
04/19 12:30:02 PM: Update 5589: task wsj, batch 589 (5589): perplexity: 63.3904, wsj_loss: 4.1493 ||
04/19 12:30:13 PM: Update 5601: task wsj, batch 601 (5601): perplexity: 63.3768, wsj_loss: 4.1491 ||
04/19 12:30:23 PM: Update 5613: task wsj, batch 613 (5613): perplexity: 63.3450, wsj_loss: 4.1486 ||
04/19 12:30:33 PM: Update 5625: task wsj, batch 625 (5625): perplexity: 63.3914, wsj_loss: 4.1493 ||
04/19 12:30:44 PM: Update 5637: task wsj, batch 637 (5637): perplexity: 63.3316, wsj_loss: 4.1484 ||
04/19 12:30:54 PM: Update 5649: task wsj, batch 649 (5649): perplexity: 63.4370, wsj_loss: 4.1500 ||
04/19 12:31:05 PM: Update 5661: task wsj, batch 661 (5661): perplexity: 63.3829, wsj_loss: 4.1492 ||
04/19 12:31:16 PM: Update 5673: task wsj, batch 673 (5673): perplexity: 63.4233, wsj_loss: 4.1498 ||
04/19 12:31:26 PM: Update 5685: task wsj, batch 685 (5685): perplexity: 63.5433, wsj_loss: 4.1517 ||
04/19 12:31:36 PM: Update 5697: task wsj, batch 697 (5697): perplexity: 63.5839, wsj_loss: 4.1524 ||
04/19 12:31:47 PM: Update 5709: task wsj, batch 709 (5709): perplexity: 63.5754, wsj_loss: 4.1522 ||
04/19 12:31:57 PM: Update 5721: task wsj, batch 721 (5721): perplexity: 63.5787, wsj_loss: 4.1523 ||
04/19 12:32:08 PM: Update 5730: task wsj, batch 730 (5730): perplexity: 63.5305, wsj_loss: 4.1515 ||
04/19 12:32:18 PM: Update 5742: task wsj, batch 742 (5742): perplexity: 63.4780, wsj_loss: 4.1507 ||
04/19 12:32:28 PM: Update 5754: task wsj, batch 754 (5754): perplexity: 63.3609, wsj_loss: 4.1488 ||
04/19 12:32:39 PM: Update 5766: task wsj, batch 766 (5766): perplexity: 63.2566, wsj_loss: 4.1472 ||
04/19 12:32:49 PM: Update 5778: task wsj, batch 778 (5778): perplexity: 63.1404, wsj_loss: 4.1454 ||
04/19 12:33:00 PM: Update 5790: task wsj, batch 790 (5790): perplexity: 62.9982, wsj_loss: 4.1431 ||
04/19 12:33:10 PM: Update 5802: task wsj, batch 802 (5802): perplexity: 62.9342, wsj_loss: 4.1421 ||
04/19 12:33:20 PM: Update 5814: task wsj, batch 814 (5814): perplexity: 62.8281, wsj_loss: 4.1404 ||
04/19 12:33:36 PM: Update 5825: task wsj, batch 825 (5825): perplexity: 62.7586, wsj_loss: 4.1393 ||
04/19 12:33:47 PM: Update 5837: task wsj, batch 837 (5837): perplexity: 62.7106, wsj_loss: 4.1385 ||
04/19 12:33:57 PM: Update 5849: task wsj, batch 849 (5849): perplexity: 62.6827, wsj_loss: 4.1381 ||
04/19 12:34:08 PM: Update 5861: task wsj, batch 861 (5861): perplexity: 62.5781, wsj_loss: 4.1364 ||
04/19 12:34:18 PM: Update 5873: task wsj, batch 873 (5873): perplexity: 62.5133, wsj_loss: 4.1354 ||
04/19 12:34:28 PM: Update 5885: task wsj, batch 885 (5885): perplexity: 62.5063, wsj_loss: 4.1353 ||
04/19 12:34:39 PM: Update 5897: task wsj, batch 897 (5897): perplexity: 62.4563, wsj_loss: 4.1345 ||
04/19 12:34:49 PM: Update 5909: task wsj, batch 909 (5909): perplexity: 62.4111, wsj_loss: 4.1337 ||
04/19 12:34:59 PM: Update 5921: task wsj, batch 921 (5921): perplexity: 62.2824, wsj_loss: 4.1317 ||
04/19 12:35:09 PM: Update 5933: task wsj, batch 933 (5933): perplexity: 62.2800, wsj_loss: 4.1316 ||
04/19 12:35:20 PM: Update 5945: task wsj, batch 945 (5945): perplexity: 62.2390, wsj_loss: 4.1310 ||
04/19 12:35:30 PM: Update 5957: task wsj, batch 957 (5957): perplexity: 62.2360, wsj_loss: 4.1309 ||
04/19 12:35:40 PM: Update 5969: task wsj, batch 969 (5969): perplexity: 62.1919, wsj_loss: 4.1302 ||
04/19 12:35:50 PM: Update 5981: task wsj, batch 981 (5981): perplexity: 62.1860, wsj_loss: 4.1301 ||
04/19 12:36:01 PM: Update 5993: task wsj, batch 993 (5993): perplexity: 62.1456, wsj_loss: 4.1295 ||
04/19 12:36:07 PM: ***** Pass 6000 / Epoch 6 *****
04/19 12:36:07 PM: wsj: trained on 1000 batches, 2.410 epochs
04/19 12:36:07 PM: Validating...
04/19 12:36:11 PM: Batch 15/33: perplexity: 130.2754, wsj_loss: 4.8697 || , for evaluation data
04/19 12:36:16 PM: Advancing scheduler.
04/19 12:36:16 PM: 	Best macro_avg: 106.757
04/19 12:36:16 PM: 	# bad epochs: 1
04/19 12:36:16 PM: Statistic: wsj_loss
04/19 12:36:16 PM: 	training: 4.129456
04/19 12:36:16 PM: 	validation: 4.692545
04/19 12:36:16 PM: Statistic: macro_avg
04/19 12:36:16 PM: 	validation: 109.130572
04/19 12:36:16 PM: Statistic: micro_avg
04/19 12:36:16 PM: 	validation: 109.130572
04/19 12:36:16 PM: Statistic: wsj_perplexity
04/19 12:36:16 PM: 	training: 62.144135
04/19 12:36:16 PM: 	validation: 109.130572
04/19 12:36:16 PM: global_lr: 0.001000
04/19 12:36:17 PM: Saved files to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0
04/19 12:36:21 PM: Update 6005: task wsj, batch 5 (6005): perplexity: 60.6556, wsj_loss: 4.1052 ||
04/19 12:36:31 PM: Update 6017: task wsj, batch 17 (6017): perplexity: 63.1429, wsj_loss: 4.1454 ||
04/19 12:36:42 PM: Update 6029: task wsj, batch 29 (6029): perplexity: 63.4058, wsj_loss: 4.1496 ||
04/19 12:36:52 PM: Update 6041: task wsj, batch 41 (6041): perplexity: 62.7062, wsj_loss: 4.1385 ||
04/19 12:37:03 PM: Update 6053: task wsj, batch 53 (6053): perplexity: 62.5557, wsj_loss: 4.1361 ||
04/19 12:37:13 PM: Update 6065: task wsj, batch 65 (6065): perplexity: 62.1664, wsj_loss: 4.1298 ||
04/19 12:37:23 PM: Update 6077: task wsj, batch 77 (6077): perplexity: 62.7961, wsj_loss: 4.1399 ||
04/19 12:37:34 PM: Update 6089: task wsj, batch 89 (6089): perplexity: 62.6066, wsj_loss: 4.1369 ||
04/19 12:37:44 PM: Update 6101: task wsj, batch 101 (6101): perplexity: 62.2702, wsj_loss: 4.1315 ||
04/19 12:37:54 PM: Update 6113: task wsj, batch 113 (6113): perplexity: 62.5251, wsj_loss: 4.1356 ||
04/19 12:38:05 PM: Update 6125: task wsj, batch 125 (6125): perplexity: 62.4560, wsj_loss: 4.1345 ||
04/19 12:38:15 PM: Update 6137: task wsj, batch 137 (6137): perplexity: 62.5873, wsj_loss: 4.1366 ||
04/19 12:38:26 PM: Update 6146: task wsj, batch 146 (6146): perplexity: 62.1403, wsj_loss: 4.1294 ||
04/19 12:38:37 PM: Update 6158: task wsj, batch 158 (6158): perplexity: 61.7681, wsj_loss: 4.1234 ||
04/19 12:38:47 PM: Update 6170: task wsj, batch 170 (6170): perplexity: 60.9930, wsj_loss: 4.1108 ||
04/19 12:38:57 PM: Update 6182: task wsj, batch 182 (6182): perplexity: 60.7096, wsj_loss: 4.1061 ||
04/19 12:39:08 PM: Update 6194: task wsj, batch 194 (6194): perplexity: 60.3960, wsj_loss: 4.1009 ||
04/19 12:39:18 PM: Update 6206: task wsj, batch 206 (6206): perplexity: 60.0517, wsj_loss: 4.0952 ||
04/19 12:39:28 PM: Update 6218: task wsj, batch 218 (6218): perplexity: 59.7087, wsj_loss: 4.0895 ||
04/19 12:39:39 PM: Update 6230: task wsj, batch 230 (6230): perplexity: 59.5952, wsj_loss: 4.0876 ||
04/19 12:39:55 PM: Update 6241: task wsj, batch 241 (6241): perplexity: 59.3985, wsj_loss: 4.0843 ||
04/19 12:40:05 PM: Update 6253: task wsj, batch 253 (6253): perplexity: 59.1581, wsj_loss: 4.0802 ||
04/19 12:40:15 PM: Update 6265: task wsj, batch 265 (6265): perplexity: 59.1304, wsj_loss: 4.0797 ||
04/19 12:40:25 PM: Update 6277: task wsj, batch 277 (6277): perplexity: 58.9834, wsj_loss: 4.0773 ||
04/19 12:40:36 PM: Update 6289: task wsj, batch 289 (6289): perplexity: 58.8507, wsj_loss: 4.0750 ||
04/19 12:40:46 PM: Update 6301: task wsj, batch 301 (6301): perplexity: 58.7281, wsj_loss: 4.0729 ||
04/19 12:40:56 PM: Update 6313: task wsj, batch 313 (6313): perplexity: 58.7201, wsj_loss: 4.0728 ||
04/19 12:41:07 PM: Update 6325: task wsj, batch 325 (6325): perplexity: 58.6062, wsj_loss: 4.0708 ||
04/19 12:41:17 PM: Update 6337: task wsj, batch 337 (6337): perplexity: 58.6125, wsj_loss: 4.0709 ||
04/19 12:41:27 PM: Update 6349: task wsj, batch 349 (6349): perplexity: 58.5945, wsj_loss: 4.0706 ||
04/19 12:41:37 PM: Update 6361: task wsj, batch 361 (6361): perplexity: 58.6088, wsj_loss: 4.0709 ||
04/19 12:41:48 PM: Update 6373: task wsj, batch 373 (6373): perplexity: 58.5647, wsj_loss: 4.0701 ||
04/19 12:41:58 PM: Update 6385: task wsj, batch 385 (6385): perplexity: 58.5139, wsj_loss: 4.0693 ||
04/19 12:42:08 PM: Update 6397: task wsj, batch 397 (6397): perplexity: 58.4958, wsj_loss: 4.0690 ||
04/19 12:42:19 PM: Update 6409: task wsj, batch 409 (6409): perplexity: 58.5439, wsj_loss: 4.0698 ||
04/19 12:42:29 PM: Update 6421: task wsj, batch 421 (6421): perplexity: 58.4857, wsj_loss: 4.0688 ||
04/19 12:42:40 PM: Update 6433: task wsj, batch 433 (6433): perplexity: 58.5099, wsj_loss: 4.0692 ||
04/19 12:42:50 PM: Update 6445: task wsj, batch 445 (6445): perplexity: 58.5702, wsj_loss: 4.0702 ||
04/19 12:43:00 PM: Update 6457: task wsj, batch 457 (6457): perplexity: 58.5827, wsj_loss: 4.0704 ||
04/19 12:43:10 PM: Update 6469: task wsj, batch 469 (6469): perplexity: 58.5683, wsj_loss: 4.0702 ||
04/19 12:43:21 PM: Update 6481: task wsj, batch 481 (6481): perplexity: 58.6448, wsj_loss: 4.0715 ||
04/19 12:43:31 PM: Update 6493: task wsj, batch 493 (6493): perplexity: 58.7630, wsj_loss: 4.0735 ||
04/19 12:43:41 PM: Update 6505: task wsj, batch 505 (6505): perplexity: 58.8077, wsj_loss: 4.0743 ||
04/19 12:43:51 PM: Update 6517: task wsj, batch 517 (6517): perplexity: 58.8143, wsj_loss: 4.0744 ||
04/19 12:44:02 PM: Update 6529: task wsj, batch 529 (6529): perplexity: 58.9026, wsj_loss: 4.0759 ||
04/19 12:44:12 PM: Update 6541: task wsj, batch 541 (6541): perplexity: 58.9209, wsj_loss: 4.0762 ||
04/19 12:44:22 PM: Update 6553: task wsj, batch 553 (6553): perplexity: 58.9743, wsj_loss: 4.0771 ||
04/19 12:44:33 PM: Update 6562: task wsj, batch 562 (6562): perplexity: 58.9486, wsj_loss: 4.0767 ||
04/19 12:44:43 PM: Update 6574: task wsj, batch 574 (6574): perplexity: 58.8765, wsj_loss: 4.0754 ||
04/19 12:44:53 PM: Update 6586: task wsj, batch 586 (6586): perplexity: 58.7585, wsj_loss: 4.0734 ||
04/19 12:45:04 PM: Update 6598: task wsj, batch 598 (6598): perplexity: 58.6546, wsj_loss: 4.0717 ||
04/19 12:45:14 PM: Update 6610: task wsj, batch 610 (6610): perplexity: 58.5359, wsj_loss: 4.0696 ||
04/19 12:45:24 PM: Update 6622: task wsj, batch 622 (6622): perplexity: 58.4575, wsj_loss: 4.0683 ||
04/19 12:45:34 PM: Update 6634: task wsj, batch 634 (6634): perplexity: 58.2907, wsj_loss: 4.0654 ||
04/19 12:45:45 PM: Update 6646: task wsj, batch 646 (6646): perplexity: 58.1778, wsj_loss: 4.0635 ||
04/19 12:46:01 PM: Update 6657: task wsj, batch 657 (6657): perplexity: 58.0900, wsj_loss: 4.0620 ||
04/19 12:46:11 PM: Update 6669: task wsj, batch 669 (6669): perplexity: 58.0105, wsj_loss: 4.0606 ||
04/19 12:46:21 PM: Update 6681: task wsj, batch 681 (6681): perplexity: 57.9068, wsj_loss: 4.0588 ||
04/19 12:46:32 PM: Update 6693: task wsj, batch 693 (6693): perplexity: 57.8774, wsj_loss: 4.0583 ||
04/19 12:46:42 PM: Update 6705: task wsj, batch 705 (6705): perplexity: 57.8488, wsj_loss: 4.0578 ||
04/19 12:46:53 PM: Update 6717: task wsj, batch 717 (6717): perplexity: 57.7990, wsj_loss: 4.0570 ||
04/19 12:47:03 PM: Update 6729: task wsj, batch 729 (6729): perplexity: 57.7725, wsj_loss: 4.0565 ||
04/19 12:47:14 PM: Update 6741: task wsj, batch 741 (6741): perplexity: 57.7845, wsj_loss: 4.0567 ||
04/19 12:47:24 PM: Update 6753: task wsj, batch 753 (6753): perplexity: 57.7980, wsj_loss: 4.0570 ||
04/19 12:47:35 PM: Update 6765: task wsj, batch 765 (6765): perplexity: 57.8305, wsj_loss: 4.0575 ||
04/19 12:47:45 PM: Update 6777: task wsj, batch 777 (6777): perplexity: 57.7282, wsj_loss: 4.0557 ||
04/19 12:47:55 PM: Update 6789: task wsj, batch 789 (6789): perplexity: 57.6942, wsj_loss: 4.0552 ||
04/19 12:48:06 PM: Update 6801: task wsj, batch 801 (6801): perplexity: 57.7318, wsj_loss: 4.0558 ||
04/19 12:48:16 PM: Update 6813: task wsj, batch 813 (6813): perplexity: 57.7368, wsj_loss: 4.0559 ||
04/19 12:48:26 PM: Update 6825: task wsj, batch 825 (6825): perplexity: 57.7892, wsj_loss: 4.0568 ||
04/19 12:48:37 PM: Update 6837: task wsj, batch 837 (6837): perplexity: 57.7432, wsj_loss: 4.0560 ||
04/19 12:48:47 PM: Update 6849: task wsj, batch 849 (6849): perplexity: 57.7389, wsj_loss: 4.0559 ||
04/19 12:48:58 PM: Update 6861: task wsj, batch 861 (6861): perplexity: 57.7190, wsj_loss: 4.0556 ||
04/19 12:49:08 PM: Update 6873: task wsj, batch 873 (6873): perplexity: 57.6903, wsj_loss: 4.0551 ||
04/19 12:49:18 PM: Update 6885: task wsj, batch 885 (6885): perplexity: 57.6943, wsj_loss: 4.0552 ||
04/19 12:49:29 PM: Update 6897: task wsj, batch 897 (6897): perplexity: 57.7173, wsj_loss: 4.0556 ||
04/19 12:49:39 PM: Update 6909: task wsj, batch 909 (6909): perplexity: 57.7180, wsj_loss: 4.0556 ||
04/19 12:49:50 PM: Update 6921: task wsj, batch 921 (6921): perplexity: 57.6964, wsj_loss: 4.0552 ||
04/19 12:50:00 PM: Update 6933: task wsj, batch 933 (6933): perplexity: 57.6590, wsj_loss: 4.0545 ||
04/19 12:50:10 PM: Update 6945: task wsj, batch 945 (6945): perplexity: 57.6263, wsj_loss: 4.0540 ||
04/19 12:50:21 PM: Update 6957: task wsj, batch 957 (6957): perplexity: 57.6597, wsj_loss: 4.0546 ||
04/19 12:50:31 PM: Update 6969: task wsj, batch 969 (6969): perplexity: 57.6985, wsj_loss: 4.0552 ||
04/19 12:50:42 PM: Update 6978: task wsj, batch 978 (6978): perplexity: 57.6809, wsj_loss: 4.0549 ||
04/19 12:50:52 PM: Update 6990: task wsj, batch 990 (6990): perplexity: 57.5859, wsj_loss: 4.0533 ||
04/19 12:51:01 PM: ***** Pass 7000 / Epoch 7 *****
04/19 12:51:01 PM: wsj: trained on 1000 batches, 2.410 epochs
04/19 12:51:01 PM: Validating...
04/19 12:51:02 PM: Batch 5/33: perplexity: 157.8779, wsj_loss: 5.0618 || , for evaluation data
04/19 12:51:10 PM: Advancing scheduler.
04/19 12:51:10 PM: 	Best macro_avg: 106.757
04/19 12:51:10 PM: 	# bad epochs: 2
04/19 12:51:10 PM: Statistic: wsj_loss
04/19 12:51:10 PM: 	training: 4.052890
04/19 12:51:10 PM: 	validation: 4.695817
04/19 12:51:10 PM: Statistic: macro_avg
04/19 12:51:10 PM: 	validation: 109.488263
04/19 12:51:10 PM: Statistic: micro_avg
04/19 12:51:10 PM: 	validation: 109.488263
04/19 12:51:10 PM: Statistic: wsj_perplexity
04/19 12:51:10 PM: 	training: 57.563603
04/19 12:51:10 PM: 	validation: 109.488263
04/19 12:51:10 PM: global_lr: 0.001000
04/19 12:51:10 PM: Saved files to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0
04/19 12:51:13 PM: Update 7003: task wsj, batch 3 (7003): perplexity: 51.8975, wsj_loss: 3.9493 ||
04/19 12:51:23 PM: Update 7015: task wsj, batch 15 (7015): perplexity: 52.1923, wsj_loss: 3.9549 ||
04/19 12:51:33 PM: Update 7027: task wsj, batch 27 (7027): perplexity: 51.7049, wsj_loss: 3.9456 ||
04/19 12:51:44 PM: Update 7039: task wsj, batch 39 (7039): perplexity: 51.5575, wsj_loss: 3.9427 ||
04/19 12:51:54 PM: Update 7051: task wsj, batch 51 (7051): perplexity: 51.5495, wsj_loss: 3.9425 ||
04/19 12:52:05 PM: Update 7063: task wsj, batch 63 (7063): perplexity: 51.0957, wsj_loss: 3.9337 ||
04/19 12:52:20 PM: Update 7073: task wsj, batch 73 (7073): perplexity: 51.3800, wsj_loss: 3.9392 ||
04/19 12:52:31 PM: Update 7085: task wsj, batch 85 (7085): perplexity: 51.7315, wsj_loss: 3.9461 ||
04/19 12:52:41 PM: Update 7097: task wsj, batch 97 (7097): perplexity: 51.9480, wsj_loss: 3.9502 ||
04/19 12:52:52 PM: Update 7109: task wsj, batch 109 (7109): perplexity: 52.0698, wsj_loss: 3.9526 ||
04/19 12:53:02 PM: Update 7121: task wsj, batch 121 (7121): perplexity: 52.3387, wsj_loss: 3.9577 ||
04/19 12:53:12 PM: Update 7133: task wsj, batch 133 (7133): perplexity: 52.5020, wsj_loss: 3.9609 ||
04/19 12:53:23 PM: Update 7145: task wsj, batch 145 (7145): perplexity: 52.7147, wsj_loss: 3.9649 ||
04/19 12:53:33 PM: Update 7157: task wsj, batch 157 (7157): perplexity: 52.5513, wsj_loss: 3.9618 ||
04/19 12:53:44 PM: Update 7169: task wsj, batch 169 (7169): perplexity: 52.4000, wsj_loss: 3.9589 ||
04/19 12:53:54 PM: Update 7181: task wsj, batch 181 (7181): perplexity: 52.5267, wsj_loss: 3.9613 ||
04/19 12:54:05 PM: Update 7193: task wsj, batch 193 (7193): perplexity: 52.7533, wsj_loss: 3.9656 ||
04/19 12:54:15 PM: Update 7205: task wsj, batch 205 (7205): perplexity: 52.8183, wsj_loss: 3.9669 ||
04/19 12:54:25 PM: Update 7217: task wsj, batch 217 (7217): perplexity: 52.9789, wsj_loss: 3.9699 ||
04/19 12:54:36 PM: Update 7229: task wsj, batch 229 (7229): perplexity: 53.1487, wsj_loss: 3.9731 ||
04/19 12:54:46 PM: Update 7241: task wsj, batch 241 (7241): perplexity: 53.1551, wsj_loss: 3.9732 ||
04/19 12:54:57 PM: Update 7253: task wsj, batch 253 (7253): perplexity: 53.2781, wsj_loss: 3.9755 ||
04/19 12:55:07 PM: Update 7265: task wsj, batch 265 (7265): perplexity: 53.3994, wsj_loss: 3.9778 ||
04/19 12:55:17 PM: Update 7277: task wsj, batch 277 (7277): perplexity: 53.4890, wsj_loss: 3.9795 ||
04/19 12:55:27 PM: Update 7289: task wsj, batch 289 (7289): perplexity: 53.4911, wsj_loss: 3.9795 ||
04/19 12:55:38 PM: Update 7301: task wsj, batch 301 (7301): perplexity: 53.7948, wsj_loss: 3.9852 ||
04/19 12:55:48 PM: Update 7313: task wsj, batch 313 (7313): perplexity: 53.8102, wsj_loss: 3.9855 ||
04/19 12:55:59 PM: Update 7325: task wsj, batch 325 (7325): perplexity: 53.9826, wsj_loss: 3.9887 ||
04/19 12:56:09 PM: Update 7337: task wsj, batch 337 (7337): perplexity: 53.9569, wsj_loss: 3.9882 ||
04/19 12:56:19 PM: Update 7349: task wsj, batch 349 (7349): perplexity: 53.9662, wsj_loss: 3.9884 ||
04/19 12:56:29 PM: Update 7361: task wsj, batch 361 (7361): perplexity: 54.1209, wsj_loss: 3.9912 ||
04/19 12:56:40 PM: Update 7373: task wsj, batch 373 (7373): perplexity: 54.1994, wsj_loss: 3.9927 ||
04/19 12:56:50 PM: Update 7385: task wsj, batch 385 (7385): perplexity: 54.3970, wsj_loss: 3.9963 ||
04/19 12:57:00 PM: Update 7394: task wsj, batch 394 (7394): perplexity: 54.3280, wsj_loss: 3.9950 ||
04/19 12:57:11 PM: Update 7406: task wsj, batch 406 (7406): perplexity: 54.2766, wsj_loss: 3.9941 ||
04/19 12:57:21 PM: Update 7418: task wsj, batch 418 (7418): perplexity: 54.2380, wsj_loss: 3.9934 ||
04/19 12:57:31 PM: Update 7430: task wsj, batch 430 (7430): perplexity: 53.9644, wsj_loss: 3.9883 ||
04/19 12:57:42 PM: Update 7442: task wsj, batch 442 (7442): perplexity: 53.8970, wsj_loss: 3.9871 ||
04/19 12:57:52 PM: Update 7454: task wsj, batch 454 (7454): perplexity: 53.7888, wsj_loss: 3.9851 ||
04/19 12:58:02 PM: Update 7466: task wsj, batch 466 (7466): perplexity: 53.7269, wsj_loss: 3.9839 ||
04/19 12:58:12 PM: Update 7478: task wsj, batch 478 (7478): perplexity: 53.6541, wsj_loss: 3.9826 ||
04/19 12:58:28 PM: Update 7489: task wsj, batch 489 (7489): perplexity: 53.5588, wsj_loss: 3.9808 ||
04/19 12:58:39 PM: Update 7501: task wsj, batch 501 (7501): perplexity: 53.4964, wsj_loss: 3.9796 ||
04/19 12:58:49 PM: Update 7513: task wsj, batch 513 (7513): perplexity: 53.4982, wsj_loss: 3.9796 ||
04/19 12:58:59 PM: Update 7525: task wsj, batch 525 (7525): perplexity: 53.3871, wsj_loss: 3.9776 ||
04/19 12:59:09 PM: Update 7537: task wsj, batch 537 (7537): perplexity: 53.2853, wsj_loss: 3.9757 ||
04/19 12:59:19 PM: Update 7549: task wsj, batch 549 (7549): perplexity: 53.2307, wsj_loss: 3.9746 ||
04/19 12:59:30 PM: Update 7561: task wsj, batch 561 (7561): perplexity: 53.2262, wsj_loss: 3.9746 ||
04/19 12:59:40 PM: Update 7573: task wsj, batch 573 (7573): perplexity: 53.1564, wsj_loss: 3.9732 ||
04/19 12:59:50 PM: Update 7585: task wsj, batch 585 (7585): perplexity: 53.1497, wsj_loss: 3.9731 ||
04/19 01:00:01 PM: Update 7597: task wsj, batch 597 (7597): perplexity: 53.1490, wsj_loss: 3.9731 ||
04/19 01:00:11 PM: Update 7609: task wsj, batch 609 (7609): perplexity: 53.1267, wsj_loss: 3.9727 ||
04/19 01:00:21 PM: Update 7621: task wsj, batch 621 (7621): perplexity: 53.1514, wsj_loss: 3.9731 ||
04/19 01:00:32 PM: Update 7633: task wsj, batch 633 (7633): perplexity: 53.1388, wsj_loss: 3.9729 ||
04/19 01:00:42 PM: Update 7645: task wsj, batch 645 (7645): perplexity: 53.1869, wsj_loss: 3.9738 ||
04/19 01:00:52 PM: Update 7657: task wsj, batch 657 (7657): perplexity: 53.2357, wsj_loss: 3.9747 ||
04/19 01:01:03 PM: Update 7669: task wsj, batch 669 (7669): perplexity: 53.2275, wsj_loss: 3.9746 ||
04/19 01:01:13 PM: Update 7681: task wsj, batch 681 (7681): perplexity: 53.2970, wsj_loss: 3.9759 ||
04/19 01:01:24 PM: Update 7693: task wsj, batch 693 (7693): perplexity: 53.2722, wsj_loss: 3.9754 ||
04/19 01:01:34 PM: Update 7705: task wsj, batch 705 (7705): perplexity: 53.2987, wsj_loss: 3.9759 ||
04/19 01:01:44 PM: Update 7717: task wsj, batch 717 (7717): perplexity: 53.3908, wsj_loss: 3.9776 ||
04/19 01:01:54 PM: Update 7729: task wsj, batch 729 (7729): perplexity: 53.4367, wsj_loss: 3.9785 ||
04/19 01:02:05 PM: Update 7741: task wsj, batch 741 (7741): perplexity: 53.4784, wsj_loss: 3.9793 ||
04/19 01:02:15 PM: Update 7753: task wsj, batch 753 (7753): perplexity: 53.4817, wsj_loss: 3.9793 ||
04/19 01:02:25 PM: Update 7765: task wsj, batch 765 (7765): perplexity: 53.5652, wsj_loss: 3.9809 ||
04/19 01:02:36 PM: Update 7777: task wsj, batch 777 (7777): perplexity: 53.5910, wsj_loss: 3.9814 ||
04/19 01:02:46 PM: Update 7789: task wsj, batch 789 (7789): perplexity: 53.6393, wsj_loss: 3.9823 ||
04/19 01:02:56 PM: Update 7801: task wsj, batch 801 (7801): perplexity: 53.5978, wsj_loss: 3.9815 ||
04/19 01:03:07 PM: Update 7810: task wsj, batch 810 (7810): perplexity: 53.5697, wsj_loss: 3.9810 ||
04/19 01:03:18 PM: Update 7822: task wsj, batch 822 (7822): perplexity: 53.5163, wsj_loss: 3.9800 ||
04/19 01:03:28 PM: Update 7834: task wsj, batch 834 (7834): perplexity: 53.4151, wsj_loss: 3.9781 ||
04/19 01:03:38 PM: Update 7846: task wsj, batch 846 (7846): perplexity: 53.3292, wsj_loss: 3.9765 ||
04/19 01:03:49 PM: Update 7858: task wsj, batch 858 (7858): perplexity: 53.2769, wsj_loss: 3.9755 ||
04/19 01:03:59 PM: Update 7870: task wsj, batch 870 (7870): perplexity: 53.2236, wsj_loss: 3.9745 ||
04/19 01:04:09 PM: Update 7882: task wsj, batch 882 (7882): perplexity: 53.1836, wsj_loss: 3.9737 ||
04/19 01:04:20 PM: Update 7894: task wsj, batch 894 (7894): perplexity: 53.0649, wsj_loss: 3.9715 ||
04/19 01:04:37 PM: Update 7905: task wsj, batch 905 (7905): perplexity: 53.0508, wsj_loss: 3.9712 ||
04/19 01:04:47 PM: Update 7917: task wsj, batch 917 (7917): perplexity: 52.9788, wsj_loss: 3.9699 ||
04/19 01:04:58 PM: Update 7929: task wsj, batch 929 (7929): perplexity: 53.0009, wsj_loss: 3.9703 ||
04/19 01:05:08 PM: Update 7941: task wsj, batch 941 (7941): perplexity: 52.9706, wsj_loss: 3.9697 ||
04/19 01:05:19 PM: Update 7953: task wsj, batch 953 (7953): perplexity: 52.9237, wsj_loss: 3.9689 ||
04/19 01:05:29 PM: Update 7965: task wsj, batch 965 (7965): perplexity: 52.8891, wsj_loss: 3.9682 ||
04/19 01:05:40 PM: Update 7977: task wsj, batch 977 (7977): perplexity: 52.8517, wsj_loss: 3.9675 ||
04/19 01:05:50 PM: Update 7989: task wsj, batch 989 (7989): perplexity: 52.8237, wsj_loss: 3.9670 ||
04/19 01:05:59 PM: ***** Pass 8000 / Epoch 8 *****
04/19 01:05:59 PM: wsj: trained on 1000 batches, 2.410 epochs
04/19 01:05:59 PM: Validating...
04/19 01:06:00 PM: Batch 2/33: perplexity: 131.1466, wsj_loss: 4.8763 || , for evaluation data
04/19 01:06:08 PM: Advancing scheduler.
04/19 01:06:08 PM: 	Best macro_avg: 106.757
04/19 01:06:08 PM: 	# bad epochs: 3
04/19 01:06:08 PM: Statistic: wsj_loss
04/19 01:06:08 PM: 	training: 3.966486
04/19 01:06:08 PM: 	validation: 4.732899
04/19 01:06:08 PM: Statistic: macro_avg
04/19 01:06:08 PM: 	validation: 113.624452
04/19 01:06:08 PM: Statistic: micro_avg
04/19 01:06:08 PM: 	validation: 113.624452
04/19 01:06:08 PM: Statistic: wsj_perplexity
04/19 01:06:08 PM: 	training: 52.798664
04/19 01:06:08 PM: 	validation: 113.624452
04/19 01:06:08 PM: global_lr: 0.001000
04/19 01:06:09 PM: Saved files to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0
04/19 01:06:10 PM: Update 8002: task wsj, batch 2 (8002): perplexity: 50.6673, wsj_loss: 3.9253 ||
04/19 01:06:21 PM: Update 8014: task wsj, batch 14 (8014): perplexity: 50.0966, wsj_loss: 3.9140 ||
04/19 01:06:31 PM: Update 8026: task wsj, batch 26 (8026): perplexity: 50.8972, wsj_loss: 3.9298 ||
04/19 01:06:42 PM: Update 8038: task wsj, batch 38 (8038): perplexity: 50.0617, wsj_loss: 3.9133 ||
04/19 01:06:52 PM: Update 8050: task wsj, batch 50 (8050): perplexity: 51.1701, wsj_loss: 3.9352 ||
04/19 01:07:03 PM: Update 8062: task wsj, batch 62 (8062): perplexity: 51.3421, wsj_loss: 3.9385 ||
04/19 01:07:13 PM: Update 8074: task wsj, batch 74 (8074): perplexity: 51.1824, wsj_loss: 3.9354 ||
04/19 01:07:24 PM: Update 8086: task wsj, batch 86 (8086): perplexity: 51.5178, wsj_loss: 3.9419 ||
04/19 01:07:34 PM: Update 8098: task wsj, batch 98 (8098): perplexity: 51.2080, wsj_loss: 3.9359 ||
04/19 01:07:44 PM: Update 8110: task wsj, batch 110 (8110): perplexity: 51.4886, wsj_loss: 3.9414 ||
04/19 01:07:55 PM: Update 8122: task wsj, batch 122 (8122): perplexity: 52.0124, wsj_loss: 3.9515 ||
04/19 01:08:05 PM: Update 8134: task wsj, batch 134 (8134): perplexity: 52.4027, wsj_loss: 3.9590 ||
04/19 01:08:16 PM: Update 8146: task wsj, batch 146 (8146): perplexity: 52.4228, wsj_loss: 3.9593 ||
04/19 01:08:26 PM: Update 8158: task wsj, batch 158 (8158): perplexity: 52.7164, wsj_loss: 3.9649 ||
04/19 01:08:37 PM: Update 8170: task wsj, batch 170 (8170): perplexity: 52.8548, wsj_loss: 3.9675 ||
04/19 01:08:47 PM: Update 8182: task wsj, batch 182 (8182): perplexity: 52.7484, wsj_loss: 3.9655 ||
04/19 01:08:58 PM: Update 8194: task wsj, batch 194 (8194): perplexity: 52.8780, wsj_loss: 3.9680 ||
04/19 01:09:08 PM: Update 8206: task wsj, batch 206 (8206): perplexity: 52.9705, wsj_loss: 3.9697 ||
04/19 01:09:21 PM: Update 8218: task wsj, batch 218 (8218): perplexity: 53.0161, wsj_loss: 3.9706 ||
04/19 01:09:32 PM: Update 8230: task wsj, batch 230 (8230): perplexity: 52.6220, wsj_loss: 3.9631 ||
04/19 01:09:42 PM: Update 8242: task wsj, batch 242 (8242): perplexity: 52.3941, wsj_loss: 3.9588 ||
04/19 01:09:53 PM: Update 8254: task wsj, batch 254 (8254): perplexity: 52.2592, wsj_loss: 3.9562 ||
04/19 01:10:03 PM: Update 8266: task wsj, batch 266 (8266): perplexity: 51.9550, wsj_loss: 3.9504 ||
04/19 01:10:13 PM: Update 8278: task wsj, batch 278 (8278): perplexity: 51.7760, wsj_loss: 3.9469 ||
04/19 01:10:24 PM: Update 8290: task wsj, batch 290 (8290): perplexity: 51.5316, wsj_loss: 3.9422 ||
04/19 01:10:34 PM: Update 8302: task wsj, batch 302 (8302): perplexity: 51.3192, wsj_loss: 3.9381 ||
04/19 01:10:45 PM: Update 8314: task wsj, batch 314 (8314): perplexity: 51.3828, wsj_loss: 3.9393 ||
04/19 01:11:00 PM: Update 8321: task wsj, batch 321 (8321): perplexity: 51.3601, wsj_loss: 3.9389 ||
04/19 01:11:11 PM: Update 8333: task wsj, batch 333 (8333): perplexity: 51.3244, wsj_loss: 3.9382 ||
04/19 01:11:21 PM: Update 8345: task wsj, batch 345 (8345): perplexity: 51.2724, wsj_loss: 3.9372 ||
04/19 01:11:32 PM: Update 8357: task wsj, batch 357 (8357): perplexity: 51.2553, wsj_loss: 3.9368 ||
04/19 01:11:42 PM: Update 8369: task wsj, batch 369 (8369): perplexity: 51.2029, wsj_loss: 3.9358 ||
04/19 01:11:53 PM: Update 8381: task wsj, batch 381 (8381): perplexity: 51.0488, wsj_loss: 3.9328 ||
04/19 01:12:03 PM: Update 8393: task wsj, batch 393 (8393): perplexity: 50.9541, wsj_loss: 3.9309 ||
04/19 01:12:14 PM: Update 8405: task wsj, batch 405 (8405): perplexity: 50.8605, wsj_loss: 3.9291 ||
04/19 01:12:24 PM: Update 8417: task wsj, batch 417 (8417): perplexity: 50.9168, wsj_loss: 3.9302 ||
04/19 01:12:34 PM: Update 8429: task wsj, batch 429 (8429): perplexity: 50.9128, wsj_loss: 3.9301 ||
04/19 01:12:45 PM: Update 8441: task wsj, batch 441 (8441): perplexity: 50.9475, wsj_loss: 3.9308 ||
04/19 01:12:55 PM: Update 8453: task wsj, batch 453 (8453): perplexity: 50.9953, wsj_loss: 3.9317 ||
04/19 01:13:06 PM: Update 8465: task wsj, batch 465 (8465): perplexity: 50.9739, wsj_loss: 3.9313 ||
04/19 01:13:16 PM: Update 8477: task wsj, batch 477 (8477): perplexity: 50.9343, wsj_loss: 3.9305 ||
04/19 01:13:27 PM: Update 8489: task wsj, batch 489 (8489): perplexity: 50.9576, wsj_loss: 3.9310 ||
04/19 01:13:37 PM: Update 8501: task wsj, batch 501 (8501): perplexity: 50.9501, wsj_loss: 3.9308 ||
04/19 01:13:47 PM: Update 8513: task wsj, batch 513 (8513): perplexity: 50.8594, wsj_loss: 3.9291 ||
04/19 01:13:58 PM: Update 8525: task wsj, batch 525 (8525): perplexity: 50.8999, wsj_loss: 3.9299 ||
04/19 01:14:08 PM: Update 8537: task wsj, batch 537 (8537): perplexity: 50.9466, wsj_loss: 3.9308 ||
04/19 01:14:19 PM: Update 8549: task wsj, batch 549 (8549): perplexity: 50.9934, wsj_loss: 3.9317 ||
04/19 01:14:29 PM: Update 8561: task wsj, batch 561 (8561): perplexity: 51.0377, wsj_loss: 3.9326 ||
04/19 01:14:40 PM: Update 8573: task wsj, batch 573 (8573): perplexity: 50.9994, wsj_loss: 3.9318 ||
04/19 01:14:50 PM: Update 8585: task wsj, batch 585 (8585): perplexity: 51.0275, wsj_loss: 3.9324 ||
04/19 01:15:01 PM: Update 8597: task wsj, batch 597 (8597): perplexity: 51.0474, wsj_loss: 3.9328 ||
04/19 01:15:11 PM: Update 8609: task wsj, batch 609 (8609): perplexity: 51.1084, wsj_loss: 3.9339 ||
04/19 01:15:21 PM: Update 8621: task wsj, batch 621 (8621): perplexity: 51.1156, wsj_loss: 3.9341 ||
04/19 01:15:32 PM: Update 8633: task wsj, batch 633 (8633): perplexity: 51.1600, wsj_loss: 3.9350 ||
04/19 01:15:42 PM: Update 8641: task wsj, batch 641 (8641): perplexity: 51.1018, wsj_loss: 3.9338 ||
04/19 01:15:53 PM: Update 8653: task wsj, batch 653 (8653): perplexity: 51.0634, wsj_loss: 3.9331 ||
04/19 01:16:03 PM: Update 8665: task wsj, batch 665 (8665): perplexity: 50.9780, wsj_loss: 3.9314 ||
04/19 01:16:13 PM: Update 8677: task wsj, batch 677 (8677): perplexity: 50.8870, wsj_loss: 3.9296 ||
04/19 01:16:24 PM: Update 8689: task wsj, batch 689 (8689): perplexity: 50.8252, wsj_loss: 3.9284 ||
04/19 01:16:34 PM: Update 8701: task wsj, batch 701 (8701): perplexity: 50.6798, wsj_loss: 3.9255 ||
04/19 01:16:44 PM: Update 8713: task wsj, batch 713 (8713): perplexity: 50.6068, wsj_loss: 3.9241 ||
04/19 01:16:55 PM: Update 8725: task wsj, batch 725 (8725): perplexity: 50.5644, wsj_loss: 3.9232 ||
04/19 01:17:12 PM: Update 8737: task wsj, batch 737 (8737): perplexity: 50.4820, wsj_loss: 3.9216 ||
04/19 01:17:22 PM: Update 8749: task wsj, batch 749 (8749): perplexity: 50.4299, wsj_loss: 3.9206 ||
04/19 01:17:33 PM: Update 8761: task wsj, batch 761 (8761): perplexity: 50.4391, wsj_loss: 3.9208 ||
04/19 01:17:43 PM: Update 8773: task wsj, batch 773 (8773): perplexity: 50.3462, wsj_loss: 3.9189 ||
04/19 01:17:53 PM: Update 8785: task wsj, batch 785 (8785): perplexity: 50.3156, wsj_loss: 3.9183 ||
04/19 01:18:04 PM: Update 8797: task wsj, batch 797 (8797): perplexity: 50.2799, wsj_loss: 3.9176 ||
04/19 01:18:14 PM: Update 8809: task wsj, batch 809 (8809): perplexity: 50.2565, wsj_loss: 3.9171 ||
04/19 01:18:25 PM: Update 8821: task wsj, batch 821 (8821): perplexity: 50.2228, wsj_loss: 3.9165 ||
04/19 01:18:35 PM: Update 8833: task wsj, batch 833 (8833): perplexity: 50.2219, wsj_loss: 3.9165 ||
04/19 01:18:46 PM: Update 8845: task wsj, batch 845 (8845): perplexity: 50.2246, wsj_loss: 3.9165 ||
04/19 01:18:56 PM: Update 8857: task wsj, batch 857 (8857): perplexity: 50.2264, wsj_loss: 3.9165 ||
04/19 01:19:06 PM: Update 8869: task wsj, batch 869 (8869): perplexity: 50.2152, wsj_loss: 3.9163 ||
04/19 01:19:16 PM: Update 8881: task wsj, batch 881 (8881): perplexity: 50.1590, wsj_loss: 3.9152 ||
04/19 01:19:27 PM: Update 8893: task wsj, batch 893 (8893): perplexity: 50.1021, wsj_loss: 3.9141 ||
04/19 01:19:37 PM: Update 8905: task wsj, batch 905 (8905): perplexity: 50.0889, wsj_loss: 3.9138 ||
04/19 01:19:47 PM: Update 8917: task wsj, batch 917 (8917): perplexity: 50.0892, wsj_loss: 3.9138 ||
04/19 01:19:58 PM: Update 8929: task wsj, batch 929 (8929): perplexity: 50.1162, wsj_loss: 3.9143 ||
04/19 01:20:08 PM: Update 8941: task wsj, batch 941 (8941): perplexity: 50.1266, wsj_loss: 3.9146 ||
04/19 01:20:18 PM: Update 8953: task wsj, batch 953 (8953): perplexity: 50.1408, wsj_loss: 3.9148 ||
04/19 01:20:28 PM: Update 8965: task wsj, batch 965 (8965): perplexity: 50.1166, wsj_loss: 3.9144 ||
04/19 01:20:39 PM: Update 8977: task wsj, batch 977 (8977): perplexity: 50.1364, wsj_loss: 3.9147 ||
04/19 01:20:49 PM: Update 8989: task wsj, batch 989 (8989): perplexity: 50.1637, wsj_loss: 3.9153 ||
04/19 01:20:59 PM: ***** Pass 9000 / Epoch 9 *****
04/19 01:20:59 PM: wsj: trained on 1000 batches, 2.410 epochs
04/19 01:20:59 PM: Validating...
04/19 01:20:59 PM: Batch 2/33: perplexity: 132.1992, wsj_loss: 4.8843 || , for evaluation data
04/19 01:21:08 PM: Advancing scheduler.
04/19 01:21:08 PM: 	Best macro_avg: 106.757
04/19 01:21:08 PM: 	# bad epochs: 4
04/19 01:21:08 PM: Statistic: wsj_loss
04/19 01:21:08 PM: 	training: 3.916316
04/19 01:21:08 PM: 	validation: 4.747480
04/19 01:21:08 PM: Statistic: macro_avg
04/19 01:21:08 PM: 	validation: 115.293379
04/19 01:21:08 PM: Statistic: micro_avg
04/19 01:21:08 PM: 	validation: 115.293379
04/19 01:21:08 PM: Statistic: wsj_perplexity
04/19 01:21:08 PM: 	training: 50.215101
04/19 01:21:08 PM: 	validation: 115.293379
04/19 01:21:08 PM: global_lr: 0.001000
04/19 01:21:08 PM: Saved files to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0
04/19 01:21:10 PM: Update 9002: task wsj, batch 2 (9002): perplexity: 62.9619, wsj_loss: 4.1425 ||
04/19 01:21:20 PM: Update 9014: task wsj, batch 14 (9014): perplexity: 50.4442, wsj_loss: 3.9209 ||
04/19 01:21:30 PM: Update 9026: task wsj, batch 26 (9026): perplexity: 51.6468, wsj_loss: 3.9444 ||
04/19 01:21:40 PM: Update 9038: task wsj, batch 38 (9038): perplexity: 52.1558, wsj_loss: 3.9542 ||
04/19 01:21:55 PM: Update 9050: task wsj, batch 50 (9050): perplexity: 50.9734, wsj_loss: 3.9313 ||
04/19 01:22:05 PM: Update 9062: task wsj, batch 62 (9062): perplexity: 49.8893, wsj_loss: 3.9098 ||
04/19 01:22:15 PM: Update 9074: task wsj, batch 74 (9074): perplexity: 49.0966, wsj_loss: 3.8938 ||
04/19 01:22:25 PM: Update 9086: task wsj, batch 86 (9086): perplexity: 48.5660, wsj_loss: 3.8829 ||
04/19 01:22:35 PM: Update 9098: task wsj, batch 98 (9098): perplexity: 48.1676, wsj_loss: 3.8747 ||
04/19 01:22:46 PM: Update 9110: task wsj, batch 110 (9110): perplexity: 48.0640, wsj_loss: 3.8725 ||
04/19 01:22:56 PM: Update 9122: task wsj, batch 122 (9122): perplexity: 47.8208, wsj_loss: 3.8675 ||
04/19 01:23:06 PM: Update 9134: task wsj, batch 134 (9134): perplexity: 47.6951, wsj_loss: 3.8648 ||
04/19 01:23:17 PM: Update 9146: task wsj, batch 146 (9146): perplexity: 47.5447, wsj_loss: 3.8617 ||
04/19 01:23:29 PM: Update 9153: task wsj, batch 153 (9153): perplexity: 47.4229, wsj_loss: 3.8591 ||
04/19 01:23:39 PM: Update 9165: task wsj, batch 165 (9165): perplexity: 47.5019, wsj_loss: 3.8608 ||
04/19 01:23:50 PM: Update 9177: task wsj, batch 177 (9177): perplexity: 47.4075, wsj_loss: 3.8588 ||
04/19 01:24:00 PM: Update 9189: task wsj, batch 189 (9189): perplexity: 47.4685, wsj_loss: 3.8601 ||
04/19 01:24:10 PM: Update 9201: task wsj, batch 201 (9201): perplexity: 47.4875, wsj_loss: 3.8605 ||
04/19 01:24:21 PM: Update 9214: task wsj, batch 214 (9214): perplexity: 47.5289, wsj_loss: 3.8613 ||
04/19 01:24:31 PM: Update 9226: task wsj, batch 226 (9226): perplexity: 47.5023, wsj_loss: 3.8608 ||
04/19 01:24:41 PM: Update 9238: task wsj, batch 238 (9238): perplexity: 47.5333, wsj_loss: 3.8614 ||
04/19 01:24:52 PM: Update 9250: task wsj, batch 250 (9250): perplexity: 47.6158, wsj_loss: 3.8632 ||
04/19 01:25:02 PM: Update 9262: task wsj, batch 262 (9262): perplexity: 47.5189, wsj_loss: 3.8611 ||
04/19 01:25:12 PM: Update 9274: task wsj, batch 274 (9274): perplexity: 47.5376, wsj_loss: 3.8615 ||
04/19 01:25:23 PM: Update 9286: task wsj, batch 286 (9286): perplexity: 47.5797, wsj_loss: 3.8624 ||
04/19 01:25:33 PM: Update 9298: task wsj, batch 298 (9298): perplexity: 47.5172, wsj_loss: 3.8611 ||
04/19 01:25:43 PM: Update 9310: task wsj, batch 310 (9310): perplexity: 47.6438, wsj_loss: 3.8638 ||
04/19 01:25:54 PM: Update 9322: task wsj, batch 322 (9322): perplexity: 47.6624, wsj_loss: 3.8641 ||
04/19 01:26:04 PM: Update 9334: task wsj, batch 334 (9334): perplexity: 47.5857, wsj_loss: 3.8625 ||
04/19 01:26:15 PM: Update 9346: task wsj, batch 346 (9346): perplexity: 47.6728, wsj_loss: 3.8644 ||
04/19 01:26:25 PM: Update 9358: task wsj, batch 358 (9358): perplexity: 47.7154, wsj_loss: 3.8653 ||
04/19 01:26:36 PM: Update 9370: task wsj, batch 370 (9370): perplexity: 47.8271, wsj_loss: 3.8676 ||
04/19 01:26:46 PM: Update 9382: task wsj, batch 382 (9382): perplexity: 47.9419, wsj_loss: 3.8700 ||
04/19 01:26:57 PM: Update 9394: task wsj, batch 394 (9394): perplexity: 47.9864, wsj_loss: 3.8709 ||
04/19 01:27:07 PM: Update 9406: task wsj, batch 406 (9406): perplexity: 48.0746, wsj_loss: 3.8728 ||
04/19 01:27:17 PM: Update 9418: task wsj, batch 418 (9418): perplexity: 48.0624, wsj_loss: 3.8725 ||
04/19 01:27:27 PM: Update 9430: task wsj, batch 430 (9430): perplexity: 48.0085, wsj_loss: 3.8714 ||
04/19 01:27:38 PM: Update 9442: task wsj, batch 442 (9442): perplexity: 48.0862, wsj_loss: 3.8730 ||
04/19 01:27:48 PM: Update 9454: task wsj, batch 454 (9454): perplexity: 48.0982, wsj_loss: 3.8732 ||
04/19 01:28:02 PM: Update 9466: task wsj, batch 466 (9466): perplexity: 48.2204, wsj_loss: 3.8758 ||
04/19 01:28:12 PM: Update 9478: task wsj, batch 478 (9478): perplexity: 48.0966, wsj_loss: 3.8732 ||
04/19 01:28:22 PM: Update 9490: task wsj, batch 490 (9490): perplexity: 48.0272, wsj_loss: 3.8718 ||
04/19 01:28:32 PM: Update 9502: task wsj, batch 502 (9502): perplexity: 47.9489, wsj_loss: 3.8701 ||
04/19 01:28:43 PM: Update 9515: task wsj, batch 515 (9515): perplexity: 47.8583, wsj_loss: 3.8682 ||
04/19 01:28:53 PM: Update 9527: task wsj, batch 527 (9527): perplexity: 47.7936, wsj_loss: 3.8669 ||
04/19 01:29:04 PM: Update 9539: task wsj, batch 539 (9539): perplexity: 47.7380, wsj_loss: 3.8657 ||
04/19 01:29:14 PM: Update 9551: task wsj, batch 551 (9551): perplexity: 47.6514, wsj_loss: 3.8639 ||
04/19 01:29:24 PM: Update 9563: task wsj, batch 563 (9563): perplexity: 47.6187, wsj_loss: 3.8632 ||
04/19 01:29:36 PM: Update 9569: task wsj, batch 569 (9569): perplexity: 47.5181, wsj_loss: 3.8611 ||
04/19 01:29:47 PM: Update 9581: task wsj, batch 581 (9581): perplexity: 47.4459, wsj_loss: 3.8596 ||
04/19 01:29:57 PM: Update 9593: task wsj, batch 593 (9593): perplexity: 47.4023, wsj_loss: 3.8587 ||
04/19 01:30:07 PM: Update 9605: task wsj, batch 605 (9605): perplexity: 47.3965, wsj_loss: 3.8585 ||
04/19 01:30:18 PM: Update 9617: task wsj, batch 617 (9617): perplexity: 47.3656, wsj_loss: 3.8579 ||
04/19 01:30:28 PM: Update 9629: task wsj, batch 629 (9629): perplexity: 47.3342, wsj_loss: 3.8572 ||
04/19 01:30:38 PM: Update 9641: task wsj, batch 641 (9641): perplexity: 47.3157, wsj_loss: 3.8568 ||
04/19 01:30:48 PM: Update 9653: task wsj, batch 653 (9653): perplexity: 47.2958, wsj_loss: 3.8564 ||
04/19 01:30:59 PM: Update 9665: task wsj, batch 665 (9665): perplexity: 47.2259, wsj_loss: 3.8549 ||
04/19 01:31:09 PM: Update 9677: task wsj, batch 677 (9677): perplexity: 47.2552, wsj_loss: 3.8556 ||
04/19 01:31:20 PM: Update 9689: task wsj, batch 689 (9689): perplexity: 47.2765, wsj_loss: 3.8560 ||
04/19 01:31:30 PM: Update 9701: task wsj, batch 701 (9701): perplexity: 47.3079, wsj_loss: 3.8567 ||
04/19 01:31:40 PM: Update 9713: task wsj, batch 713 (9713): perplexity: 47.3125, wsj_loss: 3.8568 ||
04/19 01:31:51 PM: Update 9725: task wsj, batch 725 (9725): perplexity: 47.3039, wsj_loss: 3.8566 ||
04/19 01:32:01 PM: Update 9737: task wsj, batch 737 (9737): perplexity: 47.3164, wsj_loss: 3.8569 ||
04/19 01:32:12 PM: Update 9749: task wsj, batch 749 (9749): perplexity: 47.3330, wsj_loss: 3.8572 ||
04/19 01:32:22 PM: Update 9761: task wsj, batch 761 (9761): perplexity: 47.3333, wsj_loss: 3.8572 ||
04/19 01:32:33 PM: Update 9773: task wsj, batch 773 (9773): perplexity: 47.3881, wsj_loss: 3.8584 ||
04/19 01:32:43 PM: Update 9785: task wsj, batch 785 (9785): perplexity: 47.4297, wsj_loss: 3.8592 ||
04/19 01:32:54 PM: Update 9797: task wsj, batch 797 (9797): perplexity: 47.4388, wsj_loss: 3.8594 ||
04/19 01:33:04 PM: Update 9809: task wsj, batch 809 (9809): perplexity: 47.4611, wsj_loss: 3.8599 ||
04/19 01:33:14 PM: Update 9821: task wsj, batch 821 (9821): perplexity: 47.5018, wsj_loss: 3.8608 ||
04/19 01:33:25 PM: Update 9833: task wsj, batch 833 (9833): perplexity: 47.5048, wsj_loss: 3.8608 ||
04/19 01:33:35 PM: Update 9845: task wsj, batch 845 (9845): perplexity: 47.5089, wsj_loss: 3.8609 ||
04/19 01:33:45 PM: Update 9857: task wsj, batch 857 (9857): perplexity: 47.5158, wsj_loss: 3.8611 ||
04/19 01:33:56 PM: Update 9869: task wsj, batch 869 (9869): perplexity: 47.5520, wsj_loss: 3.8618 ||
04/19 01:34:06 PM: Update 9881: task wsj, batch 881 (9881): perplexity: 47.5890, wsj_loss: 3.8626 ||
04/19 01:34:17 PM: Update 9889: task wsj, batch 889 (9889): perplexity: 47.5630, wsj_loss: 3.8621 ||
04/19 01:34:27 PM: Update 9901: task wsj, batch 901 (9901): perplexity: 47.5508, wsj_loss: 3.8618 ||
04/19 01:34:38 PM: Update 9913: task wsj, batch 913 (9913): perplexity: 47.5073, wsj_loss: 3.8609 ||
04/19 01:34:48 PM: Update 9925: task wsj, batch 925 (9925): perplexity: 47.4480, wsj_loss: 3.8596 ||
04/19 01:34:58 PM: Update 9937: task wsj, batch 937 (9937): perplexity: 47.3722, wsj_loss: 3.8580 ||
04/19 01:35:09 PM: Update 9949: task wsj, batch 949 (9949): perplexity: 47.3168, wsj_loss: 3.8569 ||
04/19 01:35:19 PM: Update 9961: task wsj, batch 961 (9961): perplexity: 47.2390, wsj_loss: 3.8552 ||
04/19 01:35:29 PM: Update 9973: task wsj, batch 973 (9973): perplexity: 47.1947, wsj_loss: 3.8543 ||
04/19 01:35:46 PM: Update 9985: task wsj, batch 985 (9985): perplexity: 47.1505, wsj_loss: 3.8533 ||
04/19 01:35:56 PM: Update 9997: task wsj, batch 997 (9997): perplexity: 47.1254, wsj_loss: 3.8528 ||
04/19 01:35:59 PM: ***** Pass 10000 / Epoch 10 *****
04/19 01:35:59 PM: wsj: trained on 1000 batches, 2.410 epochs
04/19 01:35:59 PM: Validating...
04/19 01:36:06 PM: Batch 25/33: perplexity: 129.1440, wsj_loss: 4.8609 || , for evaluation data
04/19 01:36:08 PM: Advancing scheduler.
04/19 01:36:08 PM: 	Best macro_avg: 106.757
04/19 01:36:08 PM: 	# bad epochs: 5
04/19 01:36:08 PM: Statistic: wsj_loss
04/19 01:36:08 PM: 	training: 3.852422
04/19 01:36:08 PM: 	validation: 4.750415
04/19 01:36:08 PM: Statistic: macro_avg
04/19 01:36:08 PM: 	validation: 115.632256
04/19 01:36:08 PM: Statistic: micro_avg
04/19 01:36:08 PM: 	validation: 115.632256
04/19 01:36:08 PM: Statistic: wsj_perplexity
04/19 01:36:08 PM: 	training: 47.107009
04/19 01:36:08 PM: 	validation: 115.632256
04/19 01:36:08 PM: global_lr: 0.001000
04/19 01:36:08 PM: Saved files to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0
04/19 01:36:17 PM: Update 10010: task wsj, batch 10 (10010): perplexity: 44.8503, wsj_loss: 3.8033 ||
04/19 01:36:27 PM: Update 10022: task wsj, batch 22 (10022): perplexity: 43.4037, wsj_loss: 3.7705 ||
04/19 01:36:37 PM: Update 10034: task wsj, batch 34 (10034): perplexity: 43.3079, wsj_loss: 3.7683 ||
04/19 01:36:47 PM: Update 10046: task wsj, batch 46 (10046): perplexity: 44.5493, wsj_loss: 3.7966 ||
04/19 01:36:57 PM: Update 10058: task wsj, batch 58 (10058): perplexity: 44.8941, wsj_loss: 3.8043 ||
04/19 01:37:08 PM: Update 10070: task wsj, batch 70 (10070): perplexity: 44.7218, wsj_loss: 3.8005 ||
04/19 01:37:18 PM: Update 10082: task wsj, batch 82 (10082): perplexity: 44.3606, wsj_loss: 3.7924 ||
04/19 01:37:28 PM: Update 10094: task wsj, batch 94 (10094): perplexity: 44.8956, wsj_loss: 3.8043 ||
04/19 01:37:39 PM: Update 10106: task wsj, batch 106 (10106): perplexity: 45.2943, wsj_loss: 3.8132 ||
04/19 01:37:49 PM: Update 10118: task wsj, batch 118 (10118): perplexity: 45.4177, wsj_loss: 3.8159 ||
04/19 01:38:00 PM: Update 10130: task wsj, batch 130 (10130): perplexity: 45.4919, wsj_loss: 3.8175 ||
04/19 01:38:10 PM: Update 10142: task wsj, batch 142 (10142): perplexity: 45.3187, wsj_loss: 3.8137 ||
04/19 01:38:20 PM: Update 10154: task wsj, batch 154 (10154): perplexity: 45.3453, wsj_loss: 3.8143 ||
04/19 01:38:31 PM: Update 10166: task wsj, batch 166 (10166): perplexity: 45.5107, wsj_loss: 3.8179 ||
04/19 01:38:41 PM: Update 10178: task wsj, batch 178 (10178): perplexity: 45.6714, wsj_loss: 3.8215 ||
04/19 01:38:51 PM: Update 10190: task wsj, batch 190 (10190): perplexity: 45.7738, wsj_loss: 3.8237 ||
04/19 01:39:02 PM: Update 10202: task wsj, batch 202 (10202): perplexity: 45.9649, wsj_loss: 3.8279 ||
04/19 01:39:12 PM: Update 10214: task wsj, batch 214 (10214): perplexity: 46.1193, wsj_loss: 3.8312 ||
04/19 01:39:22 PM: Update 10226: task wsj, batch 226 (10226): perplexity: 46.3412, wsj_loss: 3.8360 ||
04/19 01:39:32 PM: Update 10238: task wsj, batch 238 (10238): perplexity: 46.3731, wsj_loss: 3.8367 ||
04/19 01:39:43 PM: Update 10250: task wsj, batch 250 (10250): perplexity: 46.2440, wsj_loss: 3.8339 ||
04/19 01:39:53 PM: Update 10262: task wsj, batch 262 (10262): perplexity: 46.3016, wsj_loss: 3.8352 ||
04/19 01:40:03 PM: Update 10274: task wsj, batch 274 (10274): perplexity: 46.4536, wsj_loss: 3.8385 ||
04/19 01:40:14 PM: Update 10286: task wsj, batch 286 (10286): perplexity: 46.6511, wsj_loss: 3.8427 ||
04/19 01:40:27 PM: Update 10298: task wsj, batch 298 (10298): perplexity: 46.7966, wsj_loss: 3.8458 ||
04/19 01:40:37 PM: Update 10310: task wsj, batch 310 (10310): perplexity: 46.5913, wsj_loss: 3.8414 ||
04/19 01:40:47 PM: Update 10322: task wsj, batch 322 (10322): perplexity: 46.4623, wsj_loss: 3.8386 ||
04/19 01:40:57 PM: Update 10334: task wsj, batch 334 (10334): perplexity: 46.3789, wsj_loss: 3.8368 ||
04/19 01:41:08 PM: Update 10346: task wsj, batch 346 (10346): perplexity: 46.2402, wsj_loss: 3.8338 ||
04/19 01:41:18 PM: Update 10358: task wsj, batch 358 (10358): perplexity: 46.0137, wsj_loss: 3.8289 ||
04/19 01:41:28 PM: Update 10370: task wsj, batch 370 (10370): perplexity: 45.9233, wsj_loss: 3.8270 ||
04/19 01:41:38 PM: Update 10382: task wsj, batch 382 (10382): perplexity: 45.8965, wsj_loss: 3.8264 ||
04/19 01:41:48 PM: Update 10394: task wsj, batch 394 (10394): perplexity: 45.8132, wsj_loss: 3.8246 ||
04/19 01:42:01 PM: Update 10401: task wsj, batch 401 (10401): perplexity: 45.7393, wsj_loss: 3.8230 ||
04/19 01:42:11 PM: Update 10413: task wsj, batch 413 (10413): perplexity: 45.7200, wsj_loss: 3.8225 ||
04/19 01:42:22 PM: Update 10425: task wsj, batch 425 (10425): perplexity: 45.7034, wsj_loss: 3.8222 ||
04/19 01:42:32 PM: Update 10437: task wsj, batch 437 (10437): perplexity: 45.6416, wsj_loss: 3.8208 ||
04/19 01:42:42 PM: Update 10449: task wsj, batch 449 (10449): perplexity: 45.6680, wsj_loss: 3.8214 ||
04/19 01:42:53 PM: Update 10461: task wsj, batch 461 (10461): perplexity: 45.6535, wsj_loss: 3.8211 ||
04/19 01:43:03 PM: Update 10473: task wsj, batch 473 (10473): perplexity: 45.6811, wsj_loss: 3.8217 ||
04/19 01:43:13 PM: Update 10485: task wsj, batch 485 (10485): perplexity: 45.5998, wsj_loss: 3.8199 ||
04/19 01:43:24 PM: Update 10497: task wsj, batch 497 (10497): perplexity: 45.5299, wsj_loss: 3.8184 ||
04/19 01:43:34 PM: Update 10509: task wsj, batch 509 (10509): perplexity: 45.5741, wsj_loss: 3.8193 ||
04/19 01:43:45 PM: Update 10521: task wsj, batch 521 (10521): perplexity: 45.5412, wsj_loss: 3.8186 ||
04/19 01:43:55 PM: Update 10533: task wsj, batch 533 (10533): perplexity: 45.5443, wsj_loss: 3.8187 ||
04/19 01:44:05 PM: Update 10545: task wsj, batch 545 (10545): perplexity: 45.5231, wsj_loss: 3.8182 ||
04/19 01:44:16 PM: Update 10557: task wsj, batch 557 (10557): perplexity: 45.5357, wsj_loss: 3.8185 ||
04/19 01:44:26 PM: Update 10569: task wsj, batch 569 (10569): perplexity: 45.4931, wsj_loss: 3.8176 ||
04/19 01:44:37 PM: Update 10581: task wsj, batch 581 (10581): perplexity: 45.5411, wsj_loss: 3.8186 ||
04/19 01:44:47 PM: Update 10593: task wsj, batch 593 (10593): perplexity: 45.5641, wsj_loss: 3.8191 ||
04/19 01:44:58 PM: Update 10605: task wsj, batch 605 (10605): perplexity: 45.5518, wsj_loss: 3.8188 ||
04/19 01:45:08 PM: Update 10617: task wsj, batch 617 (10617): perplexity: 45.5999, wsj_loss: 3.8199 ||
04/19 01:45:19 PM: Update 10629: task wsj, batch 629 (10629): perplexity: 45.6601, wsj_loss: 3.8212 ||
04/19 01:45:29 PM: Update 10641: task wsj, batch 641 (10641): perplexity: 45.7029, wsj_loss: 3.8222 ||
04/19 01:45:39 PM: Update 10653: task wsj, batch 653 (10653): perplexity: 45.7181, wsj_loss: 3.8225 ||
04/19 01:45:50 PM: Update 10665: task wsj, batch 665 (10665): perplexity: 45.7170, wsj_loss: 3.8225 ||
04/19 01:46:00 PM: Update 10677: task wsj, batch 677 (10677): perplexity: 45.7726, wsj_loss: 3.8237 ||
04/19 01:46:11 PM: Update 10689: task wsj, batch 689 (10689): perplexity: 45.7995, wsj_loss: 3.8243 ||
04/19 01:46:21 PM: Update 10701: task wsj, batch 701 (10701): perplexity: 45.8213, wsj_loss: 3.8247 ||
04/19 01:46:31 PM: Update 10713: task wsj, batch 713 (10713): perplexity: 45.8263, wsj_loss: 3.8249 ||
04/19 01:46:42 PM: Update 10722: task wsj, batch 722 (10722): perplexity: 45.7473, wsj_loss: 3.8231 ||
04/19 01:46:53 PM: Update 10734: task wsj, batch 734 (10734): perplexity: 45.6834, wsj_loss: 3.8217 ||
04/19 01:47:03 PM: Update 10746: task wsj, batch 746 (10746): perplexity: 45.6301, wsj_loss: 3.8206 ||
04/19 01:47:13 PM: Update 10758: task wsj, batch 758 (10758): perplexity: 45.5809, wsj_loss: 3.8195 ||
04/19 01:47:24 PM: Update 10770: task wsj, batch 770 (10770): perplexity: 45.5085, wsj_loss: 3.8179 ||
04/19 01:47:34 PM: Update 10782: task wsj, batch 782 (10782): perplexity: 45.4645, wsj_loss: 3.8169 ||
04/19 01:47:45 PM: Update 10794: task wsj, batch 794 (10794): perplexity: 45.3985, wsj_loss: 3.8155 ||
04/19 01:47:55 PM: Update 10806: task wsj, batch 806 (10806): perplexity: 45.3686, wsj_loss: 3.8148 ||
04/19 01:48:11 PM: Update 10817: task wsj, batch 817 (10817): perplexity: 45.3526, wsj_loss: 3.8145 ||
04/19 01:48:21 PM: Update 10829: task wsj, batch 829 (10829): perplexity: 45.3119, wsj_loss: 3.8136 ||
04/19 01:48:32 PM: Update 10841: task wsj, batch 841 (10841): perplexity: 45.2500, wsj_loss: 3.8122 ||
04/19 01:48:42 PM: Update 10853: task wsj, batch 853 (10853): perplexity: 45.2454, wsj_loss: 3.8121 ||
04/19 01:48:53 PM: Update 10865: task wsj, batch 865 (10865): perplexity: 45.2217, wsj_loss: 3.8116 ||
04/19 01:49:03 PM: Update 10877: task wsj, batch 877 (10877): perplexity: 45.1915, wsj_loss: 3.8109 ||
04/19 01:49:13 PM: Update 10889: task wsj, batch 889 (10889): perplexity: 45.1907, wsj_loss: 3.8109 ||
04/19 01:49:24 PM: Update 10901: task wsj, batch 901 (10901): perplexity: 45.2137, wsj_loss: 3.8114 ||
04/19 01:49:34 PM: Update 10913: task wsj, batch 913 (10913): perplexity: 45.2129, wsj_loss: 3.8114 ||
04/19 01:49:45 PM: Update 10925: task wsj, batch 925 (10925): perplexity: 45.1804, wsj_loss: 3.8107 ||
04/19 01:49:55 PM: Update 10937: task wsj, batch 937 (10937): perplexity: 45.1882, wsj_loss: 3.8108 ||
04/19 01:50:05 PM: Update 10949: task wsj, batch 949 (10949): perplexity: 45.2179, wsj_loss: 3.8115 ||
04/19 01:50:16 PM: Update 10961: task wsj, batch 961 (10961): perplexity: 45.1928, wsj_loss: 3.8109 ||
04/19 01:50:26 PM: Update 10973: task wsj, batch 973 (10973): perplexity: 45.1629, wsj_loss: 3.8103 ||
04/19 01:50:36 PM: Update 10985: task wsj, batch 985 (10985): perplexity: 45.1349, wsj_loss: 3.8097 ||
04/19 01:50:47 PM: Update 10997: task wsj, batch 997 (10997): perplexity: 45.0915, wsj_loss: 3.8087 ||
04/19 01:50:49 PM: ***** Pass 11000 / Epoch 11 *****
04/19 01:50:49 PM: wsj: trained on 1000 batches, 2.410 epochs
04/19 01:50:49 PM: Validating...
04/19 01:50:58 PM: Batch 27/33: perplexity: 125.7470, wsj_loss: 4.8343 || , for evaluation data
04/19 01:50:59 PM: Out of patience. Stopped tracking wsj
04/19 01:50:59 PM: Out of patience. Stopped tracking micro
04/19 01:50:59 PM: Out of patience. Stopped tracking macro
04/19 01:50:59 PM: Advancing scheduler.
04/19 01:50:59 PM: 	Best macro_avg: 106.757
04/19 01:50:59 PM: 	# bad epochs: 0
04/19 01:50:59 PM: All metrics ran out of patience. Stopping training.
04/19 01:50:59 PM: Statistic: wsj_loss
04/19 01:50:59 PM: 	training: 3.808865
04/19 01:50:59 PM: 	validation: 4.779021
04/19 01:50:59 PM: Statistic: macro_avg
04/19 01:50:59 PM: 	validation: 118.987766
04/19 01:50:59 PM: Statistic: micro_avg
04/19 01:50:59 PM: 	validation: 118.987766
04/19 01:50:59 PM: Statistic: wsj_perplexity
04/19 01:50:59 PM: 	training: 45.099232
04/19 01:50:59 PM: 	validation: 118.987766
04/19 01:50:59 PM: global_lr: 0.000500
04/19 01:50:59 PM: Saved files to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0
04/19 01:50:59 PM: Stopped training after 11 validation checks
04/19 01:50:59 PM: Trained wsj for 11000 batches or 26.506 epochs
04/19 01:50:59 PM: ***** VALIDATION RESULTS *****
04/19 01:50:59 PM: wsj_perplexity, 5, wsj_loss: 4.67056, macro_avg: 106.75748, micro_avg: 106.75748, wsj_perplexity: 106.75748
04/19 01:50:59 PM: micro_avg, 5, wsj_loss: 4.67056, macro_avg: 106.75748, micro_avg: 106.75748, wsj_perplexity: 106.75748
04/19 01:50:59 PM: macro_avg, 5, wsj_loss: 4.67056, macro_avg: 106.75748, micro_avg: 106.75748, wsj_perplexity: 106.75748
04/19 01:50:59 PM: In strict mode because do_target_task_training is off. Will crash if any tasks are missing from the checkpoint.
04/19 01:51:00 PM: Loaded model state from /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/prpn-0/model_state_main_epoch_5.best_macro.th
04/19 01:51:00 PM: Evaluating...
04/19 01:51:00 PM: Evaluating on: wsj, split: val
04/19 01:51:09 PM: Task wsj: has no predictions!
04/19 01:51:09 PM: Writing results for split 'val' to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj/results.tsv
04/19 01:51:09 PM: micro_avg: 106.757, macro_avg: 106.757, wsj_perplexity: 106.757
04/19 01:51:09 PM: Done!
Epoch    11: reducing learning rate of group 0 to 5.0000e-04.
Starting main job...
04/19 02:11:25 PM: fastText library not found!
04/19 02:11:26 PM: Loading config from config/prpn.conf
04/19 02:11:26 PM: Waiting on git info....
04/19 02:11:26 PM: Git branch: prpn
04/19 02:11:26 PM: Git SHA: 39abaff9a9d1348ba93e4fc947b6e185ef98e6db
04/19 02:11:26 PM: Parsed args: 
{
  "FASTTEXT_MODEL_FILE": "",
  "JIANT_DATA_DIR": "/scratch/pmh330/jiant-data/",
  "NFS_PROJECT_PREFIX": "/beegfs/pmh330/jiant-prpn-outs",
  "allow_missing_task_map": 0,
  "allow_reuse_of_pretraining_parameters": 0,
  "allow_untrained_encoder_parameters": 0,
  "batch_size": 64,
  "bert_embeddings_mode": "none",
  "bert_fine_tune": 0,
  "bert_model_name": "",
  "bidirectional": 0,
  "bpp_base": 1,
  "char_embs": 0,
  "char_filter_sizes": "2,3,4,5",
  "classifier": "mlp",
  "classifier_dropout": 0.2,
  "classifier_hid_dim": 512,
  "classifier_loss_fn": "",
  "classifier_span_pooling": "x,y",
  "cola": {},
  "cola_classifier_dropout": 0.2,
  "cola_classifier_hid_dim": 256,
  "cola_d_proj": 256,
  "cola_lr": 0.0003,
  "cola_val_interval": 100,
  "cove": 0,
  "cove_fine_tune": 0,
  "cuda": 0,
  "d_char": 100,
  "d_ff": 2048,
  "d_hid": 1200,
  "d_hid_attn": 512,
  "d_proj": 512,
  "d_tproj": 64,
  "d_word": 800,
  "data_dir": "/scratch/pmh330/jiant-data/",
  "dec_val_scale": 250,
  "do_full_eval": 1,
  "do_pretrain": 1,
  "do_target_task_training": 0,
  "dropout": 0.7,
  "dropout_embs": 0.7,
  "edgeprobe_cnn_context": 0,
  "edges-ccg-parse": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-ccg-tag": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-constituent-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-constituent-ptb": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-coref-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-coref-ontonotes-conll": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-dep-labeling": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-dep-labeling-ewt": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-dpr": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-ner-conll2003": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 250
  },
  "edges-ner-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-ner-tacred": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-nonterminal-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-pos-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-rel-semeval": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-rel-tacred": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-spr1": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-spr2": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-srl-conll2005": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-srl-conll2012": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-tmpl": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "elmo": 0,
  "elmo_chars_only": 1,
  "elmo_weight_file_path": "none",
  "embeddings_train": 0,
  "eval_data_fraction": 1,
  "eval_max_vals": 1000,
  "eval_val_interval": 500,
  "exp_dir": "/beegfs/pmh330/jiant-prpn-outs/prpn-wsj-2/",
  "exp_name": "prpn-wsj-2",
  "fastText": 0,
  "fastText_model_file": "",
  "force_include_wsj_vocabulary": 0,
  "global_ro_exp_dir": "/nfs/jsalt/share/exp/default",
  "grounded": {},
  "grounded_d_proj": 2048,
  "groundedsw": {},
  "groundedsw_d_proj": 2048,
  "is_probing_task": 0,
  "keep_all_checkpoints": 0,
  "load_eval_checkpoint": "none",
  "load_model": 1,
  "local_log_path": "/beegfs/pmh330/jiant-prpn-outs/prpn-wsj-2/prpn-0/log.log",
  "lr": 0.003,
  "lr_decay_factor": 0.5,
  "lr_patience": 5,
  "max_char_v_size": 250,
  "max_epochs": -1,
  "max_grad_norm": 1.0,
  "max_seq_len": 50,
  "max_targ_word_v_size": 20000,
  "max_vals": 1000,
  "max_word_v_size": 30000,
  "min_lr": 1e-06,
  "mnli": {},
  "mnli-alt": {},
  "mnli-alt_classifier_dropout": 0.2,
  "mnli-alt_classifier_hid_dim": 512,
  "mnli-alt_lr": 0.0003,
  "mnli-alt_pair_attn": 1,
  "mnli-alt_val_interval": 1000,
  "mnli-diagnostic": {
    "use_classifier": "mnli"
  },
  "mnli_classifier_dropout": 0.2,
  "mnli_classifier_hid_dim": 512,
  "mnli_lr": 0.0003,
  "mnli_pair_attn": 1,
  "mnli_val_interval": 1000,
  "mrpc": {},
  "mrpc_classifier_dropout": 0.2,
  "mrpc_classifier_hid_dim": 256,
  "mrpc_d_proj": 256,
  "mrpc_lr": 0.0003,
  "mrpc_pair_attn": 0,
  "mrpc_val_interval": 100,
  "n_char_filters": 100,
  "n_heads": 8,
  "n_layers_enc": 2,
  "n_layers_highway": 0,
  "n_slots": 15,
  "nli-prob": {
    "probe_path": ""
  },
  "onlstm_chunk_size": 10,
  "onlstm_dropconnect": 0.5,
  "onlstm_dropouth": 0.3,
  "onlstm_dropouti": 0.3,
  "onlstm_tying": 0,
  "openai_embeddings_mode": "none",
  "openai_transformer": 0,
  "openai_transformer_ckpt": "",
  "openai_transformer_fine_tune": 0,
  "optimizer": "adam",
  "pair_attn": 1,
  "patience": 5,
  "pool_type": "max",
  "pretrain_tasks": "wsj",
  "project_dir": "/beegfs/pmh330/jiant-prpn-outs",
  "qnli": {},
  "qnli-alt": {},
  "qnli-alt_classifier_dropout": 0.2,
  "qnli-alt_classifier_hid_dim": 512,
  "qnli-alt_lr": 0.0003,
  "qnli-alt_pair_attn": 1,
  "qnli-alt_val_interval": 1000,
  "qnli_classifier_dropout": 0.2,
  "qnli_classifier_hid_dim": 512,
  "qnli_lr": 0.0003,
  "qnli_pair_attn": 1,
  "qnli_val_interval": 1000,
  "qqp": {},
  "qqp-alt": {},
  "qqp-alt_classifier_dropout": 0.2,
  "qqp-alt_classifier_hid_dim": 512,
  "qqp-alt_lr": 0.0003,
  "qqp-alt_pair_attn": 1,
  "qqp-alt_val_interval": 1000,
  "qqp_classifier_dropout": 0.2,
  "qqp_classifier_hid_dim": 512,
  "qqp_lr": 0.0003,
  "qqp_pair_attn": 1,
  "qqp_val_interval": 1000,
  "random_seed": 1234,
  "reindex_tasks": "",
  "reload_indexing": 0,
  "reload_tasks": 0,
  "reload_vocab": 0,
  "remote_log_name": "prpn-wsj-2__prpn-0",
  "rte": {},
  "rte_classifier_dropout": 0.4,
  "rte_classifier_hid_dim": 128,
  "rte_d_proj": 128,
  "rte_lr": 0.0003,
  "rte_pair_attn": 0,
  "rte_val_interval": 100,
  "run_dir": "/beegfs/pmh330/jiant-prpn-outs/prpn-wsj-2/prpn-0",
  "run_name": "prpn-0",
  "s2s": {
    "attention": "bilinear",
    "d_hid_dec": 1024,
    "n_layers_dec": 1,
    "output_proj_input_dim": 1024,
    "target_embedding_dim": 300
  },
  "scaling_method": "uniform",
  "scheduler_threshold": 0.0001,
  "sent_enc": "prpn",
  "sep_embs_for_skip": 0,
  "shared_optimizer": 1,
  "shared_pair_attn": 0,
  "skip_embs": 0,
  "sst": {},
  "sst_classifier_dropout": 0.2,
  "sst_classifier_hid_dim": 256,
  "sst_d_proj": 256,
  "sst_lr": 0.0003,
  "sst_val_interval": 100,
  "sts-b": {},
  "sts-b-alt": {},
  "sts-b-alt_classifier_dropout": 0.2,
  "sts-b-alt_classifier_hid_dim": 512,
  "sts-b-alt_lr": 0.0003,
  "sts-b-alt_pair_attn": 1,
  "sts-b-alt_val_interval": 1000,
  "sts-b_classifier_dropout": 0.2,
  "sts-b_classifier_hid_dim": 512,
  "sts-b_lr": 0.0003,
  "sts-b_pair_attn": 1,
  "sts-b_val_interval": 1000,
  "target_tasks": "wsj",
  "tokenizer": "MosesTokenizer",
  "track_batch_utilization": 0,
  "trainer_type": "sampling",
  "training_data_fraction": 1,
  "transfer_paradigm": "frozen",
  "use_classifier": "",
  "val_data_limit": 5000,
  "val_interval": 1000,
  "warmup": 4000,
  "weighting_method": "proportional",
  "wnli": {},
  "wnli_classifier_dropout": 0.4,
  "wnli_classifier_hid_dim": 128,
  "wnli_d_proj": 128,
  "wnli_lr": 0.0003,
  "wnli_pair_attn": 0,
  "wnli_val_interval": 100,
  "word_embs": "scratch",
  "word_embs_file": "",
  "write_preds": 0,
  "write_strict_glue_format": 0
}
04/19 02:11:26 PM: Saved config to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj-2/prpn-0/params.conf
04/19 02:11:26 PM: Using random seed 1234
04/19 02:11:26 PM: Using GPU 0
04/19 02:11:26 PM: Loading tasks...
04/19 02:11:26 PM: Writing pre-preprocessed tasks to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj-2/
04/19 02:11:26 PM: 	Loaded existing task wsj
04/19 02:11:26 PM: 	Task 'wsj': train=18592 val=1476 test=1649
04/19 02:11:26 PM: 	Finished loading tasks: wsj.
04/19 02:11:26 PM: 	Building vocab from scratch
04/19 02:11:26 PM: 	Counting words for task: 'wsj'
04/19 02:11:27 PM: 	Finished counting words
04/19 02:11:27 PM: 	Saved vocab to /beegfs/pmh330/jiant-prpn-outs/prpn-wsj-2/vocab
04/19 02:11:27 PM: Loading token dictionary from /beegfs/pmh330/jiant-prpn-outs/prpn-wsj-2/vocab.
04/19 02:11:27 PM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 395, in <module>
    main(sys.argv[1:])
  File "main.py", line 185, in main
    pretrain_tasks, target_tasks, vocab, word_embs = build_tasks(args)
  File "/beegfs/pmh330/jiant/src/preprocess.py", line 283, in build_tasks
    vocab = Vocabulary.from_files(vocab_path)
  File "/home/pmh330/miniconda3/envs/jiant/lib/python3.6/site-packages/allennlp/data/vocabulary.py", line 289, in from_files
    vocab.set_from_file(filename, is_padded, namespace=namespace)
  File "/home/pmh330/miniconda3/envs/jiant/lib/python3.6/site-packages/allennlp/data/vocabulary.py", line 343, in set_from_file
    assert self._oov_token in self._token_to_index[namespace], "OOV token not found!"
AssertionError: OOV token not found!
Traceback (most recent call last):
  File "main.py", line 405, in <module>
    raise e  # re-raise exception, in case debugger is attached.
  File "main.py", line 395, in <module>
    main(sys.argv[1:])
  File "main.py", line 185, in main
    pretrain_tasks, target_tasks, vocab, word_embs = build_tasks(args)
  File "/beegfs/pmh330/jiant/src/preprocess.py", line 283, in build_tasks
    vocab = Vocabulary.from_files(vocab_path)
  File "/home/pmh330/miniconda3/envs/jiant/lib/python3.6/site-packages/allennlp/data/vocabulary.py", line 289, in from_files
    vocab.set_from_file(filename, is_padded, namespace=namespace)
  File "/home/pmh330/miniconda3/envs/jiant/lib/python3.6/site-packages/allennlp/data/vocabulary.py", line 343, in set_from_file
    assert self._oov_token in self._token_to_index[namespace], "OOV token not found!"
AssertionError: OOV token not found!
